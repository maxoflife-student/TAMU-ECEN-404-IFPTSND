{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "from graph_predictions import Graph_Predictions\n",
    "\n",
    "# tf.config.list_physical_devices()\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DMJ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-90a2af852fb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# path = os.getcwd()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# os.chdir(path + '\\ignorable_data')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDMJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDMJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDMJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DMJ' is not defined"
     ]
    }
   ],
   "source": [
    "# # Update the directory since all files were moved to \"ignorable_data\"\n",
    "# import os\n",
    "# path = os.getcwd()\n",
    "# os.chdir(path + '\\ignorable_data')\n",
    "# print(DMJ.data_splits['x_train'].shape)\n",
    "# print(DMJ.data_splits['x_val'].shape)\n",
    "# print(DMJ.data_splits['x_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.3711 - val_loss: 0.4959\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3690 - val_loss: 0.4934\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3670 - val_loss: 0.4908\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3649 - val_loss: 0.4884\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3629 - val_loss: 0.4859\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3609 - val_loss: 0.4834\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3589 - val_loss: 0.4810\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3570 - val_loss: 0.4787\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3550 - val_loss: 0.4763\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3531 - val_loss: 0.4740\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3512 - val_loss: 0.4717\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3493 - val_loss: 0.4694\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.500e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.3474 - val_loss: 0.4664\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3450 - val_loss: 0.4634\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3426 - val_loss: 0.4604\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3402 - val_loss: 0.4574\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3377 - val_loss: 0.4544\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3353 - val_loss: 0.4514\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3329 - val_loss: 0.4484\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3305 - val_loss: 0.4453\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3281 - val_loss: 0.4423\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3256 - val_loss: 0.4392\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3232 - val_loss: 0.4361\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3207 - val_loss: 0.4330\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 8.450e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.3183 - val_loss: 0.4289\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3151 - val_loss: 0.4248\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3119 - val_loss: 0.4208\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3087 - val_loss: 0.4168\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3057 - val_loss: 0.4128\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3026 - val_loss: 0.4089\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2997 - val_loss: 0.4051\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2967 - val_loss: 0.4012\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2939 - val_loss: 0.3974\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2910 - val_loss: 0.3937\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2881 - val_loss: 0.3899\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2852 - val_loss: 0.3862\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.099e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2823 - val_loss: 0.3813\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2785 - val_loss: 0.3764\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.2746 - val_loss: 0.3715\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.2707 - val_loss: 0.3666\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2667 - val_loss: 0.3616\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2628 - val_loss: 0.3566\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2587 - val_loss: 0.3516\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2547 - val_loss: 0.3465\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2506 - val_loss: 0.3413\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2464 - val_loss: 0.3360\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2422 - val_loss: 0.3306\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2379 - val_loss: 0.3252\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.428e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.2335 - val_loss: 0.3179\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2276 - val_loss: 0.3104\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2217 - val_loss: 0.3026\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2157 - val_loss: 0.2947\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2095 - val_loss: 0.2866\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2033 - val_loss: 0.2783\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1969 - val_loss: 0.2700\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1904 - val_loss: 0.2614\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1838 - val_loss: 0.2527\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1770 - val_loss: 0.2439\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1702 - val_loss: 0.2350\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1633 - val_loss: 0.2259\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.856e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1563 - val_loss: 0.2139\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1471 - val_loss: 0.2017\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1377 - val_loss: 0.1894\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1282 - val_loss: 0.1769\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1185 - val_loss: 0.1639\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1086 - val_loss: 0.1505\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0983 - val_loss: 0.1366\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0877 - val_loss: 0.1222\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0769 - val_loss: 0.1075\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0659 - val_loss: 0.0925\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0549 - val_loss: 0.0772\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0439 - val_loss: 0.0621\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.413e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0332 - val_loss: 0.0431\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0207 - val_loss: 0.0266\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0069 - val_loss: 0.0121\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0119 - val_loss: 0.0178\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0241 - val_loss: 0.0206\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0290 - val_loss: 0.0173\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0245 - val_loss: 0.0126\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0168 - val_loss: 0.0095\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0106 - val_loss: 0.0085\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0067 - val_loss: 0.0092\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0047 - val_loss: 0.0109\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.413e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0060 - val_loss: 0.0086\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0055 - val_loss: 0.0088\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0054 - val_loss: 0.0089\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0053 - val_loss: 0.0090\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0051 - val_loss: 0.0091\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0049 - val_loss: 0.0092\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0049 - val_loss: 0.0093\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0047 - val_loss: 0.0095\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.413e-06\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0063 - val_loss: 0.0085\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-13-2021--20--51-E2E_LSTM_ALPHA1.0_SD100998_DropOut0-108Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.003287656755709773\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.3713 - val_loss: 0.4964\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.3692 - val_loss: 0.4939\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3672 - val_loss: 0.4914\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3652 - val_loss: 0.4889\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3631 - val_loss: 0.4864\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3612 - val_loss: 0.4840\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3592 - val_loss: 0.4816\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3572 - val_loss: 0.4793\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3553 - val_loss: 0.4769\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3534 - val_loss: 0.4746\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3515 - val_loss: 0.4723\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3496 - val_loss: 0.4701\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.500e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3477 - val_loss: 0.4671\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3453 - val_loss: 0.4641\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3429 - val_loss: 0.4612\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3405 - val_loss: 0.4582\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3381 - val_loss: 0.4552\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3357 - val_loss: 0.4522\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3333 - val_loss: 0.4492\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3308 - val_loss: 0.4462\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3285 - val_loss: 0.4432\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3260 - val_loss: 0.4401\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3236 - val_loss: 0.4370\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3212 - val_loss: 0.4340\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 8.450e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3187 - val_loss: 0.4299\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3155 - val_loss: 0.4259\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3124 - val_loss: 0.4219\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3093 - val_loss: 0.4179\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3062 - val_loss: 0.4139\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3031 - val_loss: 0.4101\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3001 - val_loss: 0.4062\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2972 - val_loss: 0.4024\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2944 - val_loss: 0.3987\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2915 - val_loss: 0.3949\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2886 - val_loss: 0.3912\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2857 - val_loss: 0.3875\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.099e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2828 - val_loss: 0.3826\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.2790 - val_loss: 0.3778\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2752 - val_loss: 0.3729\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2714 - val_loss: 0.3681\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2674 - val_loss: 0.3631\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2634 - val_loss: 0.3582\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2594 - val_loss: 0.3532\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2553 - val_loss: 0.3481\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2514 - val_loss: 0.3430\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2472 - val_loss: 0.3378\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2430 - val_loss: 0.3325\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2387 - val_loss: 0.3271\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.428e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2342 - val_loss: 0.3199\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2285 - val_loss: 0.3124\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2227 - val_loss: 0.3048\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2168 - val_loss: 0.2969\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2105 - val_loss: 0.2889\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2043 - val_loss: 0.2807\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1979 - val_loss: 0.2724\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1914 - val_loss: 0.2640\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1850 - val_loss: 0.2554\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1782 - val_loss: 0.2466\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1714 - val_loss: 0.2378\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1644 - val_loss: 0.2288\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.856e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1574 - val_loss: 0.2170\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1484 - val_loss: 0.2049\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1392 - val_loss: 0.1928\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1299 - val_loss: 0.1804\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1200 - val_loss: 0.1676\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1102 - val_loss: 0.1544\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1000 - val_loss: 0.1408\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0895 - val_loss: 0.1267\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0789 - val_loss: 0.1124\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0681 - val_loss: 0.0977\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0574 - val_loss: 0.0830\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0464 - val_loss: 0.0684\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.413e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0361 - val_loss: 0.0503\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0244 - val_loss: 0.0348\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0157 - val_loss: 0.0245\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0166 - val_loss: 0.0281\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0288 - val_loss: 0.0305\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0335 - val_loss: 0.0274\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0294 - val_loss: 0.0226\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0211 - val_loss: 0.0193\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0150 - val_loss: 0.0179\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0116 - val_loss: 0.0182\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0087 - val_loss: 0.0196\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.413e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0109 - val_loss: 0.0179\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0109 - val_loss: 0.0178\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0110 - val_loss: 0.0178\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0108 - val_loss: 0.0179\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0095 - val_loss: 0.0179\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0101 - val_loss: 0.0180\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0101 - val_loss: 0.0180\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0100 - val_loss: 0.0181\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0093 - val_loss: 0.0182\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0091 - val_loss: 0.0183\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0095 - val_loss: 0.0183\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0087 - val_loss: 0.0184\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-13-2021--21--11-E2E_LSTM_ALPHA10.0_SD100998_DropOut0-96Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.003287656755709773\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 500ms/step - loss: 0.3734 - val_loss: 0.5015\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.3714 - val_loss: 0.4991\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3697 - val_loss: 0.4968\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3679 - val_loss: 0.4945\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3655 - val_loss: 0.4921\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3636 - val_loss: 0.4899\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3618 - val_loss: 0.4876\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.3596 - val_loss: 0.4854\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3580 - val_loss: 0.4832\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3560 - val_loss: 0.4811\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3542 - val_loss: 0.4790\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3523 - val_loss: 0.4769\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.500e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3502 - val_loss: 0.4741\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3483 - val_loss: 0.4714\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3462 - val_loss: 0.4687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3442 - val_loss: 0.4660\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3416 - val_loss: 0.4632\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3391 - val_loss: 0.4605\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3368 - val_loss: 0.4578\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3342 - val_loss: 0.4550\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3325 - val_loss: 0.4523\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.3300 - val_loss: 0.4495\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3277 - val_loss: 0.4467\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3253 - val_loss: 0.4439\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 8.450e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3225 - val_loss: 0.4402\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.3202 - val_loss: 0.4365\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3173 - val_loss: 0.4328\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3148 - val_loss: 0.4290\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3113 - val_loss: 0.4253\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3080 - val_loss: 0.4217\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3050 - val_loss: 0.4180\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3019 - val_loss: 0.4144\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2999 - val_loss: 0.4109\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2968 - val_loss: 0.4073\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2942 - val_loss: 0.4038\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2911 - val_loss: 0.4004\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.099e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2878 - val_loss: 0.3959\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2851 - val_loss: 0.3915\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2817 - val_loss: 0.3870\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2787 - val_loss: 0.3826\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2743 - val_loss: 0.3781\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2704 - val_loss: 0.3736\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2664 - val_loss: 0.3692\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2624 - val_loss: 0.3647\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2597 - val_loss: 0.3602\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2554 - val_loss: 0.3556\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2517 - val_loss: 0.3510\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2473 - val_loss: 0.3463\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.428e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2426 - val_loss: 0.3401\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2386 - val_loss: 0.3338\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2335 - val_loss: 0.3272\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2288 - val_loss: 0.3203\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2220 - val_loss: 0.3132\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2160 - val_loss: 0.3059\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2096 - val_loss: 0.2985\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2030 - val_loss: 0.2909\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1983 - val_loss: 0.2832\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1912 - val_loss: 0.2754\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1851 - val_loss: 0.2674\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1777 - val_loss: 0.2594\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.856e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1701 - val_loss: 0.2490\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.1632 - val_loss: 0.2386\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1554 - val_loss: 0.2281\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1481 - val_loss: 0.2175\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1369 - val_loss: 0.2067\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1286 - val_loss: 0.1958\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1195 - val_loss: 0.1848\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1099 - val_loss: 0.1737\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1016 - val_loss: 0.1626\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0917 - val_loss: 0.1518\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0849 - val_loss: 0.1414\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0738 - val_loss: 0.1317\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.413e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0661 - val_loss: 0.1209\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0609 - val_loss: 0.1129\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0588 - val_loss: 0.1090\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0589 - val_loss: 0.1087\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0513 - val_loss: 0.1105\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0624 - val_loss: 0.1109\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0659 - val_loss: 0.1088\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0643 - val_loss: 0.1050\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0542 - val_loss: 0.1010\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0485 - val_loss: 0.0978\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0486 - val_loss: 0.0954\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0396 - val_loss: 0.0940\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 3.137e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0369 - val_loss: 0.0933\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0390 - val_loss: 0.0934\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0405 - val_loss: 0.0938\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0426 - val_loss: 0.0941\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0375 - val_loss: 0.0940\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0388 - val_loss: 0.0935\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0372 - val_loss: 0.0927\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0356 - val_loss: 0.0916\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0376 - val_loss: 0.0903\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0350 - val_loss: 0.0890\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0355 - val_loss: 0.0879\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0319 - val_loss: 0.0869\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.079e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0289 - val_loss: 0.0859\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0333 - val_loss: 0.0853\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0351 - val_loss: 0.0850\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0383 - val_loss: 0.0849\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0331 - val_loss: 0.0848\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0343 - val_loss: 0.0846\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0327 - val_loss: 0.0841\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0312 - val_loss: 0.0835\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0341 - val_loss: 0.0829\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0315 - val_loss: 0.0823\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0326 - val_loss: 0.0819\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0296 - val_loss: 0.0817\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 5.302e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0272 - val_loss: 0.0813\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0319 - val_loss: 0.0809\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0334 - val_loss: 0.0804\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0357 - val_loss: 0.0797\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0309 - val_loss: 0.0788\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0307 - val_loss: 0.0780\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0285 - val_loss: 0.0773\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0272 - val_loss: 0.0766\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0299 - val_loss: 0.0760\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0276 - val_loss: 0.0755\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0291 - val_loss: 0.0748\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0255 - val_loss: 0.0741\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.893e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0243 - val_loss: 0.0732\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0271 - val_loss: 0.0721\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0289 - val_loss: 0.0711\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0305 - val_loss: 0.0701\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0256 - val_loss: 0.0690\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0264 - val_loss: 0.0678\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0245 - val_loss: 0.0666\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0228 - val_loss: 0.0654\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0250 - val_loss: 0.0642\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0219 - val_loss: 0.0631\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0231 - val_loss: 0.0616\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0199 - val_loss: 0.0603\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 8.961e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0204 - val_loss: 0.0590\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0209 - val_loss: 0.0585\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0233 - val_loss: 0.0573\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0243 - val_loss: 0.0561\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0214 - val_loss: 0.0584\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0222 - val_loss: 0.0562\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0198 - val_loss: 0.0539\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0191 - val_loss: 0.0535\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0221 - val_loss: 0.0556\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0182 - val_loss: 0.0582\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0209 - val_loss: 0.0545\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0173 - val_loss: 0.0518\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.165e-03\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0198 - val_loss: 0.0515\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0187 - val_loss: 0.0538\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0208 - val_loss: 0.0546\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0221 - val_loss: 0.0526\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0183 - val_loss: 0.0517\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0195 - val_loss: 0.0512\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0174 - val_loss: 0.0510\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0164 - val_loss: 0.0508\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0189 - val_loss: 0.0512\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0161 - val_loss: 0.0514\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0176 - val_loss: 0.0499\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0151 - val_loss: 0.0490\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.514e-03\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0171 - val_loss: 0.0486\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0164 - val_loss: 0.0490\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0183 - val_loss: 0.0477\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0190 - val_loss: 0.0463\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0155 - val_loss: 0.0467\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0170 - val_loss: 0.0459\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0148 - val_loss: 0.0440\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0137 - val_loss: 0.0434\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0162 - val_loss: 0.0450\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0138 - val_loss: 0.0455\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0151 - val_loss: 0.0416\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0122 - val_loss: 0.0414\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.969e-03\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0146 - val_loss: 0.0433\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0141 - val_loss: 0.0406\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0144 - val_loss: 0.0400\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0159 - val_loss: 0.0420\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0119 - val_loss: 0.0400\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0130 - val_loss: 0.0407\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0138 - val_loss: 0.0394\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0114 - val_loss: 0.0391\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0117 - val_loss: 0.0385\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0106 - val_loss: 0.0384\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0122 - val_loss: 0.0382\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0101 - val_loss: 0.0374\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.559e-03\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0115 - val_loss: 0.0376\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0112 - val_loss: 0.0376\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0115 - val_loss: 0.0374\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0123 - val_loss: 0.0397\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0116 - val_loss: 0.0371\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0121 - val_loss: 0.0360\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0099 - val_loss: 0.0355\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0090 - val_loss: 0.0339\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0107 - val_loss: 0.0353\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0089 - val_loss: 0.0333\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0101 - val_loss: 0.0325\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0093 - val_loss: 0.0362\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.559e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0107 - val_loss: 0.0322\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0104 - val_loss: 0.0320\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0096 - val_loss: 0.0325\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0095 - val_loss: 0.0336\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0082 - val_loss: 0.0347\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0100 - val_loss: 0.0348\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0099 - val_loss: 0.0339\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0088 - val_loss: 0.0327\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0086 - val_loss: 0.0320\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0078 - val_loss: 0.0317\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0090 - val_loss: 0.0316\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0078 - val_loss: 0.0316\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 3.327e-04\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0089 - val_loss: 0.0317\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0088 - val_loss: 0.0322\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0091 - val_loss: 0.0328\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0094 - val_loss: 0.0330\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0079 - val_loss: 0.0328\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0091 - val_loss: 0.0321\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0084 - val_loss: 0.0314\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0078 - val_loss: 0.0310\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0086 - val_loss: 0.0308\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0079 - val_loss: 0.0308\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0084 - val_loss: 0.0312\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0067 - val_loss: 0.0318\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-13-2021--21--34-E2E_LSTM_ALPHA100.0_SD100998_DropOut0-216Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0096978991738973\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 0.3942 - val_loss: 0.5522\n",
      "Epoch 2/12\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.3929 - val_loss: 0.5510\n",
      "Epoch 3/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3934 - val_loss: 0.5498\n",
      "Epoch 4/12\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3935 - val_loss: 0.5486\n",
      "Epoch 5/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3890 - val_loss: 0.5473\n",
      "Epoch 6/12\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.3872 - val_loss: 0.5460\n",
      "Epoch 7/12\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3865 - val_loss: 0.5447\n",
      "Epoch 8/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3827 - val_loss: 0.5435\n",
      "Epoch 9/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3846 - val_loss: 0.5423\n",
      "Epoch 10/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3815 - val_loss: 0.5410\n",
      "Epoch 11/12\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3805 - val_loss: 0.5398\n",
      "Epoch 12/12\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3783 - val_loss: 0.5386\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.500e-05\n",
      "Epoch 1/12\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3752 - val_loss: 0.5371\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-172886554415>\", line 24, in <module>\n",
      "    DMJ.train_model_loop(epoch_batches=12)\n",
      "  File \"C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\tensorflow_models.py\", line 613, in train_model_loop\n",
      "    self.history = self.model.fit(inputs_train,\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1103, in fit\n",
      "    callbacks.on_train_batch_end(end_step, logs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 440, in on_train_batch_end\n",
      "    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 289, in _call_batch_hook\n",
      "    self._call_batch_end_hook(mode, batch, logs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 309, in _call_batch_end_hook\n",
      "    self._call_batch_hook_helper(hook_name, batch, logs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 342, in _call_batch_hook_helper\n",
      "    hook(batch, logs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 2171, in on_train_batch_end\n",
      "    self._stop_trace()\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 2200, in _stop_trace\n",
      "    profiler.stop()\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\profiler\\profiler_v2.py\", line 144, in stop\n",
      "    _profiler.export_to_tb()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Maxwell\\AppData\\Local\\Programs\\Python\\Python38\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Maxwell\\AppData\\Local\\Programs\\Python\\Python38\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Maxwell\\AppData\\Local\\Programs\\Python\\Python38\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Maxwell\\AppData\\Local\\Programs\\Python\\Python38\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\Maxwell\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ntpath.py\", line 647, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-172886554415>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Have it train as much as it can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mDMJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\tensorflow_models.py\u001b[0m in \u001b[0;36mtrain_model_loop\u001b[1;34m(self, epoch_batches, learning_rate, one_loop_only)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m             self.history = self.model.fit(inputs_train,\n\u001b[0m\u001b[0;32m    614\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   2170\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_tracing\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_train_batch\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2171\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_stop_trace\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   2199\u001b[0m         \u001b[0msummary_ops_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'batch_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2200\u001b[1;33m     \u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2201\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_tracing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\tensorflow\\python\\profiler\\profiler_v2.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(save)\u001b[0m\n\u001b[0;32m    143\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0m_profiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_to_tb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2047\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1435\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1436\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             )\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1193\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxwell\\pycharmprojects\\ml-class\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "alpha = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "alpha = [1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# alpha.sort(reverse=True)\n",
    "\n",
    "model_folder = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "results_folder = './ignorable_data/strategies/'\n",
    "predictions_folder = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "data_block_folder = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned', model_folder, reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "for a in alpha: \n",
    "    # Reset the training object to get rid of old data\n",
    "    DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned', model_folder, reload=False)\n",
    "\n",
    "    # Create the model using parameters we're tuning\n",
    "    DMJ._generate_model(model_type='lstm', loss_function='rank_loss', activation='leaky_relu', hidden_units=64, true_random=True, alpha=a, beta=1)\n",
    "\n",
    "    # Have it train as much as it can\n",
    "    DMJ.train_model_loop(epoch_batches=12)\n",
    "\n",
    "    # Save the model with a tag\n",
    "    DMJ.save_model(tag=f'E2E_LSTM_ALPHA{a}_SD100998_DropOut0_sliding10')\n",
    "    # DMJ.save_model(tag=f'E2E_LSTM_ValSet_TFMSE_SD100998_expanding_window')\n",
    "\n",
    "    # Reset the training object to get rid of old data\n",
    "    GP = Graph_Predictions(model_folder, results_folder, 'x_val', DMJ)\n",
    "\n",
    "    # Generate the prediction file\n",
    "    GP.generate_validation_prediction_json(DMJ.model_name, predictions_folder, neural_net_type='lstm', sliding_window=10)\n",
    "\n",
    "    # Create the diagnostics file for the most recently saved model\n",
    "    GP.generate_model_diagnostics(predictions_folder + GP.model_name, datablock_folder=data_block_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = './ignorable_data/alpha_1000_beta_0.01/models'\n",
    "model_folder = './ignorable_data/models/'\n",
    "model_name = '01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--'\n",
    "\n",
    "new_name = '01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--400_slidewindow'\n",
    "\n",
    "directory = \"./ignorable_data/alpha_1000_beta_0.01/models/\"\n",
    "predictions_folder = './ignorable_data/alpha_1000_beta_0.01/models/'\n",
    "\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned', model_folder, reload=False)\n",
    "\n",
    "# For training over sliding window\n",
    "# Reset the training object to get rid of old data\n",
    "GP = Graph_Predictions(model_folder, \"./ignorable_data/strategies/RL_validation_strategies\", DMJ)\n",
    "\n",
    "# Generate the prediction file\n",
    "GP.generate_prediction_json(model_name, directory, neural_net_type='gcn', name_override=new_name, sliding_window=800)\n",
    "\n",
    "# Create the diagnostics file for the most recently saved model\n",
    "GP.generate_model_diagnostics(predictions_folder + GP.model_name, datablock_folder='./ignorable_data/alpha_1000_beta_0.01/data_blocks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.history.history['val_loss']\n",
    "DMJ.history.history['lr']\n",
    "DMJ.epochs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the  model\n",
    "model = DMJ.train_model(epochs=500)\n",
    "# model = DMJ.train_model(model, data_splits['x_train'], data_splits['y_train'], data_splits['x_val'], data_splits['y_val'], epochs=50, learning_rate=5e-5, gcn_matrix=DMJ.Normalized_Adjacency_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.save_model(tag='E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xGP = Graph_Predictions(\"./models\", \"./strategies\", DMJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=False, name_override='test_')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=False, average=10, name_override='LSTM-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=True, name_override='LSTM-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, name_override='LSTM-RankLoss-50Epoch')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='LSTM-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='LSTM-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--31--GCN1-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN1-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--31--GCN1-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN1-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--34--GCN1-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN1-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--34--GCN1-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN1-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--37--GCN2-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN2-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--37--GCN2-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN2-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--39--GCN2-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN2-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--39--GCN2-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN2-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN3-MSE-50Epoch-AvoidFall-2ndBest')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN3-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--20--46--GCN3-MSE-Ratio+1--50Epochs--mse-Loss--64-HU--', avoid_fall=True, average=1, name_override='GCN3-MSE-200Epoch-Ratio+1')\n",
    "# GP.strategy_ratio_gcn('11-22-2020--20--43--GCN3-RankLoss-Ratio+1--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, average=1, name_override='GCN3-RankLoss-200Epoch-Ratio+1')\n",
    "\n",
    "'''New prediction memory storage'''\n",
    "# GP.generate_prediction_json('11-22-2020--15--01--LSTM-MSE--50Epochs--mse-Loss--64-HU--', neural_net_type='lstm')\n",
    "# GP.generate_prediction_json('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', neural_net_type='gcn')\n",
    "\n",
    "'''Seperately trained LSTM combo'''\n",
    "# GP.generate_prediction_json('01-04-2021--14--20--SEP_LSTM_GCN3-1e-5LR--70Epochs--mse-Loss--64-HU--', neural_net_type='gcn')\n",
    "\n",
    "''''''\n",
    "GP.generate_prediction_json('02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--', neural_net_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strat_name = '02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--_PM'\n",
    "\n",
    "# Testing the PM file feature\n",
    "avg = [1, 5, 20, 50, 100, 200]\n",
    "avg = [1]\n",
    "for a in avg:\n",
    "#     GP.prediction_json_strategy_max_entities(strat_name, average=a, avoid_fall=False, name_override=strat_name+ f'{a}AVG')\n",
    "    GP.prediction_json_strategy_determine_best(strat_name, average=a, avoid_fall=False, name_override=strat_name+ f'{a}AVG_Correct_BuyDay_plus1')\n",
    "    GP.save_results()\n",
    "# Testing the mse tracking feature\n",
    "# GP.prediction_json_mse('01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--_PM')\n",
    "GP.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.generate_model_diagnostics('02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--_PM', datablock_folder='RL_validation_set')\n",
    "\n",
    "'01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--_PM'\n",
    "255025890000.0\n",
    "0.012249682697757544\n",
    "0.7766990291262136\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GP.generate_upper_lower_avg_bounds()\n",
    "\n",
    "GP.display_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP.strategy_ratio_gcn('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN3-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN3-RankLoss-50Epoch-AvoidFall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP.save_results('./strategies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import GridBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "bar = widgets.IntProgress(min=0, max=10, description='Loading:', bar_style='info')\n",
    "display(bar)\n",
    "\n",
    "for i in range(11):\n",
    "    time.sleep(0.2)\n",
    "    bar.value = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "widgets.Text(\n",
    "    value='Loading',\n",
    "    description='',\n",
    "    disabled=True\n",
    "    layout=L\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the loading bar by initializing it\n",
    "nam_bar = widgets.IntProgress(min=0, max=5, value=0, description='Loading Normalized Adjacency Matrix:',\n",
    "                              layout=widgets.Layout(width='auto'))\n",
    "text = widgets.Text(value='Loading', description='', disabled=True, layout=widgets.Layout(width='auto'))\n",
    "\n",
    "test = widgets.GridBox(children=[text, nam_bar], layout=widgets.Layout(width='auto'))\n",
    "\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.amax(GP.rr_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(GP.rr_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.rr_test[476,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(GP.rr_test[5, :], GP.rr_test[100, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def swap_random(seq):\n",
    "    idx = range(len(seq))\n",
    "    i1, i2 = random.sample(idx, 2)\n",
    "    seq[i1], seq[i2] = seq[i2], seq[i1]\n",
    "\n",
    "trials_RL = []\n",
    "for e in range(100,300):\n",
    "    RL = []\n",
    "    for t in range(300):\n",
    "        A = []; B = [];\n",
    "        for l in range(e):\n",
    "            n = random.uniform(-1, 1)\n",
    "            A.append(n)\n",
    "            B.append(n)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        \n",
    "        return_ratio = tf.constant(A, shape=(len(A), 1))\n",
    "        ground_truth = tf.constant(B, shape=(len(B), 1))\n",
    "\n",
    "        ###############################################################\n",
    "        # Create an array of all_ones so that we can calculate all permutations of subtractions\n",
    "        all_ones = tf.ones([len(return_ratio), 1], dtype=tf.float32)\n",
    "\n",
    "        # Creates a N x N matrix with every predicted return ratio for each company subtracted with every other\n",
    "        # company\n",
    "        pred_dif = tf.math.subtract(\n",
    "            tf.matmul(return_ratio, all_ones, transpose_b=True),\n",
    "            tf.matmul(all_ones, return_ratio, transpose_b=True)\n",
    "        )\n",
    "\n",
    "        # Creates an N x N matrix containing every actual return ratio for each company subtracted with every other\n",
    "        # company By switching the order of the all_ones matricies and the actual prices, a negative sign is introduced\n",
    "        # When RELU is applied later, correct predictions will not affect loss while incorrect predictions will affect\n",
    "        # loss depending on how incorrect the prediction was\n",
    "        actual_dif = tf.math.subtract(\n",
    "            tf.matmul(all_ones, ground_truth, transpose_b=True),\n",
    "            tf.matmul(ground_truth, all_ones, transpose_b=True)\n",
    "        )\n",
    "\n",
    "        # Using the above two qualities, the algorithm can be punished for incorrectly calculating when a company is\n",
    "        # doing better than another company Reduces the mean across each dimension until only 1 value remains\n",
    "        rank_loss = tf.reduce_mean(\n",
    "            # Takes if a given value is >0, it is kept, otherwise, it becomes 0\n",
    "            tf.nn.relu(\n",
    "                # Multiplies all of the\n",
    "                tf.multiply(pred_dif, actual_dif)\n",
    "            )\n",
    "        )\n",
    "        RL.append(rank_loss)\n",
    "    trials_RL.append(np.mean(RL))\n",
    "\n",
    "plt.plot(trials_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [0.1, 0.5, 0.2, 0.3]\n",
    "print(list(zip(range(4), r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [100, 300, 200, 400]\n",
    "predictions = list(zip(range(len(predictions)), predictions))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normal Rank\n",
    "A = list(range(1, 10001))\n",
    "B = [i**(-1) for i in A]\n",
    "print(np.mean(A))\n",
    "print(np.mean(B)**-1)\n",
    "\n",
    "\n",
    "'000_Avg_RR.p'\n",
    "'000_Highest_RR_Possible.p'\n",
    "'000_Lowest_RR_Possible.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import similaritymeasures as sm\n",
    "import numpy as np\n",
    "\n",
    "# Generate random experimental data\n",
    "n = 5\n",
    "x = list(range(n))\n",
    "y = [11, 26, 26, 11, -60]\n",
    "exp_data = np.zeros((n, 2))\n",
    "exp_data[:, 0] = x\n",
    "exp_data[:, 1] = y\n",
    "\n",
    "# Generate random numerical data\n",
    "x = list(range(n))\n",
    "y = [1, 2, 30, 4, -9000]\n",
    "num_data = np.zeros((n, 2))\n",
    "num_data[:, 0] = x\n",
    "num_data[:, 1] = y\n",
    "\n",
    "area = sm.area_between_two_curves(exp_data, num_data)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_name = 'uhhhh.json'\n",
    "# If the .json file was already attached, this will fix the problem\n",
    "pm_name = pm_name.split('.json')\n",
    "pm_name = pm_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
