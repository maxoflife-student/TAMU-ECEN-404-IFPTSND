{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Tensorflow 2.0 Error Messages\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import *\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import *\n",
    "\n",
    "import random\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").addHandler(logging.NullHandler(logging.ERROR))\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from bqplot import (\n",
    "    OrdinalScale, LinearScale, Bars, Lines, Axis, Figure, PanZoom, Toolbar\n",
    ")\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pickle\n",
    "\n",
    "lrelu = lambda x: tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "class Ein_Multiply(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(Ein_Multiply, self).__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.einsum('ij,ik->ik', input[0], input[1])\n",
    "\n",
    "class Ein_Multiply_Add_None(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(Ein_Multiply_Add_None, self).__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        # Multiply the output embeddings with the GCN NAM\n",
    "        # Then add another None dimension to satisfy the LSTM requirements\n",
    "        o = tf.einsum('ij,ik->ik', input[0], input[1])[None, :, :]\n",
    "\n",
    "        # Swap the None Dimensions\n",
    "        return tf.einsum(\"ij...->ji...\", o)\n",
    "    \n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_folder = Path(r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\ECEN403_Data\\2013-01-01 - NASDAQ - Cleaned\")\n",
    "\n",
    "list_of_data_csv_files = [f for f in listdir(data_folder) if isfile(join(data_folder, f))]\n",
    "\n",
    "# list_of_data_csv_files = [i[7:].replace('_1.csv','') for i in list_of_data_csv_files]\n",
    "\n",
    "# Number of companies\n",
    "N = len(list_of_data_csv_files)\n",
    "\n",
    "# In this example we're cleaning up the csv files that are being read.\n",
    "# We don't need the first column or the last row of the provided data set\n",
    "df = pd.read_csv(data_folder / list_of_data_csv_files[0], header=None, usecols=[1, 2, 3, 4, 5])\n",
    "df.columns = ['Close', '5D MA', '10D MA', '20D MA', '30D MA']\n",
    "\n",
    "# Removethe bottom 5 rows and top 1 row to avoid -1234 empty spaces\n",
    "df = df[1:-5]\n",
    "\n",
    "# Number of sequences\n",
    "S = (len(df.index))\n",
    "\n",
    "# Number of features (columns)\n",
    "D = len(df.columns)\n",
    "\n",
    "# 3 Dimensional Tensor of the input data: (Companies, Sequences, Features)\n",
    "XX_t = []\n",
    "\n",
    "# Labels for the input data\n",
    "# (Should be the closing price of each stock, but time shifted by 1 day)\n",
    "YY_t = []\n",
    "\n",
    "for stock in range(0, N):\n",
    "    # Read its CSV values, skipping the index column and the last row which has -1234 as a value\n",
    "    df = pd.read_csv(data_folder / list_of_data_csv_files[stock], header=None, usecols=[1, 2, 3, 4, 5], engine='python')\n",
    "    df.columns = ['Close', '5D MA', '10D MA', '20D MA', '30D MA']\n",
    "    \n",
    "    # Removethe bottom 5 rows and top 1 row to avoid -1234 empty spaces\n",
    "    df = df[1:-5]\n",
    "    \n",
    "    '''All stocks that contained missing/NaN values have already been removed from the dataset'''\n",
    "    \n",
    "    # Append the 2x2 tensor of each company's features through time to the main tensor\n",
    "    XX_t.append(df[0:].values)\n",
    "    YY_t.append(df['Close'].values)\n",
    "    # print(f'Loading: {stock}, ', end='')\n",
    "\n",
    "\n",
    "# Shift the Y values so that the future is time shifted by 1 day\n",
    "for i in range(0, len(YY_t)):\n",
    "    YY_t[i] = YY_t[i][1:]\n",
    "\n",
    "# Remove the last X value since we don't have a predictor for it\n",
    "for i in range(0, len(XX_t)):\n",
    "    XX_t[i] = XX_t[i][0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of list of lists into a tensor object\n",
    "XX_tf = tf.constant(XX_t, dtype='float32')\n",
    "YY_tf = tf.constant(YY_t, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([881, 745, 5]) TensorShape([881, 185, 5]) TensorShape([881, 309, 5])\r\n"
     ]
    }
   ],
   "source": [
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "    \n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[0] = new_splits[0] + (total - sum(new_splits))\n",
    "    \n",
    "    return new_splits\n",
    "\n",
    "# Split the data into 3 categories\n",
    "x_train, x_val, x_test = tf.split(XX_tf, split_windows(XX_tf.shape[1], [60, 15, 25]), axis=1)\n",
    "y_train, y_val, y_test = tf.split(YY_tf, split_windows(YY_tf.shape[1], [60, 15, 25]), axis=1)\n",
    "\n",
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list, return the max index of that list\n",
    "# If given rank, then it returns the rank-th highest index\n",
    "def max_index(l, rank=0):\n",
    "    items = []\n",
    "    for idx, value in enumerate(l):\n",
    "        items.append((idx, value))\n",
    "    \n",
    "    # Sort the ratios and their indexes from high to low\n",
    "    items.sort(key= lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return(items[rank][0])\n",
    "\n",
    "# Given a list, return the min index of that list\n",
    "def min_index(l):\n",
    "    min_value = l[0]\n",
    "    min_i = 0\n",
    "    for i, v in enumerate(l):\n",
    "        if min_value > v:\n",
    "            min_value = v\n",
    "            min_i = i\n",
    "    return min_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-c77a06ad04fe>:84: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  degree[i] = 1.0 / degree[i]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Create a list of the companies that are actually being trained over\n",
    "# Only works with the EXPLICIT file format that we've currently provided\n",
    "companies = [i[7:].replace('_1.csv','') for i in list_of_data_csv_files]\n",
    "\n",
    "# Load in the JSON file as a DICT containing the industry relationships\n",
    "with open(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\Temporal_Relational_Stock_Ranking-master\\data\\RELATIONS\\sector_industry\\NASDAQ_industry_ticker.json') as read_file:\n",
    "    given_industry_relations = json.load(read_file)\n",
    "\n",
    "# Create a new dictionary of industry relationships containing only the companies we've trained over\n",
    "new_industry_relations = {}\n",
    "companies = set(companies)\n",
    "for key, value in given_industry_relations.items():\n",
    "    value = set(value)\n",
    "    value.intersection_update(companies)\n",
    "    value = list(value)\n",
    "    \n",
    "    # Only count relations that contain more than 1 stock\n",
    "    if len(value) > 1:\n",
    "        new_industry_relations[key] = value\n",
    "\n",
    "# Create a json file containing the new relations to be used in other programs\n",
    "filename = 'updated_relations.json'\n",
    "with open(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\Temporal_Relational_Stock_Ranking-master\\data\\RELATIONS\\sector_industry\\\\' + filename, 'w') as write_file:\n",
    "    json.dump(new_industry_relations, write_file)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that companies is in the proper order\n",
    "companies = list(companies)\n",
    "companies.sort()\n",
    "\n",
    "'''Given an item and a list, returns the index of that where that item is stored in a list'''\n",
    "def return_idx(item, l):\n",
    "    i = 0\n",
    "    for thing in l:\n",
    "        if item == thing:\n",
    "            return i\n",
    "        else:\n",
    "            i += 1\n",
    "    # If it's not in the list, then it's failed\n",
    "    return -1\n",
    "\n",
    "# Iterate through each company ticker and replace it with a tuple that contains its index and ticker\n",
    "for key, value in new_industry_relations.items():\n",
    "    new_value = []\n",
    "    for v in value:\n",
    "        new_value.append((return_idx(v, companies), v))\n",
    "    new_industry_relations[key] = new_value\n",
    "\n",
    "# Iterate through each industry relationship and create an N x N adjacency matrix\n",
    "# Combine them all to create the final adjacency matrix in the same format as Paper #2\n",
    "RR_t = []\n",
    "for sector in new_industry_relations.keys():\n",
    "    # Create an empty relationship matrix\n",
    "    all_zeroes = tf.zeros([len(companies), len(companies)])\n",
    "    relation_slice = all_zeroes.numpy()\n",
    "    \n",
    "    # Gather all the companies that exist in this sector\n",
    "    siblings = new_industry_relations[sector]\n",
    "    for i in siblings:\n",
    "        for j in siblings:\n",
    "            relation_slice[i[0], j[0]] = 1\n",
    "            relation_slice[j[0], i[0]] = 1\n",
    "    RR_t.append(relation_slice)\n",
    "\n",
    "RR_tf = tf.constant(RR_t)\n",
    "\n",
    "RR_tf = tf.transpose(RR_tf)\n",
    "\n",
    "relation_encoding = RR_tf.numpy()\n",
    "# relation_encoding.shape = [1024, 1024, 97]\n",
    "\n",
    "rel_shape = [relation_encoding.shape[0], relation_encoding.shape[1]]\n",
    "\n",
    "mask_flags = np.equal(np.zeros(rel_shape, dtype=int), np.sum(relation_encoding, axis=2))\n",
    "\n",
    "# N x N\n",
    "ajacent = np.where(mask_flags, np.zeros(rel_shape, dtype=float), np.ones(rel_shape, dtype=float))\n",
    "\n",
    "# N x 1\n",
    "degree = np.sum(ajacent, axis=0)\n",
    "for i in range(len(degree)):\n",
    "    degree[i] = 1.0 / degree[i]\n",
    "np.sqrt(degree, degree)\n",
    "deg_neg_half_power = np.diag(degree)\n",
    "\n",
    "GCN_mat = np.dot(np.dot(deg_neg_half_power, ajacent), deg_neg_half_power)\n",
    "\n",
    "# Some relation values in my degree matrix were nan, let's try replacing with 0\n",
    "GCN_mat = np.nan_to_num(GCN_mat)\n",
    "GCN_mat = tf.constant(GCN_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCompare():\n",
    "    def __init__(self, test_set, increment):\n",
    "        self.test_set = test_set.numpy()\n",
    "        self.starting_investment = 2 * increment\n",
    "        self.increment = increment\n",
    "        \n",
    "        self.num_companies = int(self.test_set.shape[0])\n",
    "        self.num_days = int(self.test_set.shape[1])\n",
    "        \n",
    "        # Strategy_Name: [Daily portfolio value]\n",
    "        self.strategy_results = {}\n",
    "        \n",
    "        # The amount of testing data we want the models to see before actually starting to spend money\n",
    "        self.lag = 1\n",
    "    \n",
    "    def save_results(self, path):\n",
    "        for key, value in self.strategy_results.items():\n",
    "            pickle.dump(value, open (path + fr'\\{key}.p', 'wb'))\n",
    "    \n",
    "    def simple_graph(self, sel_comp, show_all):\n",
    "        \n",
    "        # Create some colors for the graphed lines to cycle through\n",
    "        colors_list = [hex(0x8A0808 - i *0xB0D0) for i in range(25)]\n",
    "        colors_list = [str(i) for i in colors_list]\n",
    "        colors_list = ['#' + i[2:] for i in colors_list]\n",
    "        shuffle(colors_list)\n",
    "    \n",
    "        strategy_totals = self.strategy_results[sel_comp]\n",
    "\n",
    "        if not show_all:\n",
    "            x_data = list(range(0, len(strategy_totals)))\n",
    "        if show_all:\n",
    "            lengths = [len(i) for i in self.strategy_results.values()]\n",
    "            x_data = list(range(0, max(lengths)))\n",
    "        y_data = list(strategy_totals)\n",
    "\n",
    "        x_scale = LinearScale()\n",
    "        y_scale = LinearScale()\n",
    "\n",
    "        ax_x = Axis(scale=x_scale, label='Trading Days', grid_lines='solid')\n",
    "        ax_y = Axis(scale=y_scale, label='Portfolio Total', orientation='vertical', label_offset='50px', )\n",
    "\n",
    "        if not show_all:\n",
    "            line = [Lines(labels=[sel_comp], x=x_data, y=y_data, scales={'x': x_scale, 'y': y_scale}, display_legend=True)]\n",
    "        else:\n",
    "            values = [i for i in  self.strategy_results.values()]\n",
    "            keys = [i for i in  self.strategy_results.keys()]\n",
    "            line = [Lines(labels=[keys[i]], x=x_data, y=values[i], scales={'x': x_scale, 'y': y_scale}, colors=[colors_list[i]], display_legend=True) for i in range(len(values))]\n",
    "\n",
    "        panzoom= PanZoom(scales={'x': [x_scale], 'y': [y_scale]})\n",
    "\n",
    "        fig = Figure(marks=line, axes=[ax_x, ax_y], title='Total USD Earnings over Time', colors=['red'], legend_location='top-left', \\\n",
    "                     fit_margin={'top':100, 'bottom':100, 'left':100, 'right':100})\n",
    "\n",
    "        toolbar = Toolbar(figure = fig)\n",
    "\n",
    "        display(fig, toolbar)\n",
    "\n",
    "    \n",
    "    def graph(self):\n",
    "        graphable_strats = [key for key in self.strategy_results]\n",
    "        \n",
    "        comp_drop = widgets.Dropdown(\n",
    "            options = graphable_strats,\n",
    "            description = 'Strategy:'\n",
    "        )\n",
    "        \n",
    "        show_all = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description = 'Show All',\n",
    "            disabled=False,\n",
    "        )\n",
    "        \n",
    "        widgets.interact(self.simple_graph, sel_comp = comp_drop, show_all = show_all)\n",
    "        \n",
    "    def buy_then_sell(self, company, day, amount):\n",
    "        today_price = self.test_set[company, day, 0]\n",
    "        tomorrow_price = self.test_set[company, day+1, 0]\n",
    "        return amount * (tomorrow_price / today_price)\n",
    "    \n",
    "    def mse_sanity_check(self, path, tag='', exp_vis=False, name_override=''):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        strat_name = strat_name + tag\n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        model = tf.keras.models.load_model(path, compile=False)\n",
    "        \n",
    "        pred_act_pairs = []\n",
    "        for day in range(2, 200):\n",
    "            print(day, end='')\n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            # Make a prediction\n",
    "            pred = model.predict(tf.constant(seeable))\n",
    "            todays_closes = [self.test_set[c, day, 0] for c in range(self.num_companies)]\n",
    "            pred_act_pairs.append((pred, todays_closes))\n",
    "        \n",
    "        # Code for double checking that our model is making predictions\n",
    "        '''\n",
    "        company = 400\n",
    "        actual_price = []\n",
    "        pred_price = []\n",
    "        for day in prices:\n",
    "            pred = day[0]\n",
    "            act = day[1]\n",
    "            actual_price.append(act[company])\n",
    "            pred_price.append(pred[company])\n",
    "\n",
    "        plt.plot(actual_price)\n",
    "        plt.plot(pred_price)\n",
    "        \n",
    "        return pred_act_pairs\n",
    "        '''\n",
    "    \n",
    "    def strategy_follower(self):\n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(2, self.num_days - 2):\n",
    "        \n",
    "            if total < 0:\n",
    "                break\n",
    "            \n",
    "            two_back_closes = [self.test_set[c, day-2, 0] for c in range(self.num_companies)]\n",
    "            one_back_closes = [self.test_set[c, day-1, 0] for c in range(self.num_companies)]\n",
    "            \n",
    "            diff = [one_back_closes[i] - two_back_closes[i] for i in range(self.num_companies)]\n",
    "            \n",
    "            highest_earner = max_index(diff)\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Company: {list_of_data_csv_files[highest_earner]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            yesterday_earning = self.buy_then_sell(highest_earner, day, self.increment)\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results['Follower'] = total_by_day\n",
    "        \n",
    "    def strategy_best(self):\n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(2, self.num_days - 2):\n",
    "        \n",
    "            if total < 0:\n",
    "                break\n",
    "            \n",
    "            today_closes = [self.test_set[c, day, 0] for c in range(self.num_companies)]\n",
    "            tomorrow_closes = [self.test_set[c, day+1, 0] for c in range(self.num_companies)]\n",
    "            \n",
    "            diff = [tomorrow_closes[i] - today_closes[i] for i in range(self.num_companies)]\n",
    "            \n",
    "            highest_earner = max_index(diff)\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Company: {list_of_data_csv_files[highest_earner]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            yesterday_earning = self.buy_then_sell(highest_earner, day, self.increment)\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results['Follower'] = total_by_day\n",
    "    \n",
    "    def strategy_mse_lstm(self, path, tag='', exp_vis=False, name_override=''):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        strat_name = strat_name + tag\n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        model = tf.keras.models.load_model(path, compile=False)\n",
    "        \n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "        \n",
    "            if total < 0:\n",
    "                break\n",
    "                \n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict(tf.constant(seeable))\n",
    "            \n",
    "            todays_closes = [self.test_set[c, day, 0] for c in range(self.num_companies)]\n",
    "            \n",
    "            diff = [todays_closes[i] - pred[i] for i in range(self.num_companies)]\n",
    "            \n",
    "            highest_earner = max_index(diff)\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Company: {list_of_data_csv_files[highest_earner]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            yesterday_earning = self.buy_then_sell(highest_earner, day, self.increment)\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results[strat_name] = total_by_day\n",
    "    \n",
    "    def strategy_ratio_lstm(self, path, tag='', exp_vis=False, name_override=''):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        strat_name = strat_name + tag\n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        model = tf.keras.models.load_model(path, compile=False, custom_objects = {'<lambda>': lrelu} )\n",
    "        \n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "        \n",
    "            if total < 0:\n",
    "                break\n",
    "                \n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict(tf.constant(seeable))\n",
    "            \n",
    "            highest_earner = max_index(pred)\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Company: {list_of_data_csv_files[highest_earner]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            yesterday_earning = self.buy_then_sell(highest_earner, day, self.increment)\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results[strat_name] = total_by_day\n",
    "\n",
    "\n",
    "    def strategy_ratio_lstm(self, path, name_override='', avoid_fall=False, average=1):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        model = tf.keras.models.load_model(path, compile=False, custom_objects = {'<lambda>': lrelu} )\n",
    "        \n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        # Only used if avoid_fall strategy\n",
    "        losing_streak = -1\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "            \n",
    "            if avoid_fall:\n",
    "                if yesterday_earning < self.increment:\n",
    "                    losing_streak += 1\n",
    "            \n",
    "            if total < 0:\n",
    "                break\n",
    "                \n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict(tf.constant(seeable))\n",
    "            \n",
    "            if avoid_fall:\n",
    "                if yesterday_earning > self.increment:\n",
    "                    c_choices = [max_index(pred, i) for i in range(average)]\n",
    "                    losing_streak = 0\n",
    "                else:\n",
    "                    c_choices = [max_index(pred, i+losing_streak) for i in range(average)]\n",
    "                    \n",
    "            else:\n",
    "                c_choices = [max_index(pred, i) for i in range(average)]\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Companies: {[list_of_data_csv_files[c] for c in c_choices]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            # Calculate the amount of money that will be earned tomorrow when sold\n",
    "            sum = 0\n",
    "            for c in c_choices:\n",
    "                sum = sum + self.buy_then_sell(c, day, self.increment / len(c_choices))\n",
    "            \n",
    "            yesterday_earning = sum\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results[strat_name] = total_by_day\n",
    "        \n",
    "    \n",
    "    def strategy_ratio_gcn(self, path, tag='', exp_vis=False, name_override='', avoid_fall=True):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        strat_name = strat_name + tag\n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        model = tf.keras.models.load_model(path, compile=False, custom_objects = {'Ein_Multiply': Ein_Multiply, '<lambda>': lrelu, 'Ein_Multiply_Add_None': Ein_Multiply_Add_None})\n",
    "        \n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        losing_streak = -1\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "            \n",
    "            if yesterday_earning < self.increment:\n",
    "                losing_streak += 1\n",
    "            \n",
    "            if total < 0:\n",
    "                break\n",
    "                \n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict([tf.constant(seeable), GCN_mat])\n",
    "            \n",
    "            if avoid_fall:\n",
    "                if yesterday_earning > self.increment:\n",
    "                    highest_earner = max_index(pred, 0)\n",
    "                    losing_streak = 0\n",
    "                else:\n",
    "                    highest_earner = max_index(pred, losing_streak)\n",
    "            else:\n",
    "                highest_earner = max_index(pred, 0)\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Companies: {[list_of_data_csv_files[c] for c in c_choices]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            # Calculate the amount of money that will be earned tomorrow when sold\n",
    "            sum = 0\n",
    "            for c in c_choices:\n",
    "                sum = sum + self.buy_then_sell(c, day, self.increment / len(c_choices))\n",
    "            \n",
    "            yesterday_earning = sum\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results[strat_name] = total_by_day\n",
    "    \n",
    "    def strategy_ratio_gcn(self, path, name_override='', avoid_fall=False, average=1, expVis=False):\n",
    "        \n",
    "        # Name the strategy based on the path file\n",
    "        strat_name = path.split('\\\\')[-1] \n",
    "        \n",
    "        if name_override:\n",
    "            strat_name = name_override\n",
    "        \n",
    "        print(f\"\\nLoading Model: '{strat_name}'\")\n",
    "        # Load in the given LSTM Model\n",
    "        # This line is dependent on knowing what custom layers or functions were used\n",
    "        model = tf.keras.models.load_model(path, compile=False, custom_objects = {'Ein_Multiply': Ein_Multiply, '<lambda>': lrelu, 'Ein_Multiply_Add_None': Ein_Multiply_Add_None})\n",
    "        \n",
    "        # Portfolio to start\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        # Used in the avoid_fall scenario\n",
    "        losing_streak = -1\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "            \n",
    "            if avoid_fall:\n",
    "                if yesterday_earning < self.increment:\n",
    "                    losing_streak += 1\n",
    "            \n",
    "            if total < 0:\n",
    "                break\n",
    "                \n",
    "            # The model should only be able to see up to yesterday\n",
    "            seeable = self.test_set[:, 0:day, :]\n",
    "            \n",
    "            if expVis:\n",
    "                seeable = tf.concat([x_val, seeable], axis=1)\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict([tf.constant(seeable), GCN_mat])\n",
    "            \n",
    "            if avoid_fall:\n",
    "                if yesterday_earning > self.increment:\n",
    "                    c_choices = [max_index(pred, i) for i in range(average)]\n",
    "                    losing_streak = 0\n",
    "                else:\n",
    "                    c_choices = [max_index(pred, i+losing_streak) for i in range(average)]\n",
    "                    \n",
    "            else:\n",
    "                c_choices = [max_index(pred, i) for i in range(average)]\n",
    "\n",
    "            \n",
    "            print(f\"Day: {day} Total: {round(total)} Companies: {[list_of_data_csv_files[c] for c in c_choices]}\")\n",
    "            \n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            # Calculate the amount of money that will be earned tomorrow when sold\n",
    "            sum = 0\n",
    "            for c in c_choices:\n",
    "                sum = sum + self.buy_then_sell(c, day, self.increment / len(c_choices))\n",
    "            \n",
    "            yesterday_earning = sum\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "            \n",
    "        self.strategy_results[strat_name] = total_by_day\n",
    "    \n",
    "    def strategy_average(self):\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "            \n",
    "            # Choose 10 stocks at random\n",
    "            c_choices = [random.randint(0, self.num_companies-1) for i in range(200)]\n",
    "            \n",
    "            # Gain the money from yesterday's purchase\n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            # Calculate the amount of money that will be earned tomorrow when sold\n",
    "            sum = 0\n",
    "            for c in c_choices:\n",
    "                sum = sum + self.buy_then_sell(c, day, self.increment / len(c_choices))\n",
    "            \n",
    "            yesterday_earning = sum\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "        \n",
    "        self.strategy_results['Average'] = total_by_day \n",
    "    \n",
    "    def strategy_random(self):\n",
    "        total = self.starting_investment\n",
    "        yesterday_earning = 0\n",
    "        total_by_day = []\n",
    "        \n",
    "        for day in range(self.lag, self.num_days - self.lag):\n",
    "            # Choose X stocks at random\n",
    "            \n",
    "            chosen1 = random.randint(0, self.num_companies-1)\n",
    "            \n",
    "            # Gain the money from yesterday's purchase\n",
    "            total += yesterday_earning\n",
    "            \n",
    "            # Lose the money from the total that is spent today\n",
    "            total -= self.increment\n",
    "            \n",
    "            # Calculate the amount of money that will be earned tomorrow when sold\n",
    "            yesterday_earning = self.buy_then_sell(chosen1, day, self.increment)\n",
    "            \n",
    "            total_by_day.append(total)\n",
    "        \n",
    "        self.strategy_results['Random'] = total_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ModelCompare(x_test, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Model: 'GCN_RankLoss_Ratio_33epoch_1NewDense_VarLR-AvoidFall-expVis'\n",
      "Day: 1 Total: 100000 Companies: ['NASDAQ_SHEN_1.csv']\n",
      "Day: 2 Total: 50000 Companies: ['NASDAQ_SHEN_1.csv']\n",
      "Day: 3 Total: 50281.0 Companies: ['NASDAQ_SHEN_1.csv']\n",
      "Day: 4 Total: 50537.0 Companies: ['NASDAQ_SHEN_1.csv']\n",
      "Day: 5 Total: 50343.0 Companies: ['NASDAQ_HAIN_1.csv']\n",
      "Day: 6 Total: 50412.0 Companies: ['NASDAQ_ZEUS_1.csv']\n",
      "Day: 7 Total: 50273.0 Companies: ['NASDAQ_HAIN_1.csv']\n",
      "Day: 8 Total: 52063.0 Companies: ['NASDAQ_ZEUS_1.csv']\n",
      "Day: 9 Total: 51921.0 Companies: ['NASDAQ_HAIN_1.csv']\n",
      "Day: 10 Total: 52751.0 Companies: ['NASDAQ_HAIN_1.csv']\n",
      "Day: 11 Total: 52722.0 Companies: ['NASDAQ_HAIN_1.csv']\n",
      "Day: 12 Total: 52631.0 Companies: ['NASDAQ_TSCO_1.csv']\n",
      "Day: 13 Total: 52622.0 Companies: ['NASDAQ_LULU_1.csv']\n",
      "Day: 14 Total: 52475.0 Companies: ['NASDAQ_SPWR_1.csv']\n",
      "Day: 15 Total: 51881.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 16 Total: 51386.0 Companies: ['NASDAQ_GIII_1.csv']\n",
      "Day: 17 Total: 53754.0 Companies: ['NASDAQ_GIII_1.csv']\n",
      "Day: 18 Total: 53897.0 Companies: ['NASDAQ_GIII_1.csv']\n",
      "Day: 19 Total: 54031.0 Companies: ['NASDAQ_GIII_1.csv']\n",
      "Day: 20 Total: 54207.0 Companies: ['NASDAQ_LULU_1.csv']\n",
      "Day: 21 Total: 53970.0 Companies: ['NASDAQ_GIII_1.csv']\n",
      "Day: 22 Total: 53619.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 23 Total: 53715.0 Companies: ['NASDAQ_MLHR_1.csv']\n",
      "Day: 24 Total: 53688.0 Companies: ['NASDAQ_LULU_1.csv']\n",
      "Day: 25 Total: 53333.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 26 Total: 53252.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 27 Total: 52125.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 28 Total: 52516.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 29 Total: 50547.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 30 Total: 50714.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 31 Total: 48133.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 32 Total: 48200.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 33 Total: 47170.0 Companies: ['NASDAQ_SNBR_1.csv']\n",
      "Day: 34 Total: 47079.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 35 Total: 46996.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 36 Total: 46641.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 37 Total: 47966.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 38 Total: 47684.0 Companies: ['NASDAQ_ILMN_1.csv']\n",
      "Day: 39 Total: 46949.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 40 Total: 47018.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 41 Total: 46380.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 42 Total: 48529.0 Companies: ['NASDAQ_ALNY_1.csv']\n",
      "Day: 43 Total: 48056.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 44 Total: 51305.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 45 Total: 51794.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 46 Total: 52497.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 47 Total: 52815.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 48 Total: 53096.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 49 Total: 53350.0 Companies: ['NASDAQ_IMPV_1.csv']\n",
      "Day: 50 Total: 52875.0 Companies: ['NASDAQ_IMPV_1.csv']\n",
      "Day: 51 Total: 52850.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 52 Total: 52863.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 53 Total: 53187.0 Companies: ['NASDAQ_SYNT_1.csv']\n",
      "Day: 54 Total: 53222.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 55 Total: 53227.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 56 Total: 53706.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 57 Total: 52219.0 Companies: ['NASDAQ_CSTE_1.csv']\n",
      "Day: 58 Total: 51228.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 59 Total: 51428.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 60 Total: 51039.0 Companies: ['NASDAQ_FSLR_1.csv']\n",
      "Day: 61 Total: 49939.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 62 Total: 50896.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 63 Total: 50922.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 64 Total: 50303.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 65 Total: 50493.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 66 Total: 49729.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 67 Total: 49767.0 Companies: ['NASDAQ_IPXL_1.csv']\n",
      "Day: 68 Total: 48782.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 69 Total: 48858.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 70 Total: 49028.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 71 Total: 49743.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 72 Total: 50356.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 73 Total: 50294.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 74 Total: 51131.0 Companies: ['NASDAQ_LWAY_1.csv']\n",
      "Day: 75 Total: 50977.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 76 Total: 51135.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 77 Total: 50667.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 78 Total: 50673.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 79 Total: 50769.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 80 Total: 50991.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 81 Total: 51466.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 82 Total: 51842.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 83 Total: 52153.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 84 Total: 51844.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 85 Total: 50352.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 86 Total: 50654.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 87 Total: 49699.0 Companies: ['NASDAQ_FINL_1.csv']\n",
      "Day: 88 Total: 48435.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 89 Total: 48155.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 90 Total: 48035.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 91 Total: 47739.0 Companies: ['NASDAQ_GPOR_1.csv']\n",
      "Day: 92 Total: 46882.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 93 Total: 47232.0 Companies: ['NASDAQ_FINL_1.csv']\n",
      "Day: 94 Total: 47134.0 Companies: ['NASDAQ_COWN_1.csv']\n",
      "Day: 95 Total: 47105.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 96 Total: 47236.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 97 Total: 47269.0 Companies: ['NASDAQ_FINL_1.csv']\n",
      "Day: 98 Total: 46881.0 Companies: ['NASDAQ_FINL_1.csv']\n",
      "Day: 99 Total: 46793.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 100 Total: 46963.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 101 Total: 47319.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 102 Total: 46459.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 103 Total: 46603.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 104 Total: 47368.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 105 Total: 47879.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 106 Total: 48164.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 107 Total: 48382.0 Companies: ['NASDAQ_VIVO_1.csv']\n",
      "Day: 108 Total: 48534.0 Companies: ['NASDAQ_AMAG_1.csv']\n",
      "Day: 109 Total: 48495.0 Companies: ['NASDAQ_VIVO_1.csv']\n",
      "Day: 110 Total: 48883.0 Companies: ['NASDAQ_CORE_1.csv']\n",
      "Day: 111 Total: 48765.0 Companies: ['NASDAQ_VIVO_1.csv']\n",
      "Day: 112 Total: 48844.0 Companies: ['NASDAQ_VIVO_1.csv']\n",
      "Day: 113 Total: 48883.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 114 Total: 48805.0 Companies: ['NASDAQ_CORE_1.csv']\n",
      "Day: 115 Total: 48614.0 Companies: ['NASDAQ_CORE_1.csv']\n",
      "Day: 116 Total: 48314.0 Companies: ['NASDAQ_SPSC_1.csv']\n",
      "Day: 117 Total: 48111.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 118 Total: 47631.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 119 Total: 47336.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 120 Total: 46926.0 Companies: ['NASDAQ_SPSC_1.csv']\n",
      "Day: 121 Total: 47153.0 Companies: ['NASDAQ_MTRX_1.csv']\n",
      "Day: 122 Total: 46916.0 Companies: ['NASDAQ_SPSC_1.csv']\n",
      "Day: 123 Total: 46541.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 124 Total: 46740.0 Companies: ['NASDAQ_MTRX_1.csv']\n",
      "Day: 125 Total: 46539.0 Companies: ['NASDAQ_CRMT_1.csv']\n",
      "Day: 126 Total: 46253.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 127 Total: 46731.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 128 Total: 46750.0 Companies: ['NASDAQ_MTRX_1.csv']\n",
      "Day: 129 Total: 46731.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 130 Total: 46795.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 131 Total: 46810.0 Companies: ['NASDAQ_MTRX_1.csv']\n",
      "Day: 132 Total: 46711.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 133 Total: 46743.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 134 Total: 46623.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 135 Total: 47506.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 136 Total: 48572.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 137 Total: 49488.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 138 Total: 49941.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 139 Total: 50011.0 Companies: ['NASDAQ_ESND_1.csv']\n",
      "Day: 140 Total: 49971.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 141 Total: 50162.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 142 Total: 50991.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 143 Total: 51315.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 144 Total: 51529.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 145 Total: 51830.0 Companies: ['NASDAQ_FTR_1.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: 146 Total: 48476.0 Companies: ['NASDAQ_FTR_1.csv']\n",
      "Day: 147 Total: 48044.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 148 Total: 47269.0 Companies: ['NASDAQ_SPWR_1.csv']\n",
      "Day: 149 Total: 47040.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 150 Total: 46713.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 151 Total: 46485.0 Companies: ['NASDAQ_SPWR_1.csv']\n",
      "Day: 152 Total: 46073.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 153 Total: 46296.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 154 Total: 47695.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 155 Total: 49055.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 156 Total: 50219.0 Companies: ['NASDAQ_LAWS_1.csv']\n",
      "Day: 157 Total: 51000.0 Companies: ['NASDAQ_FNSR_1.csv']\n",
      "Day: 158 Total: 51549.0 Companies: ['NASDAQ_FNSR_1.csv']\n",
      "Day: 159 Total: 51724.0 Companies: ['NASDAQ_MDGL_1.csv']\n",
      "Day: 160 Total: 51646.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 161 Total: 51047.0 Companies: ['NASDAQ_HQCL_1.csv']\n",
      "Day: 162 Total: 50633.0 Companies: ['NASDAQ_FTR_1.csv']\n",
      "Day: 163 Total: 50156.0 Companies: ['NASDAQ_ARNA_1.csv']\n",
      "Day: 164 Total: 48224.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 165 Total: 47675.0 Companies: ['NASDAQ_HUBG_1.csv']\n",
      "Day: 166 Total: 47534.0 Companies: ['NASDAQ_AFSI_1.csv']\n",
      "Day: 167 Total: 47143.0 Companies: ['NASDAQ_FOSL_1.csv']\n",
      "Day: 168 Total: 45389.0 Companies: ['NASDAQ_MTRX_1.csv']\n",
      "Day: 169 Total: 42502.0 Companies: ['NASDAQ_HQCL_1.csv']\n",
      "Day: 170 Total: 40500.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 171 Total: 40470.0 Companies: ['NASDAQ_FOSL_1.csv']\n",
      "Day: 172 Total: 39755.0 Companies: ['NASDAQ_MYRG_1.csv']\n",
      "Day: 173 Total: 39140.0 Companies: ['NASDAQ_ARCB_1.csv']\n",
      "Day: 174 Total: 38532.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 175 Total: 39117.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 176 Total: 39876.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 177 Total: 40365.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 178 Total: 40901.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 179 Total: 41028.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 180 Total: 41153.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 181 Total: 40902.0 Companies: ['NASDAQ_MYRG_1.csv']\n",
      "Day: 182 Total: 40978.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 183 Total: 40903.0 Companies: ['NASDAQ_MYRG_1.csv']\n",
      "Day: 184 Total: 40917.0 Companies: ['NASDAQ_MYRG_1.csv']\n",
      "Day: 185 Total: 41151.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 186 Total: 41573.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 187 Total: 42253.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 188 Total: 42857.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 189 Total: 43494.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 190 Total: 44227.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 191 Total: 44692.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 192 Total: 44692.0 Companies: ['NASDAQ_NCMI_1.csv']\n",
      "Day: 193 Total: 44769.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 194 Total: 44794.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 195 Total: 45124.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 196 Total: 47415.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 197 Total: 48902.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 198 Total: 49738.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 199 Total: 49738.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 200 Total: 50410.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 201 Total: 49157.0 Companies: ['NASDAQ_SPTN_1.csv']\n",
      "Day: 202 Total: 49143.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 203 Total: 49599.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 204 Total: 49985.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 205 Total: 49602.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 206 Total: 50991.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 207 Total: 51364.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 208 Total: 51549.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 209 Total: 51773.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 210 Total: 52075.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 211 Total: 51671.0 Companies: ['NASDAQ_CRZO_1.csv']\n",
      "Day: 212 Total: 50803.0 Companies: ['NASDAQ_CRZO_1.csv']\n",
      "Day: 213 Total: 50704.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 214 Total: 51289.0 Companies: ['NASDAQ_NLNK_1.csv']\n",
      "Day: 215 Total: 50760.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 216 Total: 50596.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 217 Total: 50596.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 218 Total: 51137.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 219 Total: 51165.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 220 Total: 51839.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 221 Total: 52463.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 222 Total: 53498.0 Companies: ['NASDAQ_ORLY_1.csv']\n",
      "Day: 223 Total: 54586.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 224 Total: 55696.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 225 Total: 55696.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 226 Total: 55934.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 227 Total: 56205.0 Companies: ['NASDAQ_DMRC_1.csv']\n",
      "Day: 228 Total: 56171.0 Companies: ['NASDAQ_AMED_1.csv']\n",
      "Day: 229 Total: 55327.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 230 Total: 55570.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 231 Total: 55704.0 Companies: ['NASDAQ_AMED_1.csv']\n",
      "Day: 232 Total: 55670.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 233 Total: 55638.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 234 Total: 56188.0 Companies: ['NASDAQ_PCMI_1.csv']\n",
      "Day: 235 Total: 55953.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 236 Total: 55523.0 Companies: ['NASDAQ_MTSI_1.csv']\n",
      "Day: 237 Total: 54941.0 Companies: ['NASDAQ_LYTS_1.csv']\n",
      "Day: 238 Total: 54668.0 Companies: ['NASDAQ_TTMI_1.csv']\n",
      "Day: 239 Total: 54668.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 240 Total: 55159.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 241 Total: 55602.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 242 Total: 56307.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 243 Total: 57081.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 244 Total: 58039.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 245 Total: 59034.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 246 Total: 59325.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 247 Total: 59403.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 248 Total: 59454.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 249 Total: 59584.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 250 Total: 59842.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 251 Total: 60099.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 252 Total: 60764.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 253 Total: 61420.0 Companies: ['NASDAQ_VECO_1.csv']\n",
      "Day: 254 Total: 62092.0 Companies: ['NASDAQ_MHLD_1.csv']\n",
      "Day: 255 Total: 62878.0 Companies: ['NASDAQ_MHLD_1.csv']\n",
      "Day: 256 Total: 62878.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 257 Total: 62746.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 258 Total: 63253.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 259 Total: 63997.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 260 Total: 64657.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 261 Total: 64928.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 262 Total: 65100.0 Companies: ['NASDAQ_PETS_1.csv']\n",
      "Day: 263 Total: 65019.0 Companies: ['NASDAQ_AOBC_1.csv']\n",
      "Day: 264 Total: 64300.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 265 Total: 64456.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 266 Total: 64951.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 267 Total: 65513.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 268 Total: 65980.0 Companies: ['NASDAQ_ACET_1.csv']\n",
      "Day: 269 Total: 66548.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 270 Total: 66868.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 271 Total: 69511.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 272 Total: 72521.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 273 Total: 75367.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 274 Total: 78068.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 275 Total: 78068.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 276 Total: 78082.0 Companies: ['NASDAQ_SNCR_1.csv']\n",
      "Day: 277 Total: 77196.0 Companies: ['NASDAQ_VIIX_1.csv']\n",
      "Day: 278 Total: 75268.0 Companies: ['NASDAQ_SPWR_1.csv']\n",
      "Day: 279 Total: 74829.0 Companies: ['NASDAQ_SPWR_1.csv']\n",
      "Day: 280 Total: 74415.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 281 Total: 74609.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 282 Total: 75406.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 283 Total: 75631.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 284 Total: 76579.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 285 Total: 76689.0 Companies: ['NASDAQ_MMYT_1.csv']\n",
      "Day: 286 Total: 75979.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 287 Total: 77019.0 Companies: ['NASDAQ_MMYT_1.csv']\n",
      "Day: 288 Total: 76515.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 289 Total: 78310.0 Companies: ['NASDAQ_MNTA_1.csv']\n",
      "Day: 290 Total: 77907.0 Companies: ['NASDAQ_MNTA_1.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: 291 Total: 77532.0 Companies: ['NASDAQ_FIZZ_1.csv']\n",
      "Day: 292 Total: 76778.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 293 Total: 77032.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 294 Total: 76914.0 Companies: ['NASDAQ_CELG_1.csv']\n",
      "Day: 295 Total: 76973.0 Companies: ['NASDAQ_ACHC_1.csv']\n",
      "Day: 296 Total: 76926.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 297 Total: 76384.0 Companies: ['NASDAQ_TTS_1.csv']\n",
      "Day: 298 Total: 75010.0 Companies: ['NASDAQ_MACK_1.csv']\n",
      "Day: 299 Total: 74891.0 Companies: ['NASDAQ_ESND_1.csv']\n",
      "Day: 300 Total: 74328.0 Companies: ['NASDAQ_CELG_1.csv']\n",
      "Day: 301 Total: 74493.0 Companies: ['NASDAQ_CELG_1.csv']\n",
      "Day: 302 Total: 75046.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 303 Total: 75514.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 304 Total: 75866.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 305 Total: 76036.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 306 Total: 76570.0 Companies: ['NASDAQ_EFII_1.csv']\n",
      "Day: 307 Total: 77164.0 Companies: ['NASDAQ_EFII_1.csv']\n"
     ]
    }
   ],
   "source": [
    "path = r'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\November_Models\\\\'\n",
    "\n",
    "# A.strategy_random()\n",
    "# A.strategy_average()\n",
    "# A.strategy_average()\n",
    "# A.strategy_best()\n",
    "# A.strategy_follower()\n",
    "# A.strategy_mse_lstm(path + 'easy)\n",
    "# A.strategy_ratio_lstm(path + 'god', name_override='MSE-Ratio-104-Epoch;1e-5-LR-LeakyRelu-RankLoss')\n",
    "\n",
    "# A.strategy_ratio_gcn_average(path + 'test', name_override='GCN-RankLoss-Ratio-33-Epoch-1NewLayer;3e-5-LR-Average2', avoid_fall=False, average=2)\n",
    "# A.strategy_ratio_lstm(path + 'god', name_override='MSE-Ratio-104-Epoch;1e-5-LR-LeakyRelu-RankLoss')\n",
    "\n",
    "# A.strategy_ratio_gcn(path + 'test', avoid_fall = False, name_override='GCN_RankLoss_Ratio_33epoch_1TLSTM1FLSTM_VarLR-expVis', average=1, expVis=True)\n",
    "# A.strategy_ratio_gcn(path + 'test', avoid_fall = False, name_override='GCN_RankLoss_Ratio_33epoch_1TLSTM1FLSTM_VarLR-Average-2-expVis', average=2, expVis=True)\n",
    "A.strategy_ratio_gcn(path + 'test', avoid_fall = True, name_override='GCN_RankLoss_Ratio_33epoch_1NewDense_VarLR-AvoidFall-expVis', average=1, expVis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a0f0749c37436c8906b2c0fb7c3ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Strategy:', options=('GCN_RankLoss_Ratio_33epoch_1TLSTM1FLSTM_VarL…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A.graph()\n",
    "A.save_results(r'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\Trading_Results')\n",
    "# test = pickle.load(open(r'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\Trading_Results\\Average.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.concat([x_val, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([881, 185, 5]) TensorShape([881, 309, 5])\r\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
