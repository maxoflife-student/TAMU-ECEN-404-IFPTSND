{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e66073716a4cf0bbd9540c88f2d322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Dropdown(description='Model Types:', options=('lstm', 'lstm_gcn_1', 'lstm_gcn_2', 'lstm_gcn_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the  model\n",
    "model = DMJ.train_model_loop(epoch_batches=8, learning_rate=5e-4)\n",
    "# model = DMJ.train_model(model, data_splits['x_train'], data_splits['y_train'], data_splits['x_val'], data_splits['y_val'], epochs=50, learning_rate=5e-5, gcn_matrix=DMJ.Normalized_Adjacency_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.save_model(tag='E2E_2LSTM_FThenT_Outputsize[None,1]_VariedLR_TFMSE_Dropout0.2_30D0DSwap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = '02-15-2021--17--12-E2E_2LSTM_FThenT_Outputsize[None,1]_VariedLR_TFMSE_Dropout0.2_30D0DSwap-88Epochs-mse-Loss-32-HU-'\n",
    "new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "GP.generate_validation_prediction_json(model_name, new_directory, sliding_window=300, neural_net_type='lstm', tag='_300window_valset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GP.generate_validation_prediction_json(model_name, new_directory, sliding_window=30, neural_net_type='lstm', tag='_30window_valset')\n",
    "directory = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/'\n",
    "# directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "model = '02-16-2021--19--10-E2E-4LSTM-TTF-100.0ALPHA-[55,25,20]split-[None, 1]-sleepy-20Epochs-rlf-Loss-64-HU-_PM'\n",
    "# GP.generate_model_diagnostics(directory + model, datablock_folder='./ignorable_data/datablocks/[55, 25, 20]_splits/')\n",
    "GP.generate_model_diagnostics_given_sets(directory + model, DMJ.data_splits['x_val'], datablock_folder='./ignorable_data/datablocks/[55, 25, 20]_splits/', try_all_pred=False)\n",
    "GP.generate_model_diagnostics_given_sets(directory + model, DMJ.data_splits['x_val'], datablock_folder='./ignorable_data/datablocks/[55, 25, 20]_splits/', try_all_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.compare_data_blocks('./ignorable_data/datablocks/[55, 25, 20]_splits/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Attempting to use the companies as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.XX_tf.shape\n",
    "DMJ.YY_tf.shape\n",
    "\n",
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[0] = new_splits[0] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [79, 21]\n",
    "\n",
    "x_train, x_test = tf.split(DMJ.XX_tf, split_windows(DMJ.XX_tf.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_train, y_test = tf.split(DMJ.YY_tf, split_windows(DMJ.YY_tf.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(DMJ.YY_tf.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# Given our training data, let's split the data into two batches by company\n",
    "company_split = [66, 33]\n",
    "\n",
    "x_batch_t, x_batch_v = tf.split(x_train, split_windows(x_train.shape[0], company_split),\n",
    "                                  axis=0)\n",
    "y_batch_t, y_batch_v = tf.split(y_train, split_windows(y_train.shape[0], company_split),\n",
    "                                  axis=0)\n",
    "rr_batch_t, rr_batch_v = tf.split(rr_train, split_windows(rr_train.shape[0], company_split),\n",
    "                                     axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X\")\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_batch_t.shape, x_batch_v.shape)\n",
    "print(\"Y\")\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(y_batch_t.shape, y_batch_v.shape)\n",
    "print(\"RR\")\n",
    "print(rr_batch_t.shape, rr_batch_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an LSTM model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "\n",
    "input_seq = keras.Input(shape=(x_batch_t.shape[1], x_batch_t.shape[2]))\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 32\n",
    "activation = leaky_relu\n",
    "\n",
    "x = LSTM(hidden_units, return_sequences=True, activation=activation, dropout=0.5, recurrent_dropout=0.5)(input_seq)\n",
    "x = LSTM(hidden_units, return_sequences=False, activation=activation, dropout=0.5, recurrent_dropout=0.5)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = x_batch_t\n",
    "train_labels = y_batch_t\n",
    "\n",
    "inputs_val = x_batch_v\n",
    "val_labels = y_batch_v\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train.shape[0]),\n",
    "                              epochs=10, validation_data=(inputs_val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained in Google Colab, datablocks will be generated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "pred_file = '02-16-2021--07--32-E2E-3LSTM-TTF-0ALPHA-[55,25,20]split-[None, 1]output-30Epochs-rlf-Loss-32-HU-_PM'\n",
    "GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/')\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_test, datablock_folder='./ignorable_data/datablocks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(GP.test_obj.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n = 480\n",
    "r = 1\n",
    "for i in range(r):\n",
    "    print(f'{i+n*r} ', end='')\n",
    "    plt.plot(GP.test_obj[:, i+n*r])\n",
    "    plt.plot(x_test[i+n*r, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.keras.layers.ZeroPadding1D(padding=(1,0))(DMJ.data_splits['y_val'])\n",
    "# # x = tf.concat(tf.zeros([880, 10], tf.float32), DMJ.data_splits['y_val'])\n",
    "# print(x.shape)\n",
    "# print(x[1, 0])\n",
    "z = tf.zeros([881, 20], tf.float32)\n",
    "x = tf.concat([z, DMJ.data_splits['y_val']], 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Proving that our RR labels make sense\n",
    "# print(DMJ.data_splits['x_train'].shape)\n",
    "# print(DMJ.data_splits['rr_train'].shape)\n",
    "\n",
    "# # One company at time step 100\n",
    "# print(DMJ.data_splits['x_train'][87, 30, 0])\n",
    "# # One company at time step 101\n",
    "# print(DMJ.data_splits['x_train'][87, 31, 0])\n",
    "\n",
    "# # So if a model was able to see timesteps 0-100, then we would want this as the prediction\n",
    "# print(DMJ.data_splits['rr_train'][87, 30])\n",
    "\n",
    "# print('Therefore, RR_train is a successful label of X_train')\n",
    "\n",
    "# # One company at time step 100\n",
    "# print(DMJ.data_splits['x_train'][87, 30, 0])\n",
    "# print(DMJ.data_splits['x_train'][87, 31, 0])\n",
    "# # One company at time step 101\n",
    "# print(DMJ.data_splits['y_train'][87, 30])\n",
    "\n",
    "\n",
    "# print('Therefore, y_train is a successful label of x_train')\n",
    "# # Proving that our RR labels make sense\n",
    "# print(DMJ.data_splits['x_val'].shape)\n",
    "# print(DMJ.data_splits['rr_val'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we split the data into time batches ourselves and trained them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:2, :, :]\n",
    "# one_y = DMJ.YY_tf[0:2, :]\n",
    "one_x = DMJ.XX_tf\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "# # Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "# time_split = [1000, 239,]\n",
    "# x_train, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# y_train, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "#                                      axis=1)\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "y_train = y_train[:, -1]\n",
    "y_val = y_val[:, -1]\n",
    "\n",
    "# # Once we have the data partitioned, let's split it into 8 sets of 125\n",
    "# batch_splits = [1]*8\n",
    "# # Create a list of 8 different time batches\n",
    "# x_train_batches = []\n",
    "# x_train_batches.append(tf.split(x_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# x_train_batches = [item for sublist in x_train_batches for item in sublist]\n",
    "\n",
    "# y_train_batches = []\n",
    "# y_train_batches.append(tf.split(y_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# y_train_batches = [item for sublist in y_train_batches for item in sublist]\n",
    "\n",
    "# # Lets try truncating the values for the labels to only be the final value\n",
    "# for i in range(len(y_train_batches)):\n",
    "#     y_train_batches[i] = y_train_batches[i][:, -1]\n",
    "\n",
    "# print(x_train[0, -1, 0])\n",
    "\n",
    "# print(rr_train[0, -1])\n",
    "\n",
    "# print(x_val[0, 0, 0])\n",
    "# print(y_train[0, -1])\n",
    "# for set in [x_train, x_g, x_val, x_test]:\n",
    "#     print(set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an LSTM model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0.3333\n",
    "x = LSTM(hidden_units, activation=activation, dropout=do)(input_seq)\n",
    "# x = tf.keras.layers.RepeatVector(x_train_batches[0].shape[1])(x)\n",
    "# x = LSTM(hidden_units, activation=activation, dropout=do)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(hidden_units, activation=activation, input_shape=(x_train_batches[0].shape[1], x_train_batches[0].shape[2])))\n",
    "# model.add(Dense(1, activation=activation))\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_train = x_train\n",
    "train_labels = y_train\n",
    "\n",
    "inputs_val = x_val\n",
    "val_labels = y_val\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train.shape[0]),\n",
    "                              epochs=5, validation_data=(inputs_val, val_labels))\n",
    "\n",
    "for h in history.history['val_loss']:\n",
    "    losses.append(h)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# losses = []\n",
    "\n",
    "# # Train over different time series in quick succession in random order\n",
    "# items = list(range(len(x_train_batches)))\n",
    "# random.shuffle(items)\n",
    "# for i in items:\n",
    "#     print(i)\n",
    "#     inputs_train = x_train_batches[i]\n",
    "#     train_labels = y_train_batches[i]\n",
    "\n",
    "# #     inputs_val = x_batch_v\n",
    "# #     val_labels = y_batch_v\n",
    "\n",
    "# #     history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train.shape[0]),\n",
    "# #                                   epochs=10, validation_data=(inputs_val, val_labels))\n",
    "#     history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train.shape[0]),\n",
    "#                                   epochs=3)\n",
    "#     for h in history.history['loss']:\n",
    "#         losses.append(h)\n",
    "    \n",
    "# avg_losses.append(np.mean(losses))\n",
    "# print(avg_losses[-1])\n",
    "# plt.plot(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.model = model\n",
    "DMJ.model_name = \"2_companies\"\n",
    "DMJ.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_name = \"2-16-21-Seq1LSTM-F-64HU-[800,239,200]split-full_y\"\n",
    "# new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "# GP.generate_validation_prediction_json_SplitBatch_nofeat(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "# GP.generate_validation_prediction_json_SplitBatch_close_gap(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "model_name = '2_companies'\n",
    "model_dir = './ignorable_data/models/[55, 25, 20]_split'\n",
    "past = x_train\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits'\n",
    "sliding_window = x_val.shape[1]\n",
    "# sliding_window = 10\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, sliding_window)\n",
    "# GP.generate_validation_prediction_json_SplitBatch(model_name, new_dir, x_g, x_val, sliding_window=sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Model: '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "model_name = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "model_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models'\n",
    "GP.generate_dot_file(model_name, model_dir, 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_file_name = '02-17-2021--08--29-1LSTM-F-1000.0ALPHA-[55,25,20]split-[None, 1]-20Epochs-rlf-Loss-64-HU-'\n",
    "# p_file_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits'\n",
    "p_file_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions'\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "close_gap = False\n",
    "\n",
    "use_argmin = False\n",
    "yesterday_pred = True\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "use_argmin = True\n",
    "yesterday_pred = False\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "# GP.generate_model_diagnostics_given_sets(p_file_dir + f'/{p_file_name}', future, datablock_folder=new_dir, try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 597\n",
    "# n = 488\n",
    "# n = 161\n",
    "# n = 561\n",
    "# n = 440\n",
    "# # Argmin\n",
    "# n = 780\n",
    "\n",
    "# n = 876\n",
    "# n = 110\n",
    "# r = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.set_ylim([0,1])\n",
    "\n",
    "time = tf.concat([x_train, x_val, x_test], axis=1)\n",
    "\n",
    "# ax = fig.add_subplot()\n",
    "for i in range(r):\n",
    "    print(f'{i+n*r} ', end='')\n",
    "#     ax.plot(time[i+n*r, :, 0])\n",
    "    ax.plot(x_val[i+n*r, :, 0])\n",
    "    \n",
    "    ax.plot(A[:, i+n*r])\n",
    "    ax.plot(B[:, i+n*r])\n",
    "    ax.plot(C[:, i+n*r])\n",
    "#     ax.plot(D[:, i+n*r])\n",
    "# Orange\n",
    "# Green\n",
    "# Red\n",
    "# Purple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/'\n",
    "pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "A = GP.test_obj\n",
    "\n",
    "# pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '02-17-2021--07--54-1LSTM-F-10.0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "B = GP.test_obj\n",
    "\n",
    "pred_file = '02-18-2021--19--21-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-NoDropOut-30Epochs-rlf-Loss-64-HU-'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "C = GP.test_obj\n",
    "\n",
    "# pred_file = '02-18-2021--03--47-5EPOCHBAD-1LSTM-F-100ALPHA-[55,25,20]split-[None, 1]-5Epochs-rlf-Loss-32-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, None)\n",
    "# D = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/'\n",
    "pred_file = '02-17-2021--07--37-1LSTM-F-1ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, None)\n",
    "B = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "pred_dir = r\".\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/\"\n",
    "pred_file = '2-17-21-Seq1LSTM-F-64HU-[90,710,239,199]split-truncated_y_90win'\n",
    "# GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/')\n",
    "GP.generate_model_diagnostics_given_sets_close_gap(pred_dir + pred_file, x_val, datablock_folder='./ignorable_data/datablocks/', try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
