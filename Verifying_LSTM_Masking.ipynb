{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DMJ.entities_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06cecb9faef4d87a0aaa1c1ee5b8a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Dropdown(description='Model Types:', options=('lstm', 'lstm_gcn_1', 'lstm_gcn_2', 'lstm_gcn_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix = DMJ.Normalized_Adjacency_Matrix[0:880, 0:880]\n",
    "DMJ.XX_tf = DMJ.XX_tf[0:-1, :, :]\n",
    "DMJ.YY_tf = DMJ.YY_tf[0:-1, :]\n",
    "DMJ.RR_tf = DMJ.RR_tf[0:-1, :]\n",
    "DMJ.entities = DMJ.entities[0:-1]\n",
    "DMJ.entities_idx.pop('ZUMZ')\n",
    "\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we split the data into time batches ourselves and trained them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 1239, 5)\n",
      "(880, 309, 5)\n",
      "(880, 309, 5)\n",
      "tf.Tensor(0.455917, shape=(), dtype=float32)\n",
      "tf.Tensor(0.461538, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012328968, shape=(), dtype=float32)\n",
      "tf.Tensor(0.425614, shape=(), dtype=float32)\n",
      "tf.Tensor(0.415741, shape=(), dtype=float32)\n",
      "tf.Tensor([0.415741], shape=(1,), dtype=float32)\n",
      "(None, 309, 5)\n"
     ]
    }
   ],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:2, :, :]\n",
    "# one_y = DMJ.YY_tf[0:2, :]\n",
    "one_x = DMJ.XX_tf\n",
    "print(DMJ.XX_tf.shape)\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:-1, :, :]\n",
    "# one_y = DMJ.YY_tf[0:-1, :]\n",
    "\n",
    "# # Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "# time_split = [1000, 239,]\n",
    "# x_train, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# y_train, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "#                                      axis=1)\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# y_train = y_train[:, -1]\n",
    "# y_val = y_val[:, -1]\n",
    "\n",
    "# # Once we have the data partitioned, let's split it into 8 sets of 125\n",
    "# batch_splits = [1]*8\n",
    "# # Create a list of 8 different time batches\n",
    "# x_train_batches = []\n",
    "# x_train_batches.append(tf.split(x_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# x_train_batches = [item for sublist in x_train_batches for item in sublist]\n",
    "\n",
    "# y_train_batches = []\n",
    "# y_train_batches.append(tf.split(y_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# y_train_batches = [item for sublist in y_train_batches for item in sublist]\n",
    "\n",
    "# # Lets try truncating the values for the labels to only be the final value\n",
    "# for i in range(len(y_train_batches)):\n",
    "#     y_train_batches[i] = y_train_batches[i][:, -1]\n",
    "\n",
    "# print(x_train[0, -1, 0])\n",
    "\n",
    "# print(rr_train[0, -1])\n",
    "\n",
    "# print(x_val[0, 0, 0])\n",
    "# print(y_train[0, -1])\n",
    "# for set in [x_train, x_g, x_val, x_test]:\n",
    "print(x_val.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_train[0, 0, 0])\n",
    "print(x_train[0, 1, 0])\n",
    "print(rr_train[0, 0])\n",
    "\n",
    "print(x_train[0, -1, 0])\n",
    "print(x_train[0, -2, 0])\n",
    "print(x_train[0, -2:-1, 0])\n",
    "\n",
    "print(keras.Input(shape=(x_train.shape[1], x_train.shape[2])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training an LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow_models import mse_rr\n",
    "\n",
    "# from tensorflow_models import rank_loss_rr\n",
    "# from tensorflow_models import rank_loss_rr_labels\n",
    "from tensorflow_models import rank_loss_correct_pred_return_sequences\n",
    "\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "# Creating an LSTM model\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "# input_rel = keras.Input(shape=(DMJ.Normalized_Adjacency_Matrix.shape[0]))\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 32\n",
    "activation = leaky_relu\n",
    "do = 0.1\n",
    "# x = tf.keras.layers.Masking(mask_value=0.0)(input_seq)\n",
    "x = LSTM(hidden_units, activation=activation, dropout=do, recurrent_dropout=do, return_sequences=True)(input_seq)\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "# x = tf.keras.layers.RepeatVector(x_train_batches[0].shape[1])(x)\n",
    "# x = LSTM(hidden_units, activation=activation, dropout=do)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "# model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "model.compile(loss=rank_loss_correct_pred_return_sequences, optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(hidden_units, activation=activation, input_shape=(x_train_batches[0].shape[1], x_train_batches[0].shape[2])))\n",
    "# model.add(Dense(1, activation=activation))\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()\n",
    "losses = []\n",
    "\n",
    "\n",
    "# #Simple GCN Aggregate\n",
    "# from tensorflow_models import mse_rr\n",
    "# from tensorflow_models import rank_loss_rr\n",
    "# tf.random.set_seed(1337)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#             learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "#             name='Adam'\n",
    "#         )\n",
    "\n",
    "# # Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "# def leaky_relu(x):\n",
    "#     return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# hidden_units = 64\n",
    "# activation = leaky_relu\n",
    "# do = 0.1\n",
    "# # Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "# input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "# input_rel = keras.Input(shape=self.Normalized_Adjacency_Matrix.shape[0])\n",
    "\n",
    "\n",
    "# # Load in an already trained LSTM Model\n",
    "# file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "# model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "# pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "#                                               custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# # Change the names to avoid conflicts\n",
    "# pre_trained_lstm.layers[0]._name = 'InputLayer'\n",
    "# pre_trained_lstm.layers[1]._name = 'LSTM'\n",
    "# pre_trained_lstm.layers[2]._name = 'Dense'\n",
    "\n",
    "# # This is the LSTM layer\n",
    "# x = pre_trained_lstm.layers[1](input_seq)\n",
    "# # This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "# # THis\n",
    "# x = pre_trained_lstm.layers[2](x)\n",
    "\n",
    "# # x = Ein_Multiply()([input_rel, x])\n",
    "# # # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # # x = Ein_Multiply()([input_rel, x])\n",
    "# # # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # x = Dense(1, activation=activation)(x)\n",
    "# # model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "# # # Make sure that the weights for the lstm model cannot be updated\n",
    "# # # self.model.layers[1].trainable = False\n",
    "# # # self.model.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in an already trained LSTM model & Adding 1 Graphical Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_models import mse_rr\n",
    "from tensorflow_models import rank_loss_rr\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "\n",
    "# x_train = x_train[:, 0:306, :]\n",
    "# x_val = x_val[:, 0:306, :]\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "input_rel = keras.Input(shape=(DMJ.Normalized_Adjacency_Matrix.shape[0]))\n",
    "\n",
    "\n",
    "# Load in an already trained LSTM Model\n",
    "file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "                                              custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# Change the names to avoid conflicts\n",
    "pre_trained_lstm.layers[0]._name = 'InputLayer'\n",
    "pre_trained_lstm.layers[1]._name = 'LSTM'\n",
    "pre_trained_lstm.layers[2]._name = 'Dense'\n",
    "\n",
    "# # Make sure that the weights for the lstm model cannot be updated\n",
    "pre_trained_lstm.layers[0].trainable = False\n",
    "pre_trained_lstm.layers[1].trainable = False\n",
    "pre_trained_lstm.layers[2].trainable = False\n",
    "\n",
    "# This is the LSTM layer\n",
    "x = pre_trained_lstm.layers[1](input_seq)\n",
    "# This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "\n",
    "\n",
    "# # This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "x = Dense(hidden_units, activation=activation)(x)\n",
    "\n",
    "\n",
    "# This is the original Dense Layer\n",
    "x = pre_trained_lstm.layers[2](x)\n",
    "# x = Dense(1, activation=activation)(x)\n",
    "\n",
    "# # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # x = Ein_Multiply()([input_rel, x])\n",
    "# # x = Dense(hidden_units, activation=activation)(x)\n",
    "# x = Dense(1, activation=activation)(x)\n",
    "# model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View what we just created\n",
    "# model.compile(loss=rank_loss_rr, optimizer=optimizer)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.0002\n",
    "\n",
    "inputs_train = [x_train]\n",
    "train_labels = y_train\n",
    "\n",
    "inputs_val = [x_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# If we're using the GCN\n",
    "# inputs_train.append(DMJ.Normalized_Adjacency_Matrix)\n",
    "# inputs_val.append(DMJ.Normalized_Adjacency_Matrix)\n",
    "\n",
    "# int(inputs_train.shape[0])\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train[0].shape[0]/1),\n",
    "                              epochs=100, validation_data=(inputs_val, val_labels),\n",
    "                   callbacks=[model_checkpoint_callback,\n",
    "                             tf.keras.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                                      verbose=0)])\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "for h in history.history['val_loss']:\n",
    "    losses.append(h)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([x_train, DMJ.Normalized_Adjacency_Matrix], batch_size=880)\n",
    "model_save = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in history.history['val_loss']:\n",
    "#     losses.append(h)\n",
    "# plt.plot(losses)\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.model = model\n",
    "DMJ.model_name = \"3-26-21-1_[25,25,20]CorrAlpha50000_50Epoch_88BatchSize_return_seq\"\n",
    "DMJ.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"2-16-21-Seq1LSTM-F-64HU-[800,239,200]split-full_y\"\n",
    "# new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "# GP.generate_validation_prediction_json_SplitBatch_nofeat(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "# GP.generate_validation_prediction_json_SplitBatch_close_gap(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "model_name = '3-26-21-1_[25,25,20]CorrAlpha1000_60Epoch_88BatchSize_return_seq'\n",
    "model_dir = './ignorable_data/models/[55, 25, 20]_split'\n",
    "# model_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models'\n",
    "past = x_train\n",
    "future = x_val\n",
    "# new_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "sliding_window = x_val.shape[1]\n",
    "\n",
    "window = x_train.shape[1]\n",
    "\n",
    "# GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='gcn', batch_size=880)\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='lstm', batch_size=880)\n",
    "# GCN_output = GP.return_embeddings(model_name, model_dir, past, future, new_dir, w, model_type='gcn')\n",
    "# embeddings = GP.return_embeddings(model_name, model_dir, past, future, new_dir, window, model_type='gcn')\n",
    "\n",
    "#     model_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_35Epoch_80BatchSize_IncreasedVariedLR_RRMSE'\n",
    "#     GP.generate_predictions(model_name, model_dir, past, future, new_dir, w)\n",
    "# GP.generate_validation_prediction_json_SplitBatch(model_name, new_dir, x_g, x_val, sliding_window=sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the training set, these are the validation values that are output from the LSTM model\n",
    "print(embeddings.shape)\n",
    "\n",
    "# This is the NAM that we are using to aggregate the embedding results\n",
    "print(DMJ.Normalized_Adjacency_Matrix[6,6])\n",
    "\n",
    "# This should be the aggregated values output from using the NAM (The dimensions look fine)\n",
    "# new_embeddings = tf.einsum('ntd,nm->mtd', embeddings, DMJ.Normalized_Adjacency_Matrix)\n",
    "new_embeddings = tf.einsum('mn,ntd->mtd', DMJ.Normalized_Adjacency_Matrix, embeddings)\n",
    "print(new_embeddings.shape)\n",
    "\n",
    "print(embeddings[0, -7, 0:10])\n",
    "# print(embeddings[0, -1, 0:10])\n",
    "print('  ')\n",
    "print(new_embeddings[0, -7, 0:10])\n",
    "# print(new_embeddings[0, -1, 0:10])\n",
    "\n",
    "# print(tf.subtract(embeddings[0, -2, 0:10], embeddings[0, -1, 0:10]))\n",
    "# print(tf.subtract(new_embeddings[0, -2, 0:10], new_embeddings[0, -1, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above are the output embeddings of the LSTM layer\n",
    "# Below is 1 output prediction for a simple NAM Aggregate added to a pre-trained LSTM\n",
    "GCN_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_file_name = '3-29-21-1000AlphaLSTM-GoogColab-3DRankLoss-NoDropout309win_309past_309fut'\n",
    "p_file_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits'\n",
    "# p_file_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "close_gap = False\n",
    "\n",
    "use_argmin = False\n",
    "yesterday_pred = False\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred, rr_labels=False)\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\ignorable_data\\datablocks\\[55, 25, 20]_splits'):\n",
    "    for filename in files:\n",
    "#         GP.generate_prediction_results(filename, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "        GP.add_daily_value_to_datablock(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_daily_value_to_datablock_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_cumulative_return_ratio_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        None\n",
    "\n",
    "\n",
    "# use_argmin = True\n",
    "# yesterday_pred = False\n",
    "# GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "# # GP.generate_model_diagnostics_given_sets(p_file_dir + f'/{p_file_name}', future, datablock_folder=new_dir, try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_company(n):\n",
    "    # n = 597\n",
    "#     n = 488\n",
    "    # n = 161\n",
    "    # n = 561\n",
    "    # n = 440\n",
    "    # n = 200\n",
    "    # # Argmin\n",
    "    # n = 780\n",
    "\n",
    "    # n = 876\n",
    "    # n = 110\n",
    "    r = 1\n",
    "    # n = 100\n",
    "    # n=0\n",
    "\n",
    "    r=1\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "#     ax.set_ylim([0,1])\n",
    "    ax.set_xlim([0,100])\n",
    "\n",
    "    time = tf.concat([x_train, x_val, x_test], axis=1)\n",
    "\n",
    "    # ax = fig.add_subplot()\n",
    "    for i in range(r):\n",
    "    #     print(f'{i+n*r} ', end='')\n",
    "    #     ax.plot(time[i+n*r, :, 0])\n",
    "        ax.plot(x_val[i+n*r, :, 0], label='Closing Price', linewidth=2, color='black')\n",
    "    #     ax.plot(rr_val[i+n*r, :], label='Return Ratio')\n",
    "\n",
    "    ax.plot(A[:, i+n*r], label='LSTM-1000 ALPHA0')\n",
    "    ax.plot(B[:, i+n*r], label='LSTM-1000 ALPHA1')\n",
    "    ax.plot(C[:, i+n*r], label='LSTM-1000 ALPHA10')\n",
    "    ax.plot(D[:, i+n*r], label='LSTM-1000 ALPHA100')\n",
    "    ax.plot(E[:, i+n*r], label='LSTM-1000 ALPHA1000')\n",
    "    ax.plot(F[:, i+n*r], label='LSTM-1000 ALPHA10000')\n",
    "#     ax.plot(G[:, i+n*r], label='LSTM-1000 ALPHA50000')\n",
    "#     ax.plot(G[:, i+n*r], label='30')\n",
    "#     ax.plot(H[:, i+n*r], label='60')\n",
    "#     ax.plot(I[:, i+n*r], label='125')\n",
    "#     ax.plot(J[:, i+n*r], label='250')\n",
    "#     ax.plot(K[:, i+n*r], label='500')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Validation Days (Time)')\n",
    "    ax.set_ylabel('Closing Price')\n",
    "    ax.set_title(f'Company {i+n*r}')\n",
    "\n",
    "    print(DMJ.entities[n])\n",
    "\n",
    "\n",
    "\n",
    "plot_company(488)\n",
    "plot_company(221)\n",
    "plot_company(100)\n",
    "plot_company(732)\n",
    "plot_company(780)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha0_50Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "A = GP.test_obj\n",
    "\n",
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha1_60Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "B = GP.test_obj\n",
    "\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha10_50Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "C = GP.test_obj\n",
    "\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha100_50Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "D = GP.test_obj\n",
    "\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha1000_60Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "E = GP.test_obj\n",
    "\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha10000_50Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "F = GP.test_obj\n",
    "\n",
    "pred_file = '3-26-21-1_[25,25,20]CorrAlpha50000_50Epoch_88BatchSize_return_seq309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "G = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-125win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# I = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-250win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# J = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-500win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# K = GP.test_obj\n",
    "# ##################################################################################################################\n",
    "\n",
    "# pred_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/'\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# A = GP.test_obj\n",
    "\n",
    "# # pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "# pred_file = '02-17-2021--07--37-1LSTM-F-1ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# B = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--54-1LSTM-F-10.0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# C = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--08--11-1LSTM-F-100.0ALPHA-[55,25,20]split-[None, 1]-30Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# D = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--08--29-1LSTM-F-1000.0ALPHA-[55,25,20]split-[None, 1]-20Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# E = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "# pred_dir = r\".\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/\"\n",
    "# pred_file = '3-11-21-1_LSTM_[25,25,20]_60Epoch_40BatchSize_100ALPHA_RR_Labels309win_309past_309fut_220predBatch'\n",
    "# GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/', rr_labels=True)\n",
    "# # GP.generate_model_diagnostics_given_sets_close_gap(pred_dir + pred_file, x_val, datablock_folder='./ignorable_data/datablocks/', try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac101e9de1ad49f985d2d2b736ee5988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='Search', placeholder='Tag'), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Let's look at our relations_file real quick\n",
    "with open('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/updated_relations.json') as read_file:\n",
    "    relations_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(relations_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DMJ.Normalized_Adjacency_Matrix.shape)\n",
    "s = 0\n",
    "for i in DMJ.Normalized_Adjacency_Matrix[89]:\n",
    "    s = i + s\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
