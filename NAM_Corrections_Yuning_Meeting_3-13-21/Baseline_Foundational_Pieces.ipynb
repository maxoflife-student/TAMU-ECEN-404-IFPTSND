{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Change the working directory back to the original to keep paths the same between files\n",
    "os.chdir(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND')\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allows for scrolling windows to be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import TF_models and truncate the entities to only contain 880 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e33134a68f44ccb3158ec74cba39a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Dropdown(description='Model Types:', options=('lstm', 'lstm_gcn_1', 'lstm_gcn_2', 'lstm_gcn_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix = DMJ.Normalized_Adjacency_Matrix[0:880, 0:880]\n",
    "DMJ.XX_tf = DMJ.XX_tf[0:-1, :, :]\n",
    "DMJ.YY_tf = DMJ.YY_tf[0:-1, :]\n",
    "DMJ.RR_tf = DMJ.RR_tf[0:-1, :]\n",
    "DMJ.entities = DMJ.entities[0:-1]\n",
    "DMJ.entities_idx.pop('ZUMZ')\n",
    "\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we split the data into time batches ourselves and trained them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 1239, 5)\n",
      "(880, 309, 5)\n",
      "(880, 309, 5)\n",
      "tf.Tensor(0.455917, shape=(), dtype=float32)\n",
      "tf.Tensor(0.461538, shape=(), dtype=float32)\n",
      "tf.Tensor(0.012328968, shape=(), dtype=float32)\n",
      "tf.Tensor(0.425614, shape=(), dtype=float32)\n",
      "tf.Tensor(0.415741, shape=(), dtype=float32)\n",
      "tf.Tensor([0.415741], shape=(1,), dtype=float32)\n",
      "(None, 309, 5)\n"
     ]
    }
   ],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:2, :, :]\n",
    "# one_y = DMJ.YY_tf[0:2, :]\n",
    "one_x = DMJ.XX_tf\n",
    "print(DMJ.XX_tf.shape)\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:-1, :, :]\n",
    "# one_y = DMJ.YY_tf[0:-1, :]\n",
    "\n",
    "# # Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "# time_split = [1000, 239,]\n",
    "# x_train, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# y_train, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "#                                      axis=1)\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# y_train = y_train[:, -1]\n",
    "# y_val = y_val[:, -1]\n",
    "\n",
    "# # Once we have the data partitioned, let's split it into 8 sets of 125\n",
    "# batch_splits = [1]*8\n",
    "# # Create a list of 8 different time batches\n",
    "# x_train_batches = []\n",
    "# x_train_batches.append(tf.split(x_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# x_train_batches = [item for sublist in x_train_batches for item in sublist]\n",
    "\n",
    "# y_train_batches = []\n",
    "# y_train_batches.append(tf.split(y_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# y_train_batches = [item for sublist in y_train_batches for item in sublist]\n",
    "\n",
    "# # Lets try truncating the values for the labels to only be the final value\n",
    "# for i in range(len(y_train_batches)):\n",
    "#     y_train_batches[i] = y_train_batches[i][:, -1]\n",
    "\n",
    "# print(x_train[0, -1, 0])\n",
    "\n",
    "# print(rr_train[0, -1])\n",
    "\n",
    "# print(x_val[0, 0, 0])\n",
    "# print(y_train[0, -1])\n",
    "# for set in [x_train, x_g, x_val, x_test]:\n",
    "print(x_val.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_train[0, 0, 0])\n",
    "print(x_train[0, 1, 0])\n",
    "print(rr_train[0, 0])\n",
    "\n",
    "print(x_train[0, -1, 0])\n",
    "print(x_train[0, -2, 0])\n",
    "print(x_train[0, -2:-1, 0])\n",
    "\n",
    "print(keras.Input(shape=(x_train.shape[1], x_train.shape[2])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the one-hot encoded adjacency matrix to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Text(value='Loading Normalized Adjacency Matrix:', disabled=True, layout=Layout(width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Text(value='Loading Normalized Adjacency Matrix:', disabled=True, layout=Layout(width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import explore_entities\n",
    "# Load in the code\n",
    "GE = Graph_Entities('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ - Yuning/')\n",
    "# Generate the baseline components we're wokring with\n",
    "NAM, Adj, Rel = GE.get_matrix_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the dimensionality of normalized adjacency matrix we have so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symetrically Normalized DMJ: (881, 881)\n",
      "Normalized\n",
      "####\n",
      "One-Hot Encoded Relationships Split by Group: (92, 881, 881)\n",
      "####\n",
      "One-Hot Encoded All Relationships: (881, 881)\n",
      "Rows Not Normalized\n",
      "Columns Not Normalized\n",
      "####\n",
      "12.5\n"
     ]
    }
   ],
   "source": [
    "# Checks to see if a matrix is normalized among it's rows and columns to 6 decimal places\n",
    "def is_normalized(m):\n",
    "    m_t = np.transpose(m)\n",
    "    r_trigger = True\n",
    "    c_trigger = True\n",
    "    for i in m:\n",
    "        if (round(np.sum(i), 6)) != 1 and r_trigger:\n",
    "            print('Rows Not Normalized')\n",
    "            r_trigger = not r_trigger\n",
    "    for i in m_t:\n",
    "        if (round(np.sum(i), 6)) != 1 and c_trigger:\n",
    "            print('Columns Not Normalized')\n",
    "            c_trigger = not c_trigger\n",
    "    \n",
    "    if r_trigger and c_trigger:\n",
    "        print('Normalized')\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Created with D^-1/2 * A * D^-1/2\n",
    "print(f\"Symetrically Normalized DMJ: {NAM.shape}\")\n",
    "is_normalized(NAM)\n",
    "print(\"####\")\n",
    "# All relationships, but still seperated by the different groups\n",
    "print(f\"One-Hot Encoded Relationships Split by Group: {Rel.shape}\")\n",
    "print(\"####\")\n",
    "# All relationships squished into one matrix\n",
    "print(f\"One-Hot Encoded All Relationships: {Adj.shape}\")\n",
    "is_normalized(Adj)\n",
    "print(\"####\")\n",
    "\n",
    "None\n",
    "\n",
    "print(np.sum(Rel[9, :, :])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "one_x = DMJ.XX_tf\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in an already trained LSTM model & Adding Topographical Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_models import mse_rr\n",
    "from tensorflow_models import rank_loss_rr\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "\n",
    "# Create a function for the Tensorflow implementation of leaky_relu\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0\n",
    "\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "# For the time-series data\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "\n",
    "# For the Adjacency Matrix we would like to introduce to the algorithm\n",
    "adj_matrix = Adj\n",
    "\n",
    "# Since some implementations removed the last company\n",
    "if adj_matrix.shape[0] == 881:\n",
    "    adj_matrix = adj_matrix[0:-1, 0:-1]\n",
    "\n",
    "input_rel = keras.Input(shape=(adj_matrix.shape[0]))\n",
    "\n",
    "\n",
    "# Load in an already trained LSTM Model\n",
    "file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "                                              custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# Change the names to avoid conflicts\n",
    "pre_trained_lstm.layers[0]._name = 'Original-InputLayer'\n",
    "pre_trained_lstm.layers[1]._name = 'Original-LSTM'\n",
    "pre_trained_lstm.layers[2]._name = 'Original-Dense'\n",
    "\n",
    "# # Make sure that the weights for the lstm model cannot be updated\n",
    "pre_trained_lstm.layers[0].trainable = False\n",
    "pre_trained_lstm.layers[1].trainable = False\n",
    "pre_trained_lstm.layers[2].trainable = False\n",
    "\n",
    "# This is the LSTM layer, input_seq is not carried over from the original because we don't care how it was initalized\n",
    "x = pre_trained_lstm.layers[1](input_seq)\n",
    "\n",
    "# This is a seperate input layer that will take in the relational matrix\n",
    "y = tf.keras.layers.Dense(adj_matrix.shape[0], activation)(input_rel)\n",
    "\n",
    "# # This is one aggregation using the NAM\n",
    "x = Ein_Multiply()([y, x])\n",
    "# x = Dense(hidden_units, activation=activation)(x)\n",
    "\n",
    "# This is the original Dense Layer\n",
    "# x = pre_trained_lstm.layers[2](x)\n",
    "\n",
    "This is a new Dense layer we might want to experiment with\n",
    "x = Dense(1, activation=activation)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 880)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 309, 5)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 880)          775280      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Original-LSTM (LSTM)            (None, 309, 64)      17920       input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ein__multiply_7 (Ein_Multiply)  (None, 309, 64)      0           dense_14[0][0]                   \n",
      "                                                                 Original-LSTM[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Original-Dense (Dense)          (None, 309, 1)       65          ein__multiply_7[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 793,265\n",
      "Trainable params: 775,280\n",
      "Non-trainable params: 17,985\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View what we just created\n",
    "# model.compile(loss=rank_loss_rr, optimizer=optimizer)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 0.2316 - val_loss: 0.1256\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.1576 - val_loss: 0.1255\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.1797 - val_loss: 0.1258\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.1404 - val_loss: 0.1265\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.1774 - val_loss: 0.1274\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.2155 - val_loss: 0.1280\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.1788 - val_loss: 0.1289\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.2347 - val_loss: 0.1300\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.1424 - val_loss: 0.1311\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.1966 - val_loss: 0.1319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fe61082940>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.00007\n",
    "\n",
    "inputs_train = [x_train]\n",
    "train_labels = y_train\n",
    "\n",
    "inputs_val = [x_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# If we're using the GCN\n",
    "inputs_train.append(adj_matrix)\n",
    "inputs_val.append(adj_matrix)\n",
    "\n",
    "# int(inputs_train.shape[0])\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train[0].shape[0]),\n",
    "                              epochs=10, validation_data=(inputs_val, val_labels),\n",
    "                   callbacks=[model_checkpoint_callback,\n",
    "                             tf.keras.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                                      verbose=0)])\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "for h in history.history['val_loss']:\n",
    "    losses.append(h)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([x_train, DMJ.Normalized_Adjacency_Matrix], batch_size=880)\n",
    "model_save = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in history.history['val_loss']:\n",
    "#     losses.append(h)\n",
    "# plt.plot(losses)\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None--0Epochs-None-Loss-None-HU-'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DMJ.model = model\n",
    "DMJ.model_name = \"3-15-21-1_LSTM_[25,25,20]_650Epoch_881BatchSize_TFMSE_881Dense-X-SquishAdj\"\n",
    "DMJ.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Model: '3-15-21-1_LSTM_[25,25,20]_650Epoch_881BatchSize_TFMSE_881Dense-X-SquishAdj'\n",
      "Total number of days: 308\n",
      "Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | "
     ]
    }
   ],
   "source": [
    "# model_name = \"2-16-21-Seq1LSTM-F-64HU-[800,239,200]split-full_y\"\n",
    "# new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "# GP.generate_validation_prediction_json_SplitBatch_nofeat(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "# GP.generate_validation_prediction_json_SplitBatch_close_gap(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "model_name = '3-15-21-1_LSTM_[25,25,20]_650Epoch_881BatchSize_TFMSE_881Dense-X-SquishAdj'\n",
    "model_dir = './ignorable_data/models/[55, 25, 20]_split'\n",
    "# model_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models'\n",
    "past = x_train\n",
    "future = x_val\n",
    "# new_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "sliding_window = x_val.shape[1]\n",
    "\n",
    "window = x_train.shape[1]\n",
    "\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='gcn', batch_size=881)\n",
    "# GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='lstm', batch_size=880)\n",
    "# GCN_output = GP.return_embeddings(model_name, model_dir, past, future, new_dir, w, model_type='gcn')\n",
    "# embeddings = GP.return_embeddings(model_name, model_dir, past, future, new_dir, window, model_type='gcn')\n",
    "\n",
    "#     model_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_35Epoch_80BatchSize_IncreasedVariedLR_RRMSE'\n",
    "#     GP.generate_predictions(model_name, model_dir, past, future, new_dir, w)\n",
    "# GP.generate_validation_prediction_json_SplitBatch(model_name, new_dir, x_g, x_val, sliding_window=sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the training set, these are the validation values that are output from the LSTM model\n",
    "print(embeddings.shape)\n",
    "\n",
    "# This is the NAM that we are using to aggregate the embedding results\n",
    "print(DMJ.Normalized_Adjacency_Matrix[6,6])\n",
    "\n",
    "# This should be the aggregated values output from using the NAM (The dimensions look fine)\n",
    "# new_embeddings = tf.einsum('ntd,nm->mtd', embeddings, DMJ.Normalized_Adjacency_Matrix)\n",
    "new_embeddings = tf.einsum('mn,ntd->mtd', DMJ.Normalized_Adjacency_Matrix, embeddings)\n",
    "print(new_embeddings.shape)\n",
    "\n",
    "print(embeddings[0, -7, 0:10])\n",
    "# print(embeddings[0, -1, 0:10])\n",
    "print('  ')\n",
    "print(new_embeddings[0, -7, 0:10])\n",
    "# print(new_embeddings[0, -1, 0:10])\n",
    "\n",
    "# print(tf.subtract(embeddings[0, -2, 0:10], embeddings[0, -1, 0:10]))\n",
    "# print(tf.subtract(new_embeddings[0, -2, 0:10], new_embeddings[0, -1, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above are the output embeddings of the LSTM layer\n",
    "# Below is 1 output prediction for a simple NAM Aggregate added to a pre-trained LSTM\n",
    "GCN_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_file_name = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer309win_309past_309fut'\n",
    "p_file_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits'\n",
    "# p_file_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "close_gap = False\n",
    "\n",
    "use_argmin = False\n",
    "yesterday_pred = False\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred, rr_labels=True)\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\ignorable_data\\datablocks\\[55, 25, 20]_splits'):\n",
    "    for filename in files:\n",
    "#         GP.generate_prediction_results(filename, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "        GP.add_daily_value_to_datablock(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_daily_value_to_datablock_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_cumulative_return_ratio_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        None\n",
    "\n",
    "\n",
    "# use_argmin = True\n",
    "# yesterday_pred = False\n",
    "# GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "# # GP.generate_model_diagnostics_given_sets(p_file_dir + f'/{p_file_name}', future, datablock_folder=new_dir, try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 597\n",
    "# n = 488\n",
    "# n = 161\n",
    "# n = 561\n",
    "# n = 440\n",
    "# n = 200\n",
    "# # Argmin\n",
    "# n = 780\n",
    "\n",
    "# n = 876\n",
    "# n = 110\n",
    "r = 1\n",
    "# n = 100\n",
    "n=0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# ax.set_ylim([0,1])\n",
    "\n",
    "time = tf.concat([x_train, x_val, x_test], axis=1)\n",
    "\n",
    "# ax = fig.add_subplot()\n",
    "for i in range(r):\n",
    "#     print(f'{i+n*r} ', end='')\n",
    "#     ax.plot(time[i+n*r, :, 0])\n",
    "    ax.plot(x_val[i+n*r, :, 0], label='Closing Price')\n",
    "#     ax.plot(rr_val[i+n*r, :], label='Return Ratio')\n",
    "    \n",
    "    ax.plot(A[:, i+n*r], label='LSTM-1000 ALPHA')\n",
    "    ax.plot(B[:, i+n*r], label='LSTM-1000 ALPHA + 1 Dense')\n",
    "    ax.plot(C[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM')\n",
    "    ax.plot(D[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM + 1 Dense')\n",
    "#     ax.plot(D[:, i+n*r], label='15')\n",
    "#     ax.plot(E[:, i+n*r], label='20')\n",
    "#     ax.plot(F[:, i+n*r], label='25')\n",
    "#     ax.plot(G[:, i+n*r], label='30')\n",
    "#     ax.plot(H[:, i+n*r], label='60')\n",
    "#     ax.plot(I[:, i+n*r], label='125')\n",
    "#     ax.plot(J[:, i+n*r], label='250')\n",
    "#     ax.plot(K[:, i+n*r], label='500')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Validation Days (Time)')\n",
    "    ax.set_ylabel('Closing Price')\n",
    "    ax.set_title(f'Company {i+n*r}')\n",
    "# Orange\n",
    "# Green\n",
    "# Red\n",
    "# Purple\n",
    "# Brown\n",
    "\n",
    "print(DMJ.entities[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]Reloaded_1000ALPHA309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "A = GP.test_obj\n",
    "\n",
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "B = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1NAMAGGREGATE309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "C = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1DenseGCN_TFMSE_FullTrain309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "D = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-20win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# E = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-25win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# F = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-30win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# G = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-60win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# H = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-125win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# I = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-250win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# J = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-500win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# K = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "# pred_dir = r\".\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/\"\n",
    "# pred_file = '3-11-21-1_LSTM_[25,25,20]_60Epoch_40BatchSize_100ALPHA_RR_Labels309win_309past_309fut_220predBatch'\n",
    "# GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/', rr_labels=True)\n",
    "# # GP.generate_model_diagnostics_given_sets_close_gap(pred_dir + pred_file, x_val, datablock_folder='./ignorable_data/datablocks/', try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Let's look at our relations_file real quick\n",
    "with open('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/updated_relations.json') as read_file:\n",
    "    relations_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(relations_dict.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
