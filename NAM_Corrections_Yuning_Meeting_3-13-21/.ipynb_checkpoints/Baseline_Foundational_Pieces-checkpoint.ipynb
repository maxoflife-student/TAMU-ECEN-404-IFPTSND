{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Change the working directory back to the original to keep paths the same between files\n",
    "os.chdir(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND')\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allows for scrolling windows to be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import TF_models and truncate the entities to only contain 880 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33647934a9c14a6fa8d46f004f572f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Dropdown(description='Model Types:', options=('lstm', 'lstm_gcn_1', 'lstm_gcn_2', 'lstm_gcn_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix = DMJ.Normalized_Adjacency_Matrix[0:880, 0:880]\n",
    "DMJ.XX_tf = DMJ.XX_tf[0:-1, :, :]\n",
    "DMJ.YY_tf = DMJ.YY_tf[0:-1, :]\n",
    "DMJ.RR_tf = DMJ.RR_tf[0:-1, :]\n",
    "DMJ.entities = DMJ.entities[0:-1]\n",
    "DMJ.entities_idx.pop('ZUMZ')\n",
    "\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we split the data into time batches ourselves and trained them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:2, :, :]\n",
    "# one_y = DMJ.YY_tf[0:2, :]\n",
    "one_x = DMJ.XX_tf\n",
    "print(DMJ.XX_tf.shape)\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:-1, :, :]\n",
    "# one_y = DMJ.YY_tf[0:-1, :]\n",
    "\n",
    "# # Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "# time_split = [1000, 239,]\n",
    "# x_train, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# y_train, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "#                                      axis=1)\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# y_train = y_train[:, -1]\n",
    "# y_val = y_val[:, -1]\n",
    "\n",
    "# # Once we have the data partitioned, let's split it into 8 sets of 125\n",
    "# batch_splits = [1]*8\n",
    "# # Create a list of 8 different time batches\n",
    "# x_train_batches = []\n",
    "# x_train_batches.append(tf.split(x_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# x_train_batches = [item for sublist in x_train_batches for item in sublist]\n",
    "\n",
    "# y_train_batches = []\n",
    "# y_train_batches.append(tf.split(y_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# y_train_batches = [item for sublist in y_train_batches for item in sublist]\n",
    "\n",
    "# # Lets try truncating the values for the labels to only be the final value\n",
    "# for i in range(len(y_train_batches)):\n",
    "#     y_train_batches[i] = y_train_batches[i][:, -1]\n",
    "\n",
    "# print(x_train[0, -1, 0])\n",
    "\n",
    "# print(rr_train[0, -1])\n",
    "\n",
    "# print(x_val[0, 0, 0])\n",
    "# print(y_train[0, -1])\n",
    "# for set in [x_train, x_g, x_val, x_test]:\n",
    "print(x_val.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_train[0, 0, 0])\n",
    "print(x_train[0, 1, 0])\n",
    "print(rr_train[0, 0])\n",
    "\n",
    "print(x_train[0, -1, 0])\n",
    "print(x_train[0, -2, 0])\n",
    "print(x_train[0, -2:-1, 0])\n",
    "\n",
    "print(keras.Input(shape=(x_train.shape[1], x_train.shape[2])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training an LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_models import mse_rr\n",
    "\n",
    "from tensorflow_models import rank_loss_rr\n",
    "from tensorflow_models import rank_loss_rr_labels\n",
    "\n",
    "from tensorflow_models import rank_loss_func_rr\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "# Creating an LSTM model\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "# input_rel = keras.Input(shape=(DMJ.Normalized_Adjacency_Matrix.shape[0]))\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0\n",
    "# x = tf.keras.layers.Masking(mask_value=0.0)(input_seq)\n",
    "x = LSTM(hidden_units, activation=activation, dropout=do, recurrent_dropout=do, return_sequences=True)(input_seq)\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "# x = tf.keras.layers.RepeatVector(x_train_batches[0].shape[1])(x)\n",
    "# x = LSTM(hidden_units, activation=activation, dropout=do)(x)\n",
    "x = Dense(1, activation=activation)(x)\n",
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "# model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "model.compile(loss=rank_loss_rr_labels, optimizer=optimizer)\n",
    "model.summary()\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(hidden_units, activation=activation, input_shape=(x_train_batches[0].shape[1], x_train_batches[0].shape[2])))\n",
    "# model.add(Dense(1, activation=activation))\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.summary()\n",
    "losses = []\n",
    "\n",
    "\n",
    "# #Simple GCN Aggregate\n",
    "# from tensorflow_models import mse_rr\n",
    "# from tensorflow_models import rank_loss_rr\n",
    "# tf.random.set_seed(1337)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#             learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "#             name='Adam'\n",
    "#         )\n",
    "\n",
    "# # Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "# def leaky_relu(x):\n",
    "#     return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# hidden_units = 64\n",
    "# activation = leaky_relu\n",
    "# do = 0.1\n",
    "# # Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "# input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "# input_rel = keras.Input(shape=self.Normalized_Adjacency_Matrix.shape[0])\n",
    "\n",
    "\n",
    "# # Load in an already trained LSTM Model\n",
    "# file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "# model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "# pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "#                                               custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# # Change the names to avoid conflicts\n",
    "# pre_trained_lstm.layers[0]._name = 'InputLayer'\n",
    "# pre_trained_lstm.layers[1]._name = 'LSTM'\n",
    "# pre_trained_lstm.layers[2]._name = 'Dense'\n",
    "\n",
    "# # This is the LSTM layer\n",
    "# x = pre_trained_lstm.layers[1](input_seq)\n",
    "# # This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "# # THis\n",
    "# x = pre_trained_lstm.layers[2](x)\n",
    "\n",
    "# # x = Ein_Multiply()([input_rel, x])\n",
    "# # # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # # x = Ein_Multiply()([input_rel, x])\n",
    "# # # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # x = Dense(1, activation=activation)(x)\n",
    "# # model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "# # # Make sure that the weights for the lstm model cannot be updated\n",
    "# # # self.model.layers[1].trainable = False\n",
    "# # # self.model.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in an already trained LSTM model & Adding 1 Graphical Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_models import mse_rr\n",
    "from tensorflow_models import rank_loss_rr\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "\n",
    "# x_train = x_train[:, 0:306, :]\n",
    "# x_val = x_val[:, 0:306, :]\n",
    "\n",
    "# Create a function for the Tensorflow implementation of Tensorflow 2\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "input_rel = keras.Input(shape=(DMJ.Normalized_Adjacency_Matrix.shape[0]))\n",
    "\n",
    "\n",
    "# Load in an already trained LSTM Model\n",
    "file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "                                              custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# Change the names to avoid conflicts\n",
    "pre_trained_lstm.layers[0]._name = 'InputLayer'\n",
    "pre_trained_lstm.layers[1]._name = 'LSTM'\n",
    "pre_trained_lstm.layers[2]._name = 'Dense'\n",
    "\n",
    "# # Make sure that the weights for the lstm model cannot be updated\n",
    "pre_trained_lstm.layers[0].trainable = False\n",
    "pre_trained_lstm.layers[1].trainable = False\n",
    "pre_trained_lstm.layers[2].trainable = False\n",
    "\n",
    "# This is the LSTM layer\n",
    "x = pre_trained_lstm.layers[1](input_seq)\n",
    "# This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "\n",
    "\n",
    "# # This is one aggregation using the NAM\n",
    "# x = Ein_Multiply()([input_rel, x])\n",
    "x = Dense(hidden_units, activation=activation)(x)\n",
    "\n",
    "\n",
    "# This is the original Dense Layer\n",
    "x = pre_trained_lstm.layers[2](x)\n",
    "# x = Dense(1, activation=activation)(x)\n",
    "\n",
    "# # x = Dense(hidden_units, activation=activation)(x)\n",
    "# # x = Ein_Multiply()([input_rel, x])\n",
    "# # x = Dense(hidden_units, activation=activation)(x)\n",
    "# x = Dense(1, activation=activation)(x)\n",
    "# model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "model = tf.keras.Model(inputs=[input_seq, input_rel], outputs=x)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View what we just created\n",
    "# model.compile(loss=rank_loss_rr, optimizer=optimizer)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.00001\n",
    "\n",
    "inputs_train = [x_train]\n",
    "train_labels = y_train\n",
    "\n",
    "inputs_val = [x_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# If we're using the GCN\n",
    "inputs_train.append(DMJ.Normalized_Adjacency_Matrix)\n",
    "inputs_val.append(DMJ.Normalized_Adjacency_Matrix)\n",
    "\n",
    "# int(inputs_train.shape[0])\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train[0].shape[0]),\n",
    "                              epochs=10, validation_data=(inputs_val, val_labels),\n",
    "                   callbacks=[model_checkpoint_callback,\n",
    "                             tf.keras.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                                      verbose=0)])\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "for h in history.history['val_loss']:\n",
    "    losses.append(h)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([x_train, DMJ.Normalized_Adjacency_Matrix], batch_size=880)\n",
    "model_save = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in history.history['val_loss']:\n",
    "#     losses.append(h)\n",
    "# plt.plot(losses)\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.model = model\n",
    "DMJ.model_name = \"3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer\"\n",
    "DMJ.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"2-16-21-Seq1LSTM-F-64HU-[800,239,200]split-full_y\"\n",
    "# new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "# GP.generate_validation_prediction_json_SplitBatch_nofeat(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "# GP.generate_validation_prediction_json_SplitBatch_close_gap(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "model_name = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer'\n",
    "model_dir = './ignorable_data/models/[55, 25, 20]_split'\n",
    "# model_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models'\n",
    "past = x_train\n",
    "future = x_val\n",
    "# new_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "sliding_window = x_val.shape[1]\n",
    "\n",
    "window = x_train.shape[1]\n",
    "\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='gcn', batch_size=880)\n",
    "# GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='lstm', batch_size=880)\n",
    "# GCN_output = GP.return_embeddings(model_name, model_dir, past, future, new_dir, w, model_type='gcn')\n",
    "# embeddings = GP.return_embeddings(model_name, model_dir, past, future, new_dir, window, model_type='gcn')\n",
    "\n",
    "#     model_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_35Epoch_80BatchSize_IncreasedVariedLR_RRMSE'\n",
    "#     GP.generate_predictions(model_name, model_dir, past, future, new_dir, w)\n",
    "# GP.generate_validation_prediction_json_SplitBatch(model_name, new_dir, x_g, x_val, sliding_window=sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the training set, these are the validation values that are output from the LSTM model\n",
    "print(embeddings.shape)\n",
    "\n",
    "# This is the NAM that we are using to aggregate the embedding results\n",
    "print(DMJ.Normalized_Adjacency_Matrix[6,6])\n",
    "\n",
    "# This should be the aggregated values output from using the NAM (The dimensions look fine)\n",
    "# new_embeddings = tf.einsum('ntd,nm->mtd', embeddings, DMJ.Normalized_Adjacency_Matrix)\n",
    "new_embeddings = tf.einsum('mn,ntd->mtd', DMJ.Normalized_Adjacency_Matrix, embeddings)\n",
    "print(new_embeddings.shape)\n",
    "\n",
    "print(embeddings[0, -7, 0:10])\n",
    "# print(embeddings[0, -1, 0:10])\n",
    "print('  ')\n",
    "print(new_embeddings[0, -7, 0:10])\n",
    "# print(new_embeddings[0, -1, 0:10])\n",
    "\n",
    "# print(tf.subtract(embeddings[0, -2, 0:10], embeddings[0, -1, 0:10]))\n",
    "# print(tf.subtract(new_embeddings[0, -2, 0:10], new_embeddings[0, -1, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above are the output embeddings of the LSTM layer\n",
    "# Below is 1 output prediction for a simple NAM Aggregate added to a pre-trained LSTM\n",
    "GCN_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_file_name = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer309win_309past_309fut'\n",
    "p_file_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits'\n",
    "# p_file_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "close_gap = False\n",
    "\n",
    "use_argmin = False\n",
    "yesterday_pred = False\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred, rr_labels=True)\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\ignorable_data\\datablocks\\[55, 25, 20]_splits'):\n",
    "    for filename in files:\n",
    "#         GP.generate_prediction_results(filename, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "        GP.add_daily_value_to_datablock(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_daily_value_to_datablock_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_cumulative_return_ratio_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        None\n",
    "\n",
    "\n",
    "# use_argmin = True\n",
    "# yesterday_pred = False\n",
    "# GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "# # GP.generate_model_diagnostics_given_sets(p_file_dir + f'/{p_file_name}', future, datablock_folder=new_dir, try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 597\n",
    "# n = 488\n",
    "# n = 161\n",
    "# n = 561\n",
    "# n = 440\n",
    "# n = 200\n",
    "# # Argmin\n",
    "# n = 780\n",
    "\n",
    "# n = 876\n",
    "# n = 110\n",
    "r = 1\n",
    "# n = 100\n",
    "n=0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# ax.set_ylim([0,1])\n",
    "\n",
    "time = tf.concat([x_train, x_val, x_test], axis=1)\n",
    "\n",
    "# ax = fig.add_subplot()\n",
    "for i in range(r):\n",
    "#     print(f'{i+n*r} ', end='')\n",
    "#     ax.plot(time[i+n*r, :, 0])\n",
    "    ax.plot(x_val[i+n*r, :, 0], label='Closing Price')\n",
    "#     ax.plot(rr_val[i+n*r, :], label='Return Ratio')\n",
    "    \n",
    "    ax.plot(A[:, i+n*r], label='LSTM-1000 ALPHA')\n",
    "    ax.plot(B[:, i+n*r], label='LSTM-1000 ALPHA + 1 Dense')\n",
    "    ax.plot(C[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM')\n",
    "    ax.plot(D[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM + 1 Dense')\n",
    "#     ax.plot(D[:, i+n*r], label='15')\n",
    "#     ax.plot(E[:, i+n*r], label='20')\n",
    "#     ax.plot(F[:, i+n*r], label='25')\n",
    "#     ax.plot(G[:, i+n*r], label='30')\n",
    "#     ax.plot(H[:, i+n*r], label='60')\n",
    "#     ax.plot(I[:, i+n*r], label='125')\n",
    "#     ax.plot(J[:, i+n*r], label='250')\n",
    "#     ax.plot(K[:, i+n*r], label='500')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Validation Days (Time)')\n",
    "    ax.set_ylabel('Closing Price')\n",
    "    ax.set_title(f'Company {i+n*r}')\n",
    "# Orange\n",
    "# Green\n",
    "# Red\n",
    "# Purple\n",
    "# Brown\n",
    "\n",
    "print(DMJ.entities[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]Reloaded_1000ALPHA309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "A = GP.test_obj\n",
    "\n",
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "B = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1NAMAGGREGATE309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "C = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1DenseGCN_TFMSE_FullTrain309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "D = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-20win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# E = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-25win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# F = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-30win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# G = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-60win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# H = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-125win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# I = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-250win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# J = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-500win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# K = GP.test_obj\n",
    "# ##################################################################################################################\n",
    "\n",
    "# pred_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/'\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# A = GP.test_obj\n",
    "\n",
    "# # pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "# pred_file = '02-17-2021--07--37-1LSTM-F-1ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# B = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--54-1LSTM-F-10.0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# C = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--08--11-1LSTM-F-100.0ALPHA-[55,25,20]split-[None, 1]-30Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# D = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--08--29-1LSTM-F-1000.0ALPHA-[55,25,20]split-[None, 1]-20Epochs-rlf-Loss-64-HU-'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# E = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "# pred_dir = r\".\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/\"\n",
    "# pred_file = '3-11-21-1_LSTM_[25,25,20]_60Epoch_40BatchSize_100ALPHA_RR_Labels309win_309past_309fut_220predBatch'\n",
    "# GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/', rr_labels=True)\n",
    "# # GP.generate_model_diagnostics_given_sets_close_gap(pred_dir + pred_file, x_val, datablock_folder='./ignorable_data/datablocks/', try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Let's look at our relations_file real quick\n",
    "with open('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/updated_relations.json') as read_file:\n",
    "    relations_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(relations_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DMJ.Normalized_Adjacency_Matrix.shape)\n",
    "s = 0\n",
    "for i in DMJ.Normalized_Adjacency_Matrix[89]:\n",
    "    s = i + s\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
