{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Change the working directory back to the original to keep paths the same between files\n",
    "os.chdir(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND')\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allows for scrolling windows to be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "    IPython.OutputArea.auto_scroll_threshold = 9999\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import TF_models and truncate the entities to only contain 880 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ffde3b6b744891ac7b43799b70867b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Dropdown(description='Model Types:', options=('lstm', 'lstm_gcn_1', 'lstm_gcn_2', 'lstm_gcn_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix = DMJ.Normalized_Adjacency_Matrix[0:880, 0:880]\n",
    "DMJ.XX_tf = DMJ.XX_tf[0:-1, :, :]\n",
    "DMJ.YY_tf = DMJ.YY_tf[0:-1, :]\n",
    "DMJ.RR_tf = DMJ.RR_tf[0:-1, :]\n",
    "DMJ.entities = DMJ.entities[0:-1]\n",
    "DMJ.entities_idx.pop('ZUMZ')\n",
    "\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the one-hot encoded adjacency matrix to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Text(value='Loading Normalized Adjacency Matrix:', disabled=True, layout=Layout(width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Text(value='Loading Normalized Adjacency Matrix:', disabled=True, layout=Layout(width='auto'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import explore_entities\n",
    "# Load in the code\n",
    "GE = Graph_Entities('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ - Yuning/')\n",
    "# Generate the baseline components we're wokring with\n",
    "NAM, Adj, Rel = GE.get_matrix_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "one_x = DMJ.XX_tf\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [40,40,20]\n",
    "x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_file_dir = r'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models\\after_3_dimensional_RL_change'\n",
    "\n",
    "files = os.listdir(p_file_dir)\n",
    "files = ['/' + i for i in files if i != '.ipynb_checkpoints']\n",
    "\n",
    "files = [\"04-01-21-1AlphaLSTM-GooCol-3DRL-NoDropout-85Ep-10BS\",\n",
    "        \"04-01-21-10000AlphaLSTM-GooCol-3DRL-NoDropout-85Ep-10BS\"]\n",
    "\n",
    "past = x_val\n",
    "future = x_test\n",
    "# new_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "sliding_window = x_val.shape[1]\n",
    "\n",
    "window = x_train.shape[1]\n",
    "input_Adj_matrix = adj_matrix\n",
    "# GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='gcn', input_Adj_matrix=adj_matrix, batch_size=881)\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='lstm', batch_size=881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-08-21-10Alpha22BsLSTM-GooCol-3DRL-150Ep-3DEXPLICIT_EXTENDED495win_495past_249fut\n",
      "1\n",
      "4\n",
      "8\n",
      "16\n",
      "44\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "p_file_dir = r'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\to_test_on_testing_set'\n",
    "\n",
    "files = os.listdir(p_file_dir)\n",
    "files = ['/' + i for i in files if i != '.ipynb_checkpoints']\n",
    "\n",
    "files = [\"04-08-21-10Alpha22BsLSTM-GooCol-3DRL-150Ep-3DEXPLICIT_EXTENDED495win_495past_249fut\"]\n",
    "\n",
    "future = x_test\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "\n",
    "# GP.generate_prediction_results_avg(880, files[0], p_file_dir, future, new_dir)\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    for AVG in [1, 4, 8, 16, 44, 220]:\n",
    "        print(AVG)\n",
    "        p_file = f\n",
    "\n",
    "        close_gap = False\n",
    "        use_argmin = False\n",
    "        yesterday_pred = False\n",
    "\n",
    "#         GP.generate_prediction_results(p_file, p_file_dir, future, new_dir,close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred, rr_labels=False)\n",
    "        GP.generate_prediction_results_avg(AVG, p_file, p_file_dir, future, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76e857bd5f4ab694912b9b1fb442e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='Search', placeholder='Tag'), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_daily_value_to_datablock(self, datablock, datablock_dir):\n",
    "        obj = json.load(open(f'{datablock_dir}/{datablock}', 'r'))\n",
    "        rr_values = obj['Return_Ratio_List']\n",
    "        investment = 10000\n",
    "        investment_values = []\n",
    "        for r in rr_values:\n",
    "            investment = investment * (1 + r)\n",
    "            investment_values.append(investment)\n",
    "\n",
    "        obj['Investment_Value_List'] = investment_values\n",
    "\n",
    "        json.dump(obj, open(f'{datablock_dir}/{datablock}', 'w'), indent=1)\n",
    "\n",
    "    def add_daily_value_to_datablock_discontinuous(self, datablock, datablock_dir):\n",
    "        obj = json.load(open(f'{datablock_dir}/{datablock}', 'r'))\n",
    "        rr_values = obj['Return_Ratio_List']\n",
    "        total = 10000\n",
    "        portfolio = []\n",
    "        for r in rr_values:\n",
    "            if total < 10000:\n",
    "                invest = total\n",
    "            else:\n",
    "                invest = 10000\n",
    "            # Spend the money\n",
    "            total = total - invest\n",
    "            # Multiply it by our choice\n",
    "            earnings = invest * (1 + r)\n",
    "            # Add it back to our total\n",
    "            total = total + earnings\n",
    "            # Append it to the list and go to the next day\n",
    "            portfolio.append(total)\n",
    "\n",
    "\n",
    "        obj['Discontinuous_Investment_Value_List'] = portfolio\n",
    "\n",
    "        json.dump(obj, open(f'{datablock_dir}/{datablock}', 'w'), indent=1)\n",
    "\n",
    "    def add_cumulative_return_ratio_discontinuous(self, datablock, datablock_dir):\n",
    "        obj = json.load(open(f'{datablock_dir}/{datablock}', 'r'))\n",
    "        final_total = obj['Discontinuous_Investment_Value_List']\n",
    "        # Pull out the last value of the Discontinuous Investment Value List trading strategy.\n",
    "        final_total = final_total[-1]\n",
    "        # Add to the datablocks\n",
    "        obj['Discontinuous_Cumulative_Return_Ratio'] = final_total / 10000\n",
    "\n",
    "        json.dump(obj, open(f'{datablock_dir}/{datablock}', 'w'), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the relations loaded from their dataset\n",
    "rel_encoding = np.load(r'C:\\Users\\Maxwell\\Documents\\Backups\\TAMU-ECEN-403-IFPTSND\\Temporal_Relational_Stock_Ranking-master\\data\\RELATIONS\\sector_industry\\NASDAQ_industry_relation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Shape of relational data:\\t\\t\\t\\t\\t\", rel_encoding.shape)\n",
    "print(\"Shape of relational data 1 Hidden Unit Dense Layer:\\t\\t\", tf.keras.layers.Dense(2)(tf.constant(rel_encoding)).shape)\n",
    "print(\"Shape of relational data 1 Hidden Unit Dense Layer Truncated:\\t\", tf.keras.layers.Dense(2)(rel_encoding)[:, :, -1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do we know for sure that our relationships are still in the right order?\")\n",
    "print(\"After checking the first group, yes, those orders are correct!\")\n",
    "print(Adj.shape)\n",
    "print(Rel.shape)\n",
    "print(np.transpose(Rel).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "sum = 0\n",
    "for i in np.transpose(Rel)[0,:,0]:\n",
    "    sum = sum + i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "test_layer = MyDenseLayer(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_layer(input_seq)\n",
    "x = tf.keras.layers.Dense(12)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class square_weight_layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, square_shape):\n",
    "        super(square_weight_layer, self).__init__()\n",
    "        self.square_shape = square_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.square_kernel = self.add_weight(\"square-kernel\",\n",
    "                                             shape=[int(self.square_shape), int(self.square_shape)], trainable=True,\n",
    "                                            dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "        self.square_kernel = tf.add(tf.constant(-1000, shape=[880,880], dtype=tf.float32), self.square_kernel)\n",
    "        self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[880], dtype=tf.float32))\n",
    "        \n",
    "    def call(self, input):\n",
    "#         self.square_kernel = self.square_kernel + tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "        self.square_kernel = tf.nn.softmax(self.square_kernel, axis=0)\n",
    "        self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[880], dtype=tf.float32))\n",
    "        print(self.square_kernel)\n",
    "        return tf.matmul(input, self.square_kernel)\n",
    "\n",
    "class square_keep_self_loop(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(square_keep_self_loop, self).__init__()\n",
    "        self.data_t = tf.float16\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = input_shape[0]\n",
    "        \n",
    "        \n",
    "        # Create the weights, initialized with zeroes\n",
    "        self.square_kernel = self.add_weight(\"square-kernel\",\n",
    "                                             shape=[input_shape,input_shape], trainable=True,\n",
    "                                            dtype=self.data_t, initializer=tf.zeros_initializer)\n",
    "        # Set all values to -1000\n",
    "        self.square_kernel = tf.add(tf.constant(-1000, shape=[input_shape,input_shape], dtype=self.data_t), self.square_kernel)\n",
    "        # Set the diagonal to 1\n",
    "        self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[input_shape], dtype=self.data_t))\n",
    "        \n",
    "    def call(self, input):\n",
    "#         self.square_kernel = self.square_kernel + tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "\n",
    "        # Normalize the matrix - across the horizontal axis\n",
    "        self.square_kernel = tf.nn.softmax(self.square_kernel, axis=0)\n",
    "        # Ensure that the self loops are maintained\n",
    "        self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[880], dtype=self.data_t))\n",
    "        print(self.square_kernel)\n",
    "        return tf.matmul(input, self.square_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_shape=880\n",
    "eye = tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "\n",
    "SWL = square_weight_layer()(eye)\n",
    "SWL = square_weight_layer()(eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
