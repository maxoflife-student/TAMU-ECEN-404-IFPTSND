{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Change the working directory back to the original to keep paths the same between files\n",
    "os.chdir(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND')\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import join, isfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from explore_entities import Graph_Entities\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, GridBox\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allows for scrolling windows to be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "    IPython.OutputArea.auto_scroll_threshold = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import TF_models and truncate the entities to only contain 880 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "DMJ = TF_Models('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/', './ignorable_data/models/[55, 25, 20]_split/', reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix = DMJ.Normalized_Adjacency_Matrix[0:880, 0:880]\n",
    "DMJ.XX_tf = DMJ.XX_tf[0:-1, :, :]\n",
    "DMJ.YY_tf = DMJ.YY_tf[0:-1, :]\n",
    "DMJ.RR_tf = DMJ.RR_tf[0:-1, :]\n",
    "DMJ.entities = DMJ.entities[0:-1]\n",
    "DMJ.entities_idx.pop('ZUMZ')\n",
    "\n",
    "model = DMJ.generate_model()\n",
    "\n",
    "from graph_predictions import Graph_Predictions\n",
    "GP = Graph_Predictions(\"./ignorable_data/models/[55, 25, 20]_split/\", \"./ignorable_data/strategies/RL_validation_strategies/\", 'x_val', DMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we split the data into time batches ourselves and trained them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:2, :, :]\n",
    "# one_y = DMJ.YY_tf[0:2, :]\n",
    "one_x = DMJ.XX_tf\n",
    "print(DMJ.XX_tf.shape)\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "# one_x = DMJ.XX_tf[0:-1, :, :]\n",
    "# one_y = DMJ.YY_tf[0:-1, :]\n",
    "\n",
    "# # Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "# time_split = [1000, 239,]\n",
    "# x_train, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# y_train, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "#                                   axis=1)\n",
    "# rr_train, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "#                                      axis=1)\n",
    "\n",
    "# Given a dataset, let's section off a fifth of the data to be used for testing purposes\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# y_train = y_train[:, -1]\n",
    "# y_val = y_val[:, -1]\n",
    "\n",
    "# # Once we have the data partitioned, let's split it into 8 sets of 125\n",
    "# batch_splits = [1]*8\n",
    "# # Create a list of 8 different time batches\n",
    "# x_train_batches = []\n",
    "# x_train_batches.append(tf.split(x_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# x_train_batches = [item for sublist in x_train_batches for item in sublist]\n",
    "\n",
    "# y_train_batches = []\n",
    "# y_train_batches.append(tf.split(y_train, split_windows(x_train.shape[1], batch_splits), axis=1))\n",
    "# y_train_batches = [item for sublist in y_train_batches for item in sublist]\n",
    "\n",
    "# # Lets try truncating the values for the labels to only be the final value\n",
    "# for i in range(len(y_train_batches)):\n",
    "#     y_train_batches[i] = y_train_batches[i][:, -1]\n",
    "\n",
    "# print(x_train[0, -1, 0])\n",
    "\n",
    "# print(rr_train[0, -1])\n",
    "\n",
    "# print(x_val[0, 0, 0])\n",
    "# print(y_train[0, -1])\n",
    "# for set in [x_train, x_g, x_val, x_test]:\n",
    "print(x_val.shape)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_train[0, 0, 0])\n",
    "print(x_train[0, 1, 0])\n",
    "print(rr_train[0, 0])\n",
    "\n",
    "print(x_train[0, -1, 0])\n",
    "print(x_train[0, -2, 0])\n",
    "print(x_train[0, -2:-1, 0])\n",
    "\n",
    "print(keras.Input(shape=(x_train.shape[1], x_train.shape[2])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the one-hot encoded adjacency matrix to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explore_entities\n",
    "# Load in the code\n",
    "GE = Graph_Entities('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ - Yuning/')\n",
    "# Generate the baseline components we're wokring with\n",
    "NAM, Adj, Rel = GE.get_matrix_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the dimensionality of normalized adjacency matrix we have so far "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if a matrix is normalized among it's rows and columns to 6 decimal places\n",
    "def is_normalized(m):\n",
    "    m_t = np.transpose(m)\n",
    "    r_trigger = True\n",
    "    c_trigger = True\n",
    "    for i in m:\n",
    "        if (round(np.sum(i), 6)) != 1 and r_trigger:\n",
    "            print('Rows Not Normalized')\n",
    "            r_trigger = not r_trigger\n",
    "    for i in m_t:\n",
    "        if (round(np.sum(i), 6)) != 1 and c_trigger:\n",
    "            print('Columns Not Normalized')\n",
    "            c_trigger = not c_trigger\n",
    "    \n",
    "    if r_trigger and c_trigger:\n",
    "        print('Normalized')\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Created with D^-1/2 * A * D^-1/2\n",
    "print(f\"Symetrically Normalized DMJ: {NAM.shape}\")\n",
    "is_normalized(NAM)\n",
    "print(\"####\")\n",
    "# All relationships, but still seperated by the different groups\n",
    "print(f\"One-Hot Encoded Relationships Split by Group: {Rel.shape}\")\n",
    "print(\"####\")\n",
    "# All relationships squished into one matrix\n",
    "print(f\"One-Hot Encoded All Relationships: {Adj.shape}\")\n",
    "is_normalized(Adj)\n",
    "print(\"####\")\n",
    "\n",
    "None\n",
    "\n",
    "print(np.sum(Rel[9, :, :])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a total and list of splits, evenly distributes the total amount proportional to the given list\n",
    "def split_windows(total, percentages_list):\n",
    "    # Get a sum of the initial total\n",
    "    percentage_sum = sum(percentages_list)\n",
    "\n",
    "    # Create a new list based on a percentage of the total\n",
    "    new_splits = []\n",
    "    for perc in percentages_list:\n",
    "        new_splits.append(int(total * (perc / percentage_sum)))\n",
    "    \n",
    "    # Where to shift the extra days that don't exactly divide between the values\n",
    "    if sum(new_splits) != total:\n",
    "        new_splits[-1] = new_splits[-1] + (total - sum(new_splits))\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "one_x = DMJ.XX_tf\n",
    "one_y = DMJ.YY_tf\n",
    "\n",
    "time_split = [90, 710, 239, 199]\n",
    "time_split = [55, 25, 20]\n",
    "\n",
    "time_split = [30, 25, 25, 20]\n",
    "x_g, x_train, x_val, x_test = tf.split(one_x, split_windows(one_x.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "y_g, y_train, y_val, y_test = tf.split(one_y, split_windows(one_y.shape[1], time_split),\n",
    "                                  axis=1)\n",
    "rr_g, rr_train, rr_val, rr_test = tf.split(DMJ.RR_tf, split_windows(one_y.shape[1], time_split),\n",
    "                                     axis=1)\n",
    "\n",
    "# For the Adjacency Matrix we would like to introduce to the algorithm\n",
    "# N x N x K\n",
    "adj_matrix = np.transpose(Rel)\n",
    "\n",
    "# Since some implementations removed the last company...\n",
    "if adj_matrix.shape[0] == 881:\n",
    "    adj_matrix = adj_matrix[0:-1, 0:-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in an already trained LSTM model & Adding Topographical Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "            name='Adam'\n",
    "        )\n",
    "\n",
    "# Create a function for the Tensorflow implementation of leaky_relu\n",
    "def leaky_relu(x):\n",
    "    return tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# Shouldn't be neccessary if we use softmax\n",
    "# def normalize_adj_matrix(adj):\n",
    "#     degree = tf.reduce_sum(adj, axis=0)\n",
    "#     inv_degree = tf.math.reciprocal(degree)\n",
    "#     diag_inv_degree = tf.linalg.diag(inv_degree)\n",
    "#     norm_adj = diag_inv_degree * adj\n",
    "#     return norm_adj\n",
    "\n",
    "hidden_units = 64\n",
    "activation = leaky_relu\n",
    "do = 0\n",
    "\n",
    "# Assuming all time steps are the same size, we can just use the first item to determine input shape\n",
    "# For the time-series data\n",
    "input_seq = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "    \n",
    "# Create the input layer for the data that comes from outside the graph (needs to be NxN)\n",
    "input_rel = keras.Input(shape=(adj_matrix.shape[1], adj_matrix.shape[2]))\n",
    "\n",
    "\n",
    "# Load in an already trained LSTM Model\n",
    "file_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA'\n",
    "model_path = './ignorable_data/models/[55, 25, 20]_split/'\n",
    "pre_trained_lstm = tf.keras.models.load_model(model_path + f'{file_name}', compile=False,\n",
    "                                              custom_objects={'leaky_relu': leaky_relu})\n",
    "\n",
    "# Change the names to avoid conflicts\n",
    "pre_trained_lstm.layers[0]._name = 'Original-InputLayer'\n",
    "pre_trained_lstm.layers[1]._name = 'Original-LSTM'\n",
    "pre_trained_lstm.layers[2]._name = 'Original-Dense'\n",
    "\n",
    "# # Make sure that the weights for the lstm model cannot be updated\n",
    "pre_trained_lstm.layers[0].trainable = False\n",
    "pre_trained_lstm.layers[1].trainable = False\n",
    "pre_trained_lstm.layers[2].trainable = False\n",
    "\n",
    "# This is the LSTM layer, input_seq is not carried over from the original because we don't care how it was initalized\n",
    "x = pre_trained_lstm.layers[1](input_seq)\n",
    "\n",
    "# Experimental vector multiplication\n",
    "# W = tf.Variable(tf.random.uniform(shape=[adj_matrix.shape[0], 1]))\n",
    "# W = tf.keras.layers.Variable\n",
    "# y = Ein_Multiply()([input_rel, W], \"ij, k->ik\")\n",
    "\n",
    "# This is a seperate input layer that will take in the relational matrix\n",
    "# Dimensionality: N x N x 1\n",
    "# y = tf.keras.layers.Dense(1, activation=activation)(input_rel)\n",
    "\n",
    "# These are the similarity values\n",
    "# Dimensionality: N x T x N\n",
    "\n",
    "# As a memory test, let's only take 1 time embedding and see if our computer can handle it\n",
    "x = x[:, -1, :]\n",
    "\n",
    "# w = Ein_Multiply()([w, x], \"ij, jk->ik\")\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "# w = MaskedDense(880)(x)\n",
    "e_bar = square_keep_self_loop(880)(x)\n",
    "# w = tf.matmul(w, x)\n",
    "\n",
    "# This is where we can decide to add an additional dense layer for learning or not, but we're currently not implementing this\n",
    "\n",
    "# Lastly, we want to recombine the original embeddings and with the new ones, so that both data is present for learning\n",
    "# Dimensionality: NxTxD*2\n",
    "\n",
    "# # This is the original Dense Layer\n",
    "# x = pre_trained_lstm.layers[2](x)\n",
    "\n",
    "# This is a new Dense layer we might want to experiment with\n",
    "o = Dense(1, activation=activation)(e_bar)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_seq], outputs=o)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_models import rank_loss_correct_pred_rr\n",
    "\n",
    "# View what we just created\n",
    "# model.compile(loss=rank_loss_rr, optimizer=optimizer)\n",
    "model.compile(loss=rank_loss_correct_pred_rr, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(model.layers)):\n",
    "#     print(model.layers[i].name)\n",
    "tf.print(model.layers[13].name)\n",
    "tf.print(model.layers[13].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    return 0.00035\n",
    "\n",
    "inputs_train = [x_train]\n",
    "train_labels = y_train\n",
    "\n",
    "inputs_val = [x_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# If we're using the GCN\n",
    "inputs_train.append(adj_matrix)\n",
    "inputs_val.append(adj_matrix)\n",
    "\n",
    "# int(inputs_train.shape[0])\n",
    "\n",
    "history = model.fit(inputs_train, train_labels, batch_size=int(inputs_train[0].shape[0]),\n",
    "                              epochs=0, validation_data=(inputs_val, val_labels),\n",
    "                   callbacks=[model_checkpoint_callback,\n",
    "                             tf.keras.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                                      verbose=0)])\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "for h in history.history['val_loss']:\n",
    "    losses.append(h)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([x_train, DMJ.Normalized_Adjacency_Matrix], batch_size=880)\n",
    "model_save = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in history.history['val_loss']:\n",
    "#     losses.append(h)\n",
    "# plt.plot(losses)\n",
    "\n",
    "DMJ.Normalized_Adjacency_Matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.model = model\n",
    "DMJ.model_name = \"3-29-21-1_[25,25,20]_400Epoch_NaiveWeightAlpha_1000_1.02val_loss\"\n",
    "DMJ.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"2-16-21-Seq1LSTM-F-64HU-[800,239,200]split-full_y\"\n",
    "# new_directory = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "# GP.generate_validation_prediction_json_SplitBatch_nofeat(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "# GP.generate_validation_prediction_json_SplitBatch_close_gap(model_name, new_directory, x_g, x_val, sliding_window=30)\n",
    "model_name = '3-28-21-1_[25,25,20]_100Epoch_BrokenNaiveAlpha_1000'\n",
    "model_dir = './ignorable_data/models/[55, 25, 20]_split'\n",
    "# model_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\models'\n",
    "past = x_train\n",
    "future = x_val\n",
    "# new_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "new_dir = './ignorable_data/prediction_results/[55, 25, 20]_splits/'\n",
    "sliding_window = x_val.shape[1]\n",
    "\n",
    "window = x_train.shape[1]\n",
    "input_Adj_matrix = adj_matrix\n",
    "# GP.generate_predictions(model_name, model_dir, past, future, new_dir, window,\n",
    "#                         model_type='gcn', input_Adj_matrix=adj_matrix, batch_size=880)\n",
    "GP.generate_predictions(model_name, model_dir, past, future, new_dir, window, model_type='lstm', batch_size=880)\n",
    "# GCN_output = GP.return_embeddings(model_name, model_dir, past, future, new_dir, w, model_type='gcn')\n",
    "# embeddings = GP.return_embeddings(model_name, model_dir, past, future, new_dir, window, model_type='gcn')\n",
    "\n",
    "#     model_name = '2-20-21-1_LSTM_[25,25,20]_NoDropout_35Epoch_80BatchSize_IncreasedVariedLR_RRMSE'\n",
    "#     GP.generate_predictions(model_name, model_dir, past, future, new_dir, w)\n",
    "# GP.generate_validation_prediction_json_SplitBatch(model_name, new_dir, x_g, x_val, sliding_window=sliding_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the training set, these are the validation values that are output from the LSTM model\n",
    "print(embeddings.shape)\n",
    "\n",
    "# This is the NAM that we are using to aggregate the embedding results\n",
    "print(DMJ.Normalized_Adjacency_Matrix[6,6])\n",
    "\n",
    "# This should be the aggregated values output from using the NAM (The dimensions look fine)\n",
    "# new_embeddings = tf.einsum('ntd,nm->mtd', embeddings, DMJ.Normalized_Adjacency_Matrix)\n",
    "new_embeddings = tf.einsum('mn,ntd->mtd', DMJ.Normalized_Adjacency_Matrix, embeddings)\n",
    "print(new_embeddings.shape)\n",
    "\n",
    "print(embeddings[0, -7, 0:10])\n",
    "# print(embeddings[0, -1, 0:10])\n",
    "print('  ')\n",
    "print(new_embeddings[0, -7, 0:10])\n",
    "# print(new_embeddings[0, -1, 0:10])\n",
    "\n",
    "# print(tf.subtract(embeddings[0, -2, 0:10], embeddings[0, -1, 0:10]))\n",
    "# print(tf.subtract(new_embeddings[0, -2, 0:10], new_embeddings[0, -1, 0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above are the output embeddings of the LSTM layer\n",
    "# Below is 1 output prediction for a simple NAM Aggregate added to a pre-trained LSTM\n",
    "GCN_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_file_name = '3-28-21-1_[25,25,20]_100Epoch_BrokenNaiveAlpha_1000309win_309past_309fut'\n",
    "p_file_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits'\n",
    "# p_file_dir = 'G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions\\pc_version'\n",
    "future = x_val\n",
    "new_dir = './ignorable_data/datablocks/[55, 25, 20]_splits/'\n",
    "close_gap = False\n",
    "\n",
    "use_argmin = False\n",
    "yesterday_pred = False\n",
    "GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir,\n",
    "                               close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred, rr_labels=False)\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\Maxwell\\PycharmProjects\\TAMU-ECEN-403-IFPTSND\\ECEN_403_IFM\\TAMU-ECEN-403-IFPTSND\\ignorable_data\\datablocks\\[55, 25, 20]_splits'):\n",
    "    for filename in files:\n",
    "#         GP.generate_prediction_results(filename, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "        GP.add_daily_value_to_datablock(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_daily_value_to_datablock_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        GP.add_cumulative_return_ratio_discontinuous(filename, './ignorable_data/datablocks/[55, 25, 20]_splits')\n",
    "        None\n",
    "\n",
    "\n",
    "# use_argmin = True\n",
    "# yesterday_pred = False\n",
    "# GP.generate_prediction_results(p_file_name, p_file_dir, future, new_dir, close_gap=close_gap, use_argmin=use_argmin, yesterday_pred=yesterday_pred)\n",
    "# # GP.generate_model_diagnostics_given_sets(p_file_dir + f'/{p_file_name}', future, datablock_folder=new_dir, try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_company(n):\n",
    "    # n = 597\n",
    "    # n = 488\n",
    "    # n = 161\n",
    "    # n = 561\n",
    "    # n = 440\n",
    "    # n = 200\n",
    "    # # Argmin\n",
    "    # n = 780\n",
    "\n",
    "    # n = 876\n",
    "    # n = 110\n",
    "    r = 1\n",
    "    # n = 100\n",
    "    # n=0\n",
    "\n",
    "    r=1\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "#     ax.set_ylim([0,1])\n",
    "    ax.set_xlim([20,80])\n",
    "\n",
    "    time = tf.concat([x_train, x_val, x_test], axis=1)\n",
    "\n",
    "    # ax = fig.add_subplot()\n",
    "    for i in range(r):\n",
    "    #     print(f'{i+n*r} ', end='')\n",
    "    #     ax.plot(time[i+n*r, :, 0])\n",
    "        ax.plot(x_val[i+n*r, :, 0], label='Closing Price', linewidth=2, color='black')\n",
    "    #     ax.plot(rr_val[i+n*r, :], label='Return Ratio')\n",
    "\n",
    "        ax.plot(A[:, i+n*r], label='LSTM-1000 ALPHA')\n",
    "    #     ax.plot(B[:, i+n*r], label='LSTM-1000 ALPHA + 1 Dense')\n",
    "    #     ax.plot(C[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM')\n",
    "    #     ax.plot(D[:, i+n*r], label='LSTM-1000 ALPHA + 1NAM + 1 Dense')\n",
    "    #     ax.plot(E[:, i+n*r], label='LSTM-1000 ALPHA + 881Dense-X-Adj')\n",
    "    #     ax.plot(F[:, i+n*r], label='LSTM-1000 ALPHA + 881Dense-X-Adj + Trainable Final Layer')\n",
    "        ax.plot(G[:, i+n*r], label='LSTM-1000 ALPHA + Explict')\n",
    "        ax.plot(H[:, i+n*r], label='LSTM-1000 ALPHA + Implicit')\n",
    "        ax.plot(I[:, i+n*r], label='Naive')\n",
    "\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Validation Days (Time)')\n",
    "        ax.set_ylabel('Closing Price')\n",
    "        ax.set_title(f'Company {i+n*r}')\n",
    "\n",
    "    print(DMJ.entities[n])\n",
    "\n",
    "\n",
    "\n",
    "plot_company(488)\n",
    "plot_company(221)\n",
    "plot_company(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]Reloaded_1000ALPHA309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "A = GP.test_obj\n",
    "\n",
    "pred_dir = '.\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/'\n",
    "pred_file = '3-12-21-1_LSTM_[25,25,20]_10Epoch_880BatchSize_TFMSE_Additional_64_Dense_Layer309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "B = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1NAMAGGREGATE309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "C = GP.test_obj\n",
    "\n",
    "pred_file = '2-20-21-1_LSTM_[25,25,20]_NoDropout_100Epoch_80BatchSize_1000ALPHA_1DenseGCN_TFMSE_FullTrain309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "D = GP.test_obj\n",
    "\n",
    "pred_file = '3-15-21-1_LSTM_[25,25,20]_650Epoch_881BatchSize_TFMSE_881Dense-X-SquishAdj309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "E = GP.test_obj\n",
    "\n",
    "pred_file = '3-15-21-1_LSTM_[25,25,20]_100Epoch_881BatchSize_TFMSE_881Dense-X-SquishAdj_TrainableFinalDense309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "F = GP.test_obj\n",
    "\n",
    "pred_file = '3-25-21-1_[25,25,20]_300Epoch_Explicit_-1_Alpha_1000309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "G = GP.test_obj\n",
    "\n",
    "pred_file = '3-25-21-1_[25,25,20]_325Epoch_Implicit_-1_Alpha_1000309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "H = GP.test_obj\n",
    "\n",
    "pred_file = '3-28-21-1_[25,25,20]_100Epoch_BrokenNaiveAlpha_1000309win_309past_309fut'\n",
    "GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "I = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-250win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# J = GP.test_obj\n",
    "\n",
    "# pred_file = '02-17-2021--07--20-1LSTM-F-0ALPHA-[55,25,20]split-[None, 1]-40Epochs-rlf-Loss-64-HU-500win_681past_309fut'\n",
    "# GP.graph_model_prediction_given_sets(pred_dir + pred_file, x_val)\n",
    "# K = GP.test_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.compare_data_blocks('.\\ignorable_data\\datablocks\\[55, 25, 20]_splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # pred_dir = r\"G:\\Shared drives\\Max Huffman - ECEN 403 404 URS Research 2020 2021\\Datasets\\predictions/\"\n",
    "# pred_dir = r\".\\ignorable_data\\prediction_results\\[55, 25, 20]_splits/\"\n",
    "# pred_file = '3-11-21-1_LSTM_[25,25,20]_60Epoch_40BatchSize_100ALPHA_RR_Labels309win_309past_309fut_220predBatch'\n",
    "# GP.generate_model_diagnostics(pred_dir + pred_file, datablock_folder='./ignorable_data/datablocks/', rr_labels=True)\n",
    "# # GP.generate_model_diagnostics_given_sets_close_gap(pred_dir + pred_file, x_val, datablock_folder='./ignorable_data/datablocks/', try_all_pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Let's look at our relations_file real quick\n",
    "with open('./ignorable_data/data_sets/NASDAQ_Cleaned - Contains ZUMZ/updated_relations.json') as read_file:\n",
    "    relations_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The degree vector of Adj\n",
    "degree = np.sum(Adj, axis=0)\n",
    "\n",
    "# Inverse degree vector\n",
    "inv = lambda x: x**-1\n",
    "inv_degree = inv(degree)\n",
    "\n",
    "# Convert into an inverse diagonal matrix\n",
    "diag_inv_degree = np.diag(inv_degree)\n",
    "\n",
    "# Create the negative square root version\n",
    "inv_sqrt_degree = np.sqrt(inv_degree)\n",
    "\n",
    "diag_inv_sqrt_degree = np.diag(inv_sqrt_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symetric normalization yields the correct output\n",
    "print(np.sum(np.dot(np.dot(inv_sqrt_degree, Adj), inv_sqrt_degree)))\n",
    "print(np.sum(np.matmul(np.matmul(inv_sqrt_degree, Adj), inv_sqrt_degree)))\n",
    "\n",
    "# Symetric normalization yields the correct output\n",
    "print(np.dot(np.dot(inv_sqrt_degree, Adj), inv_sqrt_degree))\n",
    "print(np.matmul(np.matmul(inv_sqrt_degree, Adj), inv_sqrt_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last company, ZUMF was truncated in this example, so this value makes sense.\n",
    "print(np.sum(DMJ.Normalized_Adjacency_Matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard normalization yields the correct output\n",
    "print(np.sum(np.matmul(diag_inv_degree, Adj)))\n",
    "\n",
    "# Standard normalization yields the correct output\n",
    "print(np.matmul(diag_inv_degree, Adj))\n",
    "print(np.dot(diag_inv_degree, Adj))\n",
    "print(np.dot(Adj, diag_inv_degree))\n",
    "\n",
    "print(np.dot(Adj, diag_inv_degree)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj_matrix(adj):\n",
    "    degree = tf.reduce_sum(adj, axis=0)\n",
    "    inv_degree = tf.math.reciprocal(degree)\n",
    "    diag_inv_degree = tf.linalg.diag(inv_degree)\n",
    "    r = diag_inv_degree * adj\n",
    "    return r\n",
    "\n",
    "adf_tf = tf.constant(Adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf_tf)\n",
    "print(tf.reduce_sum(normalize_adj_matrix(adf_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.einsum(\"ij, k->ik\", Adj, Adj[:,3])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.einsum(\"ij, k->ij\", Adj, Adj[:,3])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Adj[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Temporal Relational Ranking Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the relations loaded from their dataset\n",
    "rel_encoding = np.load(r'C:\\Users\\Maxwell\\Documents\\Backups\\TAMU-ECEN-403-IFPTSND\\Temporal_Relational_Stock_Ranking-master\\data\\RELATIONS\\sector_industry\\NASDAQ_industry_relation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Shape of relational data:\\t\\t\\t\\t\\t\", rel_encoding.shape)\n",
    "print(\"Shape of relational data 1 Hidden Unit Dense Layer:\\t\\t\", tf.keras.layers.Dense(2)(tf.constant(rel_encoding)).shape)\n",
    "print(\"Shape of relational data 1 Hidden Unit Dense Layer Truncated:\\t\", tf.keras.layers.Dense(2)(rel_encoding)[:, :, -1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do we know for sure that our relationships are still in the right order?\")\n",
    "print(\"After checking the first group, yes, those orders are correct!\")\n",
    "print(Adj.shape)\n",
    "print(Rel.shape)\n",
    "print(np.transpose(Rel).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "sum = 0\n",
    "for i in np.transpose(Rel)[0,:,0]:\n",
    "    sum = sum + i\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "test_layer = MyDenseLayer(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_layer(input_seq)\n",
    "x = tf.keras.layers.Dense(12)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_seq], outputs=x)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class square_weight_layer(tf.keras.layers.Layer):\n",
    "    \n",
    "#     def __init__(self, square_shape):\n",
    "#         super(square_weight_layer, self).__init__()\n",
    "#         self.square_shape = square_shape\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.square_kernel = self.add_weight(\"square-kernel\",\n",
    "#                                              shape=[int(self.square_shape), int(self.square_shape)], trainable=True,\n",
    "#                                             dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "#         self.square_kernel = tf.add(tf.constant(-1000, shape=[880,880], dtype=tf.float32), self.square_kernel)\n",
    "#         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[880], dtype=tf.float32))\n",
    "        \n",
    "#     def call(self, input):\n",
    "# #         self.square_kernel = self.square_kernel + tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "#         self.square_kernel = tf.nn.softmax(self.square_kernel, axis=0)\n",
    "#         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=[880], dtype=tf.float32))\n",
    "#         print(self.square_kernel)\n",
    "#         return tf.matmul(self.square_kernel, input)\n",
    "\n",
    "class square_keep_self_loop(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super(square_keep_self_loop, self).__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        self.data_t = tf.float32\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Create the weights, initialized with zeroes\n",
    "        self.kernel = self.add_weight(\"square-kernel\",\n",
    "                                             shape=[self.N,self.N],\n",
    "                                            dtype=self.data_t, initializer=tf.keras.initializers.identity)\n",
    "        \n",
    "#         self.square_kernel = tf.add(tf.constant(-1000, shape=[self.N, self.N], dtype=self.data_t), self.square_kernel)\n",
    "        \n",
    "#         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=self.N, dtype=self.data_t))\n",
    "        \n",
    "    def call(self, input):\n",
    "#         self.square_kernel = self.square_kernel + tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "\n",
    "        # Normalize the matrix - across the horizontal axis\n",
    "#         self.kernel = tf.nn.softmax(self.kernel, axis=0)\n",
    "        # Ensure that the self loops are maintained\n",
    "#         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=self.N, dtype=self.data_t))\n",
    "        \n",
    "        return self.kernel\n",
    "\n",
    "# class square_keep_self_loop(tf.keras.layers.Dense):\n",
    "    \n",
    "#     def __init__(self, N):\n",
    "#         super(square_keep_self_loop, self).__init__(N)\n",
    "        \n",
    "#         self.N = N\n",
    "#         self.data_t = tf.float32\n",
    "        \n",
    "#         print(self.kernel)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "        \n",
    "#         # Create the weights, initialized with zeroes\n",
    "#         self.kernel = self.add_weight(\"square-kernel\",\n",
    "#                                              shape=[self.N,self.N],\n",
    "#                                             dtype=self.data_t, initializer=tf.zeros_initializer)\n",
    "        \n",
    "# #         self.square_kernel = tf.add(tf.constant(-1000, shape=[self.N, self.N], dtype=self.data_t), self.square_kernel)\n",
    "        \n",
    "# #         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=self.N, dtype=self.data_t))\n",
    "        \n",
    "#     def call(self, input):\n",
    "# #         self.square_kernel = self.square_kernel + tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "\n",
    "#         # Normalize the matrix - across the horizontal axis\n",
    "#         self.kernel = tf.nn.softmax(self.kernel, axis=0)\n",
    "#         # Ensure that the self loops are maintained\n",
    "# #         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=self.N, dtype=self.data_t))\n",
    "#         self.kernel = tf.matmul(self.kernel, input)\n",
    "        \n",
    "#         return super().call(input)\n",
    "\n",
    "\n",
    "# class square_keep_self_loop(tf.keras.layers.Layer):\n",
    "\n",
    "#   def __init__(self, units):\n",
    "#       super(square_keep_self_loop, self).__init__()\n",
    "#       self.units = units\n",
    "\n",
    "#   def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "#     w_init = tf.random_normal_initializer()\n",
    "#     self.w = tf.Variable(\n",
    "#         initial_value=w_init(shape=(self.units, self.units),\n",
    "#                              dtype='float32'),\n",
    "#         trainable=True)\n",
    "#     self.w = tf.nn.softmax(self.w)\n",
    "\n",
    "#   def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "        \n",
    "#         return tf.matmul(self.w, inputs)\n",
    "\n",
    "class MaskedDense(Dense):\n",
    "    #Dense layer defined here:\n",
    "    #https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/layers/core.py#L1081-L1247\n",
    "    def __init__(self, units, *args, **kwargs):\n",
    "        super(MaskedDense, self).__init__(units, *args, **kwargs)        \n",
    "        self.units = units\n",
    "    \n",
    "    def call(self, x):\n",
    "#         self.kernel = tf.nn.softmax(self.kernel, axis=0)\n",
    "#         self.kernel = tf.linalg.set_diag(self.kernel, tf.constant(1, shape=[self.units], dtype=tf.float32))\n",
    "#         self.kernel = tf.nn.softmax(self.kernel, axis=0)\n",
    "\n",
    "        return super().call(x)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(MaskedDense, self).build(input_shape)\n",
    "        \n",
    "\n",
    "class square_keep_self_loop(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super(square_keep_self_loop, self).__init__(N)\n",
    "        \n",
    "        self.N = N\n",
    "        self.data_t = tf.float32\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Create the weights, initialized with zeroes\n",
    "        self.kernel = self.add_weight(\"square-kernel\", shape=[self.N,self.N], dtype=self.data_t,\n",
    "                                      initializer=tf.initializers.identity, trainable=True)\n",
    "        \n",
    "#         self.square_kernel = tf.add(tf.constant(-1000, shape=[self.N, self.N], dtype=self.data_t), self.square_kernel)\n",
    "        \n",
    "#         self.square_kernel = tf.linalg.set_diag(self.square_kernel, tf.constant(1, shape=self.N, dtype=self.data_t))\n",
    "        \n",
    "    def call(self, input):\n",
    "        \n",
    "        # Create a second tensor that leaves the internal kernel to be trainable\n",
    "        a_mat = tf.matmul(tf.eye(self.N, self.N), self.kernel)\n",
    "        \n",
    "        # Normalize the values\n",
    "        a_mat = tf.nn.softmax(a_mat, axis=0)\n",
    "#         a_mat = a_mat + tf.eye(self.N, self.N)\n",
    "#         a_mat = tf.nn.softmax(a_mat, axis=0)\n",
    "\n",
    "        # Ensure that the self loops are maintained\n",
    "        a_mat = tf.linalg.set_diag(a_mat, tf.constant(1, shape=[self.N], dtype=self.data_t))\n",
    "        \n",
    "        output = tf.matmul(a_mat, input)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_shape=880\n",
    "eye = tf.eye(num_rows=int(square_shape), num_columns=int(square_shape))\n",
    "\n",
    "# SWL = square_weight_layer()(eye)\n",
    "SWL = square_keep_self_loop(880)(eye)\n",
    "SWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(-1000, shape=[None,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
