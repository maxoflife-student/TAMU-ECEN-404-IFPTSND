{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ignore cuDDa warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable GPU\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# # Expands the Jupyter Notebook Output Size to fit your window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Load in tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow_models import TF_Models, Ein_Multiply, leaky_relu, rank_loss_func\n",
    "from graph_predictions import Graph_Predictions\n",
    "\n",
    "# tf.config.list_physical_devices()\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the directory since all files were moved to \"ignorable_data\"\n",
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir(path + '\\ignorable_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 0.1369 - val_loss: 0.0341\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1542 - val_loss: 0.0341\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1458 - val_loss: 0.0342\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1222 - val_loss: 0.0336\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1206 - val_loss: 0.0322\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1195 - val_loss: 0.0302\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1038 - val_loss: 0.0230\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0887 - val_loss: 0.0092\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0804 - val_loss: 0.0076\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0662 - val_loss: 0.0078\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0505 - val_loss: 0.0081\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0410 - val_loss: 0.0085\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0605 - val_loss: 0.0076\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0551 - val_loss: 0.0076\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0546 - val_loss: 0.0077\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0619 - val_loss: 0.0077\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0637 - val_loss: 0.0077\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0630 - val_loss: 0.0077\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--22--15-E2E_LSTM_ValSet_1000.0-ALPHA0.0001-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0007866622300584215\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 0.1377 - val_loss: 0.0350\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1551 - val_loss: 0.0350\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1466 - val_loss: 0.0350\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1231 - val_loss: 0.0344\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1214 - val_loss: 0.0331\n",
      "Epoch 6/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1204 - val_loss: 0.0311\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1047 - val_loss: 0.0239\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0896 - val_loss: 0.0101\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0813 - val_loss: 0.0085\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0671 - val_loss: 0.0087\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0514 - val_loss: 0.0090\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0419 - val_loss: 0.0094\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0614 - val_loss: 0.0085\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0560 - val_loss: 0.0085\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0555 - val_loss: 0.0086\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0628 - val_loss: 0.0086\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0646 - val_loss: 0.0086\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0640 - val_loss: 0.0086\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--22--30-E2E_LSTM_ValSet_1000.0-ALPHA0.001-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0007942381835105019\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.1458 - val_loss: 0.0432\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1634 - val_loss: 0.0432\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1550 - val_loss: 0.0434\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1316 - val_loss: 0.0429\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1300 - val_loss: 0.0417\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1290 - val_loss: 0.0398\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1135 - val_loss: 0.0329\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0985 - val_loss: 0.0192\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0902 - val_loss: 0.0174\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0762 - val_loss: 0.0177\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0604 - val_loss: 0.0180\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0510 - val_loss: 0.0185\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0704 - val_loss: 0.0175\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0650 - val_loss: 0.0175\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0646 - val_loss: 0.0175\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0718 - val_loss: 0.0175\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0737 - val_loss: 0.0176\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0730 - val_loss: 0.0176\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Model: '02-07-2021--22--45-E2E_LSTM_ValSet_1000.0-ALPHA0.01-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0007973306795092245\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.2274 - val_loss: 0.1253\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2463 - val_loss: 0.1261\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2389 - val_loss: 0.1270\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2164 - val_loss: 0.1277\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2155 - val_loss: 0.1273\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2154 - val_loss: 0.1267\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--22--59-E2E_LSTM_ValSet_1000.0-ALPHA0.1-BETA_SD100998-6Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0011752205952040317\n",
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.1368 - val_loss: 0.0341\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.1541 - val_loss: 0.0340\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1457 - val_loss: 0.0341\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1221 - val_loss: 0.0335\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1205 - val_loss: 0.0321\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1194 - val_loss: 0.0301\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1037 - val_loss: 0.0229\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0886 - val_loss: 0.0091\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0803 - val_loss: 0.0075\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0661 - val_loss: 0.0077\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0504 - val_loss: 0.0080\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0409 - val_loss: 0.0084\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0604 - val_loss: 0.0075\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0550 - val_loss: 0.0075\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0545 - val_loss: 0.0076\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0618 - val_loss: 0.0076\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0636 - val_loss: 0.0076\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0630 - val_loss: 0.0076\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--23--13-E2E_LSTM_ValSet_1000.0-ALPHA0-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0007866622300584215\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0429 - val_loss: 0.9316\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 1.0691 - val_loss: 0.9245\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.0642 - val_loss: 0.9174\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.0375 - val_loss: 0.9101\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.0351 - val_loss: 0.9029\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.0434 - val_loss: 0.8962\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1.0335 - val_loss: 0.8861\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.0099 - val_loss: 0.8761\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0037 - val_loss: 0.8657\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0043 - val_loss: 0.8549\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.9764 - val_loss: 0.8438\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.9819 - val_loss: 0.8325\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.9471 - val_loss: 0.8157\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.9317 - val_loss: 0.8015\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step - loss: 0.9238 - val_loss: 0.7880\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.9129 - val_loss: 0.7744\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.9394 - val_loss: 0.7607\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.9171 - val_loss: 0.7470\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.8675 - val_loss: 0.7262\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.8460 - val_loss: 0.7064\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.8319 - val_loss: 0.6859\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8111 - val_loss: 0.6652\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8243 - val_loss: 0.6452\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7759 - val_loss: 0.6269\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.7871 - val_loss: 0.5984\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.7415 - val_loss: 0.5688\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.7293 - val_loss: 0.5403\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7066 - val_loss: 0.5143\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7062 - val_loss: 0.4927\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7018 - val_loss: 0.4774\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6748 - val_loss: 0.4665\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6621 - val_loss: 0.4679\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.7054 - val_loss: 0.4580\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6158 - val_loss: 0.4419\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5941 - val_loss: 0.4259\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.6665 - val_loss: 0.4144\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.5473 - val_loss: 0.4054\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6137 - val_loss: 0.4060\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5508 - val_loss: 0.4054\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5375 - val_loss: 0.3936\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5179 - val_loss: 0.3791\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5467 - val_loss: 0.3793\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--23--27-E2E_LSTM_ValSet_1000.0-ALPHA1.0-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0008737291756400031\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 9.1983 - val_loss: 9.0016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 9.1790 - val_loss: 8.9013\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 9.1085 - val_loss: 8.8028\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.9802 - val_loss: 8.7062\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step - loss: 8.8864 - val_loss: 8.6117\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.7957 - val_loss: 8.5194\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 8.7173 - val_loss: 8.3742\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 8.5319 - val_loss: 8.2278\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.3574 - val_loss: 8.0785\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.2574 - val_loss: 7.9289\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.0694 - val_loss: 7.7803\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.9421 - val_loss: 7.6331\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 7.7940 - val_loss: 7.4053\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 7.5740 - val_loss: 7.1944\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.3434 - val_loss: 6.9862\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.1004 - val_loss: 6.7775\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.9486 - val_loss: 6.5678\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.7405 - val_loss: 6.3563\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 6.4368 - val_loss: 6.0120\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 6.1023 - val_loss: 5.6492\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 5.7229 - val_loss: 5.2808\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 5.3295 - val_loss: 4.9088\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 5.0495 - val_loss: 4.5286\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.6360 - val_loss: 4.1382\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 4.2626 - val_loss: 3.5023\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.6566 - val_loss: 2.8591\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.0200 - val_loss: 2.2172\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.3924 - val_loss: 1.5946\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.8470 - val_loss: 1.0684\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.4734 - val_loss: 0.8079\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3040 - val_loss: 1.6606\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.4631 - val_loss: 1.7798\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7872 - val_loss: 1.1865\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.0415 - val_loss: 0.7879\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.4992 - val_loss: 0.6730\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3247 - val_loss: 0.7182\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.2432 - val_loss: 0.6724\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.3639 - val_loss: 0.6732\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3698 - val_loss: 0.6752\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2552 - val_loss: 0.6782\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2103 - val_loss: 0.6820\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.2726 - val_loss: 0.6863\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--23--42-E2E_LSTM_ValSet_1000.0-ALPHA10.0-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 90.7521 - val_loss: 89.7039\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 90.2774 - val_loss: 88.6917\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 89.5500 - val_loss: 87.6961\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 88.4169 - val_loss: 86.7183\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 87.4125 - val_loss: 85.7583\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 86.3184 - val_loss: 84.8164\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 85.5695 - val_loss: 83.3303\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 83.7789 - val_loss: 81.8322\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 81.9203 - val_loss: 80.3071\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 80.8135 - val_loss: 78.7763\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 79.0318 - val_loss: 77.2554\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 77.5449 - val_loss: 75.7511\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 76.3132 - val_loss: 73.4319\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 74.0364 - val_loss: 71.2762\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 71.5484 - val_loss: 69.1401\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 68.9992 - val_loss: 66.9942\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 67.0259 - val_loss: 64.8336\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 64.8456 - val_loss: 62.6497\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 62.1522 - val_loss: 59.0884\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 58.6423 - val_loss: 55.3279\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 54.5949 - val_loss: 51.4879\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 50.4211 - val_loss: 47.6156\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 47.0738 - val_loss: 43.6541\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 42.9461 - val_loss: 39.5789\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 38.8114 - val_loss: 32.9055\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 32.4541 - val_loss: 26.0921\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 25.4546 - val_loss: 19.2007\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 18.5481 - val_loss: 12.3996\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 12.2554 - val_loss: 6.3627\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.1505 - val_loss: 2.6959\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.9904 - val_loss: 11.4909\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 18.0811 - val_loss: 12.6425\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 20.1405 - val_loss: 6.7039\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 14.1631 - val_loss: 2.9724\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.1712 - val_loss: 2.1418\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.2646 - val_loss: 2.8353\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.6435 - val_loss: 2.1626\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.9020 - val_loss: 2.1946\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.0689 - val_loss: 2.2365\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.5978 - val_loss: 2.2859\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.2875 - val_loss: 2.3414\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.1374 - val_loss: 2.4004\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.7063 - val_loss: 2.1433\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 5.5603 - val_loss: 2.1449\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.9047 - val_loss: 2.1465\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.0057 - val_loss: 2.1482\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.8384 - val_loss: 2.1500\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.7457 - val_loss: 2.1519\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-07-2021--23--57-E2E_LSTM_ValSet_1000.0-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 0.0138 - val_loss: 0.0035\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0155 - val_loss: 0.0035\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0147 - val_loss: 0.0035\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0123 - val_loss: 0.0035\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0122 - val_loss: 0.0033\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0121 - val_loss: 0.0031\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0105 - val_loss: 0.0024\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0090 - val_loss: 0.0010\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0082 - val_loss: 8.5432e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0067 - val_loss: 8.7947e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0052 - val_loss: 9.0912e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0042 - val_loss: 9.5006e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0062 - val_loss: 8.5668e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0056 - val_loss: 8.5901e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0056 - val_loss: 8.6126e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0063 - val_loss: 8.6341e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0065 - val_loss: 8.6546e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0064 - val_loss: 8.6750e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--00--11-E2E_LSTM_ValSet_100.0-ALPHA0.0001-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0006663010742724646\n",
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.0146 - val_loss: 0.0043\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0163 - val_loss: 0.0043\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0155 - val_loss: 0.0043\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0132 - val_loss: 0.0043\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0130 - val_loss: 0.0042\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0129 - val_loss: 0.0040\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0114 - val_loss: 0.0033\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0099 - val_loss: 0.0020\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0091 - val_loss: 0.0017\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0077 - val_loss: 0.0018\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0061 - val_loss: 0.0018\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0051 - val_loss: 0.0019\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0071 - val_loss: 0.0018\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0065 - val_loss: 0.0018\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0065 - val_loss: 0.0018\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0072 - val_loss: 0.0018\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0074 - val_loss: 0.0018\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0073 - val_loss: 0.0018\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--00--25-E2E_LSTM_ValSet_100.0-ALPHA0.001-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.000850341941270953\n",
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.0227 - val_loss: 0.0125\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0246 - val_loss: 0.0126\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0239 - val_loss: 0.0127\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0216 - val_loss: 0.0128\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0216 - val_loss: 0.0127\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0216 - val_loss: 0.0127\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--00--38-E2E_LSTM_ValSet_100.0-ALPHA0.01-BETA_SD100998-6Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0011752205952040317\n",
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 0.1043 - val_loss: 0.0932\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1069 - val_loss: 0.0925\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1064 - val_loss: 0.0918\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.1038 - val_loss: 0.0910\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.1035 - val_loss: 0.0903\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1044 - val_loss: 0.0896\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1034 - val_loss: 0.0886\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1010 - val_loss: 0.0876\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1004 - val_loss: 0.0866\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1005 - val_loss: 0.0855\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0977 - val_loss: 0.0844\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0982 - val_loss: 0.0833\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0948 - val_loss: 0.0816\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0932 - val_loss: 0.0802\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0924 - val_loss: 0.0789\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0914 - val_loss: 0.0775\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0940 - val_loss: 0.0761\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0918 - val_loss: 0.0748\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0868 - val_loss: 0.0727\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0847 - val_loss: 0.0707\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0833 - val_loss: 0.0687\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0812 - val_loss: 0.0666\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0825 - val_loss: 0.0646\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0777 - val_loss: 0.0628\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0788 - val_loss: 0.0600\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0743 - val_loss: 0.0570\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0730 - val_loss: 0.0542\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0708 - val_loss: 0.0516\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0707 - val_loss: 0.0494\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0703 - val_loss: 0.0478\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0676 - val_loss: 0.0466\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0662 - val_loss: 0.0466\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0706 - val_loss: 0.0457\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0617 - val_loss: 0.0443\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0596 - val_loss: 0.0427\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0670 - val_loss: 0.0416\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0549 - val_loss: 0.0407\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0618 - val_loss: 0.0406\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0553 - val_loss: 0.0406\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0540 - val_loss: 0.0395\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0521 - val_loss: 0.0382\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0551 - val_loss: 0.0383\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--00--53-E2E_LSTM_ValSet_100.0-ALPHA0.1-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0008737291756400031\n",
      "Model: \"functional_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_51 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.0137 - val_loss: 0.0034\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0154 - val_loss: 0.0034\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0146 - val_loss: 0.0034\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0122 - val_loss: 0.0034\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0121 - val_loss: 0.0032\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0120 - val_loss: 0.0030\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0104 - val_loss: 0.0023\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0089 - val_loss: 9.4416e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0081 - val_loss: 7.5483e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0066 - val_loss: 7.7964e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0051 - val_loss: 8.0899e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0041 - val_loss: 8.4969e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0061 - val_loss: 7.5716e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0055 - val_loss: 7.5945e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0055 - val_loss: 7.6167e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0062 - val_loss: 7.6379e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0064 - val_loss: 7.6580e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0063 - val_loss: 7.6781e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--01--07-E2E_LSTM_ValSet_100.0-ALPHA0-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0006663010742724646\n",
      "Model: \"functional_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.9198 - val_loss: 0.9002\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.9179 - val_loss: 0.8901\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.9109 - val_loss: 0.8803\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.8980 - val_loss: 0.8706\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8887 - val_loss: 0.8612\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.8796 - val_loss: 0.8520\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.8718 - val_loss: 0.8375\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.8532 - val_loss: 0.8228\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.8358 - val_loss: 0.8079\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8258 - val_loss: 0.7929\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.8070 - val_loss: 0.7781\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.7943 - val_loss: 0.7634\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.7795 - val_loss: 0.7406\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.7575 - val_loss: 0.7195\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.7344 - val_loss: 0.6987\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7102 - val_loss: 0.6779\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.6950 - val_loss: 0.6569\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.6742 - val_loss: 0.6358\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6438 - val_loss: 0.6014\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.6104 - val_loss: 0.5651\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.5725 - val_loss: 0.5283\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.5332 - val_loss: 0.4911\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5052 - val_loss: 0.4531\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.4639 - val_loss: 0.4141\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4266 - val_loss: 0.3506\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3660 - val_loss: 0.2863\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3024 - val_loss: 0.2222\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2397 - val_loss: 0.1600\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1852 - val_loss: 0.1072\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1476 - val_loss: 0.0807\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1302 - val_loss: 0.1644\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2448 - val_loss: 0.1783\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2793 - val_loss: 0.1193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2052 - val_loss: 0.0791\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1506 - val_loss: 0.0673\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1327 - val_loss: 0.0716\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1246 - val_loss: 0.0672\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1366 - val_loss: 0.0673\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1372 - val_loss: 0.0674\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1257 - val_loss: 0.0677\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1212 - val_loss: 0.0681\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1274 - val_loss: 0.0685\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--01--21-E2E_LSTM_ValSet_100.0-ALPHA1.0-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 9.0752 - val_loss: 8.9704\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 9.0277 - val_loss: 8.8692\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.9550 - val_loss: 8.7696\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.8417 - val_loss: 8.6719\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.7413 - val_loss: 8.5759\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.6319 - val_loss: 8.4817\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 8.5570 - val_loss: 8.3331\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 8.3780 - val_loss: 8.1833\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1921 - val_loss: 8.0308\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.0814 - val_loss: 7.8777\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 7.9033 - val_loss: 7.7256\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.7546 - val_loss: 7.5752\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 7.6314 - val_loss: 7.3433\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 7.4037 - val_loss: 7.1277\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.1549 - val_loss: 6.9141\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.9000 - val_loss: 6.6995\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.7027 - val_loss: 6.4835\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.4847 - val_loss: 6.2651\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 6.2154 - val_loss: 5.9090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 5.8644 - val_loss: 5.5330\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 5.4597 - val_loss: 5.1490\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 5.0424 - val_loss: 4.7618\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.7077 - val_loss: 4.3657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.2949 - val_loss: 3.9582\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 3.8815 - val_loss: 3.2910\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.2458 - val_loss: 2.6097\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5459 - val_loss: 1.9206\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.8553 - val_loss: 1.2405\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.2260 - val_loss: 0.6367\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.7154 - val_loss: 0.2697\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4990 - val_loss: 1.1477\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.8067 - val_loss: 1.2646\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.0146 - val_loss: 0.6710\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4171 - val_loss: 0.2976\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.8178 - val_loss: 0.2142\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5267 - val_loss: 0.2832\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5646 - val_loss: 0.2162\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5904 - val_loss: 0.2194\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6071 - val_loss: 0.2236\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5600 - val_loss: 0.2285\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5289 - val_loss: 0.2340\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5138 - val_loss: 0.2399\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.5709 - val_loss: 0.2143\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5563 - val_loss: 0.2145\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5907 - val_loss: 0.2147\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6009 - val_loss: 0.2148\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5841 - val_loss: 0.2150\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5749 - val_loss: 0.2152\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--01--36-E2E_LSTM_ValSet_100.0-ALPHA10.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 906.1667 - val_loss: 896.6934\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 901.1086 - val_loss: 886.5618\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 893.8113 - val_loss: 876.5948\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 882.6451 - val_loss: 866.8049\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 872.5280 - val_loss: 857.1897\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 861.3815 - val_loss: 847.7510\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 853.9321 - val_loss: 832.8537\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 836.0962 - val_loss: 817.8354\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 817.3849 - val_loss: 802.5482\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 806.2007 - val_loss: 787.2026\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 788.4917 - val_loss: 771.9570\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 773.3867 - val_loss: 756.8798\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 761.3461 - val_loss: 733.6434\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 738.4927 - val_loss: 712.0362\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 713.4126 - val_loss: 690.6168\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 687.7896 - val_loss: 669.0957\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 667.5571 - val_loss: 647.4224\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 645.6423 - val_loss: 625.5103\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 619.0930 - val_loss: 589.7755\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 583.8179 - val_loss: 552.0364\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 543.0642 - val_loss: 513.4689\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 501.0653 - val_loss: 474.5830\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 466.9950 - val_loss: 434.7977\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 425.7224 - val_loss: 393.8628\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 383.9442 - val_loss: 326.7896\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 320.0439 - val_loss: 258.2365\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 249.3452 - val_loss: 188.7891\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 179.5800 - val_loss: 120.1253\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 115.7121 - val_loss: 58.8375\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 63.0692 - val_loss: 20.8444\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 40.8221 - val_loss: 108.2808\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 172.9218 - val_loss: 119.8435\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 192.0580 - val_loss: 60.7681\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 134.2785 - val_loss: 23.9859\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 73.8195 - val_loss: 16.1632\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 43.5522 - val_loss: 23.3414\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 48.6398 - val_loss: 16.4016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 50.2048 - val_loss: 16.7493\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.8757 - val_loss: 17.1920\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 48.0359 - val_loss: 17.7052\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 45.1222 - val_loss: 18.2762\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 42.7660 - val_loss: 18.8787\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 49.1440 - val_loss: 16.1814\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 46.8780 - val_loss: 16.2001\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 50.5832 - val_loss: 16.2191\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 52.0550 - val_loss: 16.2386\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 50.3624 - val_loss: 16.2590\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 47.9228 - val_loss: 16.2802\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--01--51-E2E_LSTM_ValSet_100.0-ALPHA1000.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.0015 - val_loss: 4.3340e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0016 - val_loss: 4.3529e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0016 - val_loss: 4.3787e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0013 - val_loss: 4.3497e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0013 - val_loss: 4.2394e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0013 - val_loss: 4.0809e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0012 - val_loss: 3.4778e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0010 - val_loss: 2.1366e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 9.2698e-04 - val_loss: 1.7700e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.8569e-04 - val_loss: 1.8004e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.2447e-04 - val_loss: 1.8347e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.2691e-04 - val_loss: 1.8773e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.2866e-04 - val_loss: 1.7724e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.7303e-04 - val_loss: 1.7754e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.6833e-04 - val_loss: 1.7783e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.4287e-04 - val_loss: 1.7810e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6261e-04 - val_loss: 1.7837e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5316e-04 - val_loss: 1.7863e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--02--05-E2E_LSTM_ValSet_10.0-ALPHA0.0001-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0002811713861730041\n",
      "Model: \"functional_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_70 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0022 - val_loss: 0.0013\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--02--19-E2E_LSTM_ValSet_10.0-ALPHA0.001-BETA_SD100998-6Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0011940737714132457\n",
      "Model: \"functional_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_75 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 0.0104 - val_loss: 0.0093\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0106 - val_loss: 0.0092\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0104 - val_loss: 0.0090\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0104 - val_loss: 0.0090\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0101 - val_loss: 0.0088\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0101 - val_loss: 0.0087\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0101 - val_loss: 0.0086\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0098 - val_loss: 0.0085\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0099 - val_loss: 0.0084\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0095 - val_loss: 0.0082\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0094 - val_loss: 0.0080\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0093 - val_loss: 0.0079\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0092 - val_loss: 0.0078\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0094 - val_loss: 0.0076\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0092 - val_loss: 0.0075\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0087 - val_loss: 0.0073\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0085 - val_loss: 0.0071\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0084 - val_loss: 0.0069\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0082 - val_loss: 0.0067\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0083 - val_loss: 0.0065\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0078 - val_loss: 0.0064\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0079 - val_loss: 0.0061\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0075 - val_loss: 0.0058\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0074 - val_loss: 0.0055\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0072 - val_loss: 0.0052\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0071 - val_loss: 0.0050\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0071 - val_loss: 0.0048\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0068 - val_loss: 0.0046\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0066 - val_loss: 0.0046\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0070 - val_loss: 0.0045\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0062 - val_loss: 0.0044\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0060 - val_loss: 0.0043\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0068 - val_loss: 0.0042\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0056 - val_loss: 0.0041\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0062 - val_loss: 0.0041\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0056 - val_loss: 0.0041\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0055 - val_loss: 0.0040\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0053 - val_loss: 0.0039\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0056 - val_loss: 0.0039\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.684e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0054 - val_loss: 0.0038\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0052 - val_loss: 0.0039\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0047 - val_loss: 0.0044\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.684e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0052 - val_loss: 0.0038\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0054 - val_loss: 0.0038\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0050 - val_loss: 0.0038\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0054 - val_loss: 0.0038\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--02--34-E2E_LSTM_ValSet_10.0-ALPHA0.01-BETA_SD100998-54Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0004081752594254747\n",
      "Model: \"functional_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_78 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_79 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.0920 - val_loss: 0.0900\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0918 - val_loss: 0.0890\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0911 - val_loss: 0.0880\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0898 - val_loss: 0.0871\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0889 - val_loss: 0.0861\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0880 - val_loss: 0.0852\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0872 - val_loss: 0.0838\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0854 - val_loss: 0.0823\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0836 - val_loss: 0.0808\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0826 - val_loss: 0.0793\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0807 - val_loss: 0.0779\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0795 - val_loss: 0.0764\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0780 - val_loss: 0.0741\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0758 - val_loss: 0.0720\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0735 - val_loss: 0.0699\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0711 - val_loss: 0.0679\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0696 - val_loss: 0.0658\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0675 - val_loss: 0.0637\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0645 - val_loss: 0.0602\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0612 - val_loss: 0.0566\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0574 - val_loss: 0.0530\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0535 - val_loss: 0.0493\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0507 - val_loss: 0.0455\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0466 - val_loss: 0.0416\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0429 - val_loss: 0.0353\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0369 - val_loss: 0.0290\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0306 - val_loss: 0.0226\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0243 - val_loss: 0.0164\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0189 - val_loss: 0.0110\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0150 - val_loss: 0.0080\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0129 - val_loss: 0.0152\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0233 - val_loss: 0.0180\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0283 - val_loss: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0213 - val_loss: 0.0082\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0155 - val_loss: 0.0067\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0135 - val_loss: 0.0070\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0127 - val_loss: 0.0067\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0139 - val_loss: 0.0067\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0139 - val_loss: 0.0067\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0127 - val_loss: 0.0067\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0122 - val_loss: 0.0067\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0128 - val_loss: 0.0068\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--02--49-E2E_LSTM_ValSet_10.0-ALPHA0.1-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_82 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_83 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.0014 - val_loss: 3.4204e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0015 - val_loss: 3.4311e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0015 - val_loss: 3.4480e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0012 - val_loss: 3.4067e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0012 - val_loss: 3.2859e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0012 - val_loss: 3.1131e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0011 - val_loss: 2.4804e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 9.0557e-04 - val_loss: 1.1165e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.2746e-04 - val_loss: 7.7518e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.8518e-04 - val_loss: 8.0239e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.2387e-04 - val_loss: 8.3339e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.2623e-04 - val_loss: 8.7345e-05\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 6.2818e-04 - val_loss: 7.7758e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.7249e-04 - val_loss: 7.8016e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.6781e-04 - val_loss: 7.8267e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.4231e-04 - val_loss: 7.8506e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.6172e-04 - val_loss: 7.8736e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.5283e-04 - val_loss: 7.8963e-05\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--03--03-E2E_LSTM_ValSet_10.0-ALPHA0-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.000528343475064648\n",
      "Model: \"functional_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_86 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_87 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.9075 - val_loss: 0.8970\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.9028 - val_loss: 0.8869\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.8955 - val_loss: 0.8770\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8842 - val_loss: 0.8672\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8742 - val_loss: 0.8576\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8632 - val_loss: 0.8482\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.8557 - val_loss: 0.8333\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.8378 - val_loss: 0.8184\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8193 - val_loss: 0.8031\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.8082 - val_loss: 0.7878\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.7904 - val_loss: 0.7726\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.7755 - val_loss: 0.7576\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.7632 - val_loss: 0.7344\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.7405 - val_loss: 0.7129\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.7156 - val_loss: 0.6915\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.6901 - val_loss: 0.6701\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6704 - val_loss: 0.6485\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6486 - val_loss: 0.6266\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6217 - val_loss: 0.5911\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.5866 - val_loss: 0.5535\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5462 - val_loss: 0.5151\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.5045 - val_loss: 0.4764\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4710 - val_loss: 0.4369\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.4298 - val_loss: 0.3962\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3885 - val_loss: 0.3295\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3250 - val_loss: 0.2614\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2551 - val_loss: 0.1926\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1861 - val_loss: 0.1246\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1231 - val_loss: 0.0642\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0720 - val_loss: 0.0271\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0498 - val_loss: 0.1125\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1782 - val_loss: 0.1273\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2026 - val_loss: 0.0683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1434 - val_loss: 0.0303\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0828 - val_loss: 0.0214\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0530 - val_loss: 0.0279\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0568 - val_loss: 0.0215\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0593 - val_loss: 0.0218\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0610 - val_loss: 0.0222\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0562 - val_loss: 0.0227\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0530 - val_loss: 0.0232\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0515 - val_loss: 0.0238\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0575 - val_loss: 0.0214\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0560 - val_loss: 0.0214\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0594 - val_loss: 0.0214\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0604 - val_loss: 0.0214\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0588 - val_loss: 0.0214\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0579 - val_loss: 0.0215\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--03--18-E2E_LSTM_ValSet_10.0-ALPHA1.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_90 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_91 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 90.6167 - val_loss: 89.6693\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 90.1109 - val_loss: 88.6562\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 89.3811 - val_loss: 87.6595\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 88.2645 - val_loss: 86.6805\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 87.2528 - val_loss: 85.7190\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 86.1382 - val_loss: 84.7751\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 85.3933 - val_loss: 83.2854\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 83.6097 - val_loss: 81.7836\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 81.7385 - val_loss: 80.2549\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 80.6201 - val_loss: 78.7203\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 78.8493 - val_loss: 77.1958\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 77.3388 - val_loss: 75.6880\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 76.1347 - val_loss: 73.3644\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 73.8494 - val_loss: 71.2037\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 71.3414 - val_loss: 69.0618\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 68.7791 - val_loss: 66.9097\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 108ms/step - loss: 66.7559 - val_loss: 64.7424\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 64.5644 - val_loss: 62.5512\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 61.9095 - val_loss: 58.9777\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 58.3820 - val_loss: 55.2039\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 54.3066 - val_loss: 51.3471\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 50.1068 - val_loss: 47.4586\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 46.6998 - val_loss: 43.4801\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 42.5725 - val_loss: 39.3866\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 38.3948 - val_loss: 32.6794\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 32.0048 - val_loss: 25.8242\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 24.9350 - val_loss: 18.8795\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 17.9586 - val_loss: 12.0132\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 11.5717 - val_loss: 5.8843\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.3073 - val_loss: 2.0846\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 4.0821 - val_loss: 10.8266\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 17.2907 - val_loss: 11.9843\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 19.2059 - val_loss: 6.0772\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 13.4285 - val_loss: 2.3989\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.3825 - val_loss: 1.6163\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.3555 - val_loss: 2.3338\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.8642 - val_loss: 1.6401\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.0207 - val_loss: 1.6749\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.1878 - val_loss: 1.7191\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.8037 - val_loss: 1.7704\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5123 - val_loss: 1.8275\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.2767 - val_loss: 1.8877\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.9147 - val_loss: 1.6181\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.6880 - val_loss: 1.6200\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.0585 - val_loss: 1.6219\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.2057 - val_loss: 1.6238\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.0365 - val_loss: 1.6259\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.7926 - val_loss: 1.6280\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--03--33-E2E_LSTM_ValSet_10.0-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_94 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_95 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 906.1544 - val_loss: 896.6903\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 901.0935 - val_loss: 886.5585\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 893.7960 - val_loss: 876.5915\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 882.6312 - val_loss: 866.8015\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 872.5135 - val_loss: 857.1861\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 861.3651 - val_loss: 847.7474\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 853.9160 - val_loss: 832.8497\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 836.0807 - val_loss: 817.8311\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 817.3684 - val_loss: 802.5436\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 806.1831 - val_loss: 787.1976\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 788.4750 - val_loss: 771.9516\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 773.3680 - val_loss: 756.8741\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 761.3300 - val_loss: 733.6373\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 738.4758 - val_loss: 712.0297\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 713.3939 - val_loss: 690.6099\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 687.7696 - val_loss: 669.0882\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 667.5327 - val_loss: 647.4142\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 645.6168 - val_loss: 625.5015\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 619.0709 - val_loss: 589.7656\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 583.7944 - val_loss: 552.0254\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 543.0381 - val_loss: 513.4564\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 501.0369 - val_loss: 474.5691\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 466.9612 - val_loss: 434.7821\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 425.6886 - val_loss: 393.8456\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 383.9066 - val_loss: 326.7695\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 320.0034 - val_loss: 258.2128\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 249.2984 - val_loss: 188.7606\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 179.5269 - val_loss: 120.0909\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 115.6503 - val_loss: 58.7943\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 62.9926 - val_loss: 20.7887\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 40.7391 - val_loss: 108.2193\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 172.8488 - val_loss: 119.7828\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 191.9719 - val_loss: 60.7109\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 134.2111 - val_loss: 23.9338\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 73.7476 - val_loss: 16.1152\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 43.4696 - val_loss: 23.2956\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 48.5688 - val_loss: 16.3539\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 50.1244 - val_loss: 16.7019\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 51.7953 - val_loss: 17.1448\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 47.9637 - val_loss: 17.6581\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 45.0515 - val_loss: 18.2293\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 42.6875 - val_loss: 18.8319\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 49.0717 - val_loss: 16.1335\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 46.7983 - val_loss: 16.1523\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 50.5060 - val_loss: 16.1712\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.9818 - val_loss: 16.1907\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 50.2893 - val_loss: 16.2112\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 47.8359 - val_loss: 16.2324\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--03--48-E2E_LSTM_ValSet_10.0-ALPHA1000.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_98 (LSTM)               (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_99 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 2.2737e-04 - val_loss: 1.2566e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.4761e-04 - val_loss: 1.2674e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.4126e-04 - val_loss: 1.2792e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.1950e-04 - val_loss: 1.2921e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.1922e-04 - val_loss: 1.2934e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.2084e-04 - val_loss: 1.2935e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--04--02-E2E_LSTM_ValSet_1.0-ALPHA0.0001-BETA_SD100998-6Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.001074384042275736\n",
      "Model: \"functional_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_102 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_103 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.0010 - val_loss: 9.3302e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0011 - val_loss: 9.2692e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0011 - val_loss: 9.2094e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0010 - val_loss: 9.1460e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0010 - val_loss: 9.0829e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0010 - val_loss: 9.0236e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0010 - val_loss: 8.9343e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0010 - val_loss: 8.8469e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0010 - val_loss: 8.7582e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0010 - val_loss: 8.6679e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 9.8852e-04 - val_loss: 8.5732e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 9.9460e-04 - val_loss: 8.4756e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 9.6244e-04 - val_loss: 8.3110e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 9.4803e-04 - val_loss: 8.1652e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 9.3960e-04 - val_loss: 8.0388e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 9.3095e-04 - val_loss: 7.9173e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 9.5564e-04 - val_loss: 7.7972e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 9.3655e-04 - val_loss: 7.6755e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 8.8761e-04 - val_loss: 7.4763e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 8.6972e-04 - val_loss: 7.2920e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.5523e-04 - val_loss: 7.1283e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.3901e-04 - val_loss: 6.9594e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.5022e-04 - val_loss: 6.7851e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.0573e-04 - val_loss: 6.6136e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 8.1764e-04 - val_loss: 6.3662e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 7.7577e-04 - val_loss: 6.1252e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.6092e-04 - val_loss: 5.8808e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.4606e-04 - val_loss: 5.6350e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.3696e-04 - val_loss: 5.3933e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.3579e-04 - val_loss: 5.1695e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 7.0736e-04 - val_loss: 4.8511e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 6.7508e-04 - val_loss: 4.5941e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.9012e-04 - val_loss: 4.4250e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.1728e-04 - val_loss: 4.3428e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.0859e-04 - val_loss: 4.3387e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.8755e-04 - val_loss: 4.3623e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 5.9533e-04 - val_loss: 4.3372e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 6.7522e-04 - val_loss: 4.3330e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.4893e-04 - val_loss: 4.3242e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.3688e-04 - val_loss: 4.3148e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.1418e-04 - val_loss: 4.3055e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.5767e-04 - val_loss: 4.2972e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 6.1700e-04 - val_loss: 4.2820e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 6.4004e-04 - val_loss: 4.2688e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.4292e-04 - val_loss: 4.2581e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.0238e-04 - val_loss: 4.2493e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.0194e-04 - val_loss: 4.2434e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.5170e-04 - val_loss: 4.2430e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.684e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 5.7377e-04 - val_loss: 4.2437e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.2131e-04 - val_loss: 4.2483e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.4843e-04 - val_loss: 4.2536e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.2508e-04 - val_loss: 4.2632e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.9345e-04 - val_loss: 4.2724e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.3695e-04 - val_loss: 4.2834e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--04--18-E2E_LSTM_ValSet_1.0-ALPHA0.001-BETA_SD100998-54Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.000989736019802835\n",
      "Model: \"functional_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_106 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_107 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.0092 - val_loss: 0.0090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0092 - val_loss: 0.0089\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0090 - val_loss: 0.0087\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0088 - val_loss: 0.0085\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0084 - val_loss: 0.0081\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0083 - val_loss: 0.0080\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0080 - val_loss: 0.0077\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0068 - val_loss: 0.0064\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0038 - val_loss: 0.0031\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0016 - val_loss: 8.7048e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0028 - val_loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0025 - val_loss: 9.8437e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0018 - val_loss: 7.0857e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0015 - val_loss: 6.6198e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0012 - val_loss: 8.1116e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.678e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0012 - val_loss: 6.6776e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0013 - val_loss: 6.7563e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0013 - val_loss: 6.8488e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0013 - val_loss: 6.9501e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0012 - val_loss: 7.0567e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0014 - val_loss: 7.1645e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--04--33-E2E_LSTM_ValSet_1.0-ALPHA0.01-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005227113467776171\n",
      "Model: \"functional_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_110 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_111 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 0.0908 - val_loss: 0.0897\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0903 - val_loss: 0.0887\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0896 - val_loss: 0.0877\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0884 - val_loss: 0.0867\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0874 - val_loss: 0.0858\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0863 - val_loss: 0.0848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0856 - val_loss: 0.0834\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0838 - val_loss: 0.0819\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0820 - val_loss: 0.0803\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0809 - val_loss: 0.0788\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0791 - val_loss: 0.0773\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0776 - val_loss: 0.0758\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0764 - val_loss: 0.0735\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0741 - val_loss: 0.0713\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0716 - val_loss: 0.0692\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0691 - val_loss: 0.0671\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0671 - val_loss: 0.0649\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0649 - val_loss: 0.0628\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0623 - val_loss: 0.0592\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0588 - val_loss: 0.0555\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0548 - val_loss: 0.0517\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0506 - val_loss: 0.0478\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0473 - val_loss: 0.0439\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0432 - val_loss: 0.0399\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0391 - val_loss: 0.0332\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0328 - val_loss: 0.0265\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0258 - val_loss: 0.0197\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0190 - val_loss: 0.0129\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0127 - val_loss: 0.0068\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0075 - val_loss: 0.0028\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0050 - val_loss: 0.0097\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0161 - val_loss: 0.0132\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0209 - val_loss: 0.0076\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0154 - val_loss: 0.0034\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0089 - val_loss: 0.0021\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0055 - val_loss: 0.0026\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0059 - val_loss: 0.0021\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0061 - val_loss: 0.0021\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0063 - val_loss: 0.0022\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0058 - val_loss: 0.0022\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0054 - val_loss: 0.0022\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0052 - val_loss: 0.0023\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--04--48-E2E_LSTM_ValSet_1.0-ALPHA0.1-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_114 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_115 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 1.3676e-04 - val_loss: 3.4532e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.5559e-04 - val_loss: 3.4938e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.4825e-04 - val_loss: 3.5396e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2561e-04 - val_loss: 3.5606e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.2453e-04 - val_loss: 3.4731e-05\n",
      "Epoch 6/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step - loss: 1.2522e-04 - val_loss: 3.3672e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 1.1119e-04 - val_loss: 2.9715e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 9.6919e-05 - val_loss: 2.0118e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 9.0943e-05 - val_loss: 9.1203e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.6969e-05 - val_loss: 8.6403e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.9608e-05 - val_loss: 8.9885e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.9022e-05 - val_loss: 9.3790e-06\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.600e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 5.9066e-05 - val_loss: 8.6740e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.3769e-05 - val_loss: 8.7077e-06\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.3620e-05 - val_loss: 8.7407e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.0974e-05 - val_loss: 8.7727e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.2875e-05 - val_loss: 8.8030e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.2356e-05 - val_loss: 8.8328e-06\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--05--02-E2E_LSTM_ValSet_1.0-ALPHA0-BETA_SD100998-18Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.00032590244724551757\n",
      "Model: \"functional_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_118 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_119 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 9.0617 - val_loss: 8.9669\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 9.0111 - val_loss: 8.8656\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.9381 - val_loss: 8.7660\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.8265 - val_loss: 8.6681\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.7253 - val_loss: 8.5719\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.6139 - val_loss: 8.4776\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 8.5394 - val_loss: 8.3286\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 8.3610 - val_loss: 8.1784\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.1739 - val_loss: 8.0255\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.0621 - val_loss: 7.8721\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.8850 - val_loss: 7.7196\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.7340 - val_loss: 7.5689\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 7.6136 - val_loss: 7.3365\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 7.3850 - val_loss: 7.1205\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.1342 - val_loss: 6.9063\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.8780 - val_loss: 6.6911\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.6757 - val_loss: 6.4744\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.4566 - val_loss: 6.2553\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 6.1911 - val_loss: 5.8980\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 5.8384 - val_loss: 5.5206\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 5.4309 - val_loss: 5.1350\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 5.0109 - val_loss: 4.7462\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 4.6703 - val_loss: 4.3484\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 4.2576 - val_loss: 3.9391\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 3.8399 - val_loss: 3.2684\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 3.2009 - val_loss: 2.5829\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.4940 - val_loss: 1.8885\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.7964 - val_loss: 1.2019\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.1577 - val_loss: 0.5889\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6311 - val_loss: 0.2086\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4082 - val_loss: 1.0810\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.7274 - val_loss: 1.1989\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9213 - val_loss: 0.6084\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3439 - val_loss: 0.2402\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7389 - val_loss: 0.1616\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4358 - val_loss: 0.2331\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4866 - val_loss: 0.1640\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5023 - val_loss: 0.1674\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5190 - val_loss: 0.1718\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4805 - val_loss: 0.1769\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4514 - val_loss: 0.1826\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4277 - val_loss: 0.1886\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4917 - val_loss: 0.1618\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4690 - val_loss: 0.1620\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5060 - val_loss: 0.1622\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5208 - val_loss: 0.1624\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5039 - val_loss: 0.1626\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4795 - val_loss: 0.1628\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--05--18-E2E_LSTM_ValSet_1.0-ALPHA10.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_122 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_123 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 90.6154 - val_loss: 89.6690\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 136ms/step - loss: 90.1094 - val_loss: 88.6559\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 89.3796 - val_loss: 87.6592\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 88.2632 - val_loss: 86.6802\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 87.2514 - val_loss: 85.7187\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 86.1365 - val_loss: 84.7748\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 85.3917 - val_loss: 83.2850\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 83.6081 - val_loss: 81.7832\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 81.7369 - val_loss: 80.2544\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 80.6184 - val_loss: 78.7198\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 78.8476 - val_loss: 77.1952\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 77.3369 - val_loss: 75.6875\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 76.1331 - val_loss: 73.3638\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 73.8477 - val_loss: 71.2031\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 71.3395 - val_loss: 69.0611\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 68.7771 - val_loss: 66.9090\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 66.7534 - val_loss: 64.7416\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 64.5619 - val_loss: 62.5503\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 61.9073 - val_loss: 58.9768\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 58.3797 - val_loss: 55.2028\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 54.3041 - val_loss: 51.3460\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 50.1040 - val_loss: 47.4573\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 46.6965 - val_loss: 43.4786\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 42.5692 - val_loss: 39.3850\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 38.3911 - val_loss: 32.6775\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 32.0009 - val_loss: 25.8219\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 24.9304 - val_loss: 18.8767\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 17.9534 - val_loss: 12.0098\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 11.5657 - val_loss: 5.8801\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.2997 - val_loss: 2.0790\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 4.0739 - val_loss: 10.8200\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 17.2829 - val_loss: 11.9784\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 19.1976 - val_loss: 6.0717\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 13.4221 - val_loss: 2.3938\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.3755 - val_loss: 1.6115\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3472 - val_loss: 2.3292\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.8571 - val_loss: 1.6353\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.0127 - val_loss: 1.6701\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.1798 - val_loss: 1.7144\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.7965 - val_loss: 1.7657\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5053 - val_loss: 1.8228\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.2688 - val_loss: 1.8830\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.9075 - val_loss: 1.6133\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.6801 - val_loss: 1.6152\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.0508 - val_loss: 1.6171\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.1985 - val_loss: 1.6190\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.0292 - val_loss: 1.6211\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.7839 - val_loss: 1.6232\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--05--33-E2E_LSTM_ValSet_1.0-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_126 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_127 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 906.1533 - val_loss: 896.6898\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 901.0919 - val_loss: 886.5583\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 893.7944 - val_loss: 876.5912\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 882.6299 - val_loss: 866.8012\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 872.5121 - val_loss: 857.1858\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 861.3633 - val_loss: 847.7470\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 853.9145 - val_loss: 832.8493\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 836.0793 - val_loss: 817.8306\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 817.3668 - val_loss: 802.5432\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 806.1815 - val_loss: 787.1971\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 788.4734 - val_loss: 771.9510\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 773.3661 - val_loss: 756.8735\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 761.3282 - val_loss: 733.6368\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 738.4741 - val_loss: 712.0289\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 713.3919 - val_loss: 690.6093\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 687.7676 - val_loss: 669.0874\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 667.5302 - val_loss: 647.4135\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 645.6143 - val_loss: 625.5005\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 619.0687 - val_loss: 589.7647\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 583.7921 - val_loss: 552.0244\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 543.0355 - val_loss: 513.4551\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 501.0341 - val_loss: 474.5677\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 466.9578 - val_loss: 434.7807\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 425.6852 - val_loss: 393.8440\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 383.9029 - val_loss: 326.7676\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 319.9994 - val_loss: 258.2105\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 249.2937 - val_loss: 188.7578\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 179.5217 - val_loss: 120.0875\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 115.6442 - val_loss: 58.7900\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 62.9850 - val_loss: 20.7832\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 40.7308 - val_loss: 108.2128\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step - loss: 172.8411 - val_loss: 119.7770\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 191.9636 - val_loss: 60.7054\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 134.2047 - val_loss: 23.9286\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 73.7406 - val_loss: 16.1104\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 43.4613 - val_loss: 23.2910\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 48.5617 - val_loss: 16.3491\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 50.1163 - val_loss: 16.6971\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.7873 - val_loss: 17.1400\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 47.9564 - val_loss: 17.6534\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 45.0444 - val_loss: 18.2246\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 42.6796 - val_loss: 18.8272\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 49.0645 - val_loss: 16.1287\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 46.7904 - val_loss: 16.1474\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 50.4983 - val_loss: 16.1664\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.9745 - val_loss: 16.1859\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 50.2820 - val_loss: 16.2064\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 47.8272 - val_loss: 16.2276\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--05--48-E2E_LSTM_ValSet_1.0-ALPHA1000.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_130 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_131 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 1.0429e-04 - val_loss: 9.3564e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.0705e-04 - val_loss: 9.3167e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 1.0683e-04 - val_loss: 9.2770e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.0454e-04 - val_loss: 9.2329e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.0467e-04 - val_loss: 9.1873e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.0563e-04 - val_loss: 9.1443e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 1.0488e-04 - val_loss: 9.0782e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.0311e-04 - val_loss: 9.0128e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.0305e-04 - val_loss: 8.9495e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.0354e-04 - val_loss: 8.8874e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.0116e-04 - val_loss: 8.8244e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.0185e-04 - val_loss: 8.7612e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 9.9313e-05 - val_loss: 8.6543e-05\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step - loss: 9.8019e-05 - val_loss: 8.5412e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 9.7352e-05 - val_loss: 8.4194e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 9.6949e-05 - val_loss: 8.2968e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 9.9042e-05 - val_loss: 8.1942e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 9.7783e-05 - val_loss: 8.1023e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 9.3004e-05 - val_loss: 7.9596e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 9.1898e-05 - val_loss: 7.8169e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 9.0489e-05 - val_loss: 7.6660e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.9330e-05 - val_loss: 7.5130e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.9993e-05 - val_loss: 7.3812e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.6412e-05 - val_loss: 7.2561e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 8.7339e-05 - val_loss: 7.0613e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 8.3938e-05 - val_loss: 6.8818e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.2363e-05 - val_loss: 6.7268e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 8.1606e-05 - val_loss: 6.5708e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.9945e-05 - val_loss: 6.4105e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1077e-05 - val_loss: 6.2507e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 7.8373e-05 - val_loss: 6.0003e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 7.4659e-05 - val_loss: 5.7464e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.4686e-05 - val_loss: 5.5056e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.8503e-05 - val_loss: 5.2642e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 6.6717e-05 - val_loss: 5.0212e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.1533e-05 - val_loss: 4.7955e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 6.3409e-05 - val_loss: 4.4815e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 6.7509e-05 - val_loss: 4.2708e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.4170e-05 - val_loss: 4.1701e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.3504e-05 - val_loss: 4.1885e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.1084e-05 - val_loss: 4.2858e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.5599e-05 - val_loss: 4.4201e-05\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--06--03-E2E_LSTM_ValSet_0.1-ALPHA0.0001-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.000989736019802835\n",
      "Model: \"functional_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_134 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_135 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 627ms/step - loss: 9.1983e-04 - val_loss: 9.0159e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 9.1922e-04 - val_loss: 8.9271e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 9.1336e-04 - val_loss: 8.8386e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 9.0169e-04 - val_loss: 8.7510e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.9341e-04 - val_loss: 8.6645e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.8523e-04 - val_loss: 8.5793e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 8.7828e-04 - val_loss: 8.4461e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 8.6125e-04 - val_loss: 8.3144e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.4522e-04 - val_loss: 8.1802e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.3661e-04 - val_loss: 8.0430e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1901e-04 - val_loss: 7.9050e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.0726e-04 - val_loss: 7.7677e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 7.9368e-04 - val_loss: 7.5516e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 7.7335e-04 - val_loss: 7.3498e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.5187e-04 - val_loss: 7.1589e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.2942e-04 - val_loss: 6.9693e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 7.1593e-04 - val_loss: 6.7791e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.9743e-04 - val_loss: 6.5884e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 6.6946e-04 - val_loss: 6.2833e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 6.4044e-04 - val_loss: 5.9792e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.0723e-04 - val_loss: 5.6691e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.7301e-04 - val_loss: 5.3472e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 5.4954e-04 - val_loss: 5.0208e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 5.1423e-04 - val_loss: 4.6913e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 4.8284e-04 - val_loss: 4.1590e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 4.3123e-04 - val_loss: 3.6328e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.7744e-04 - val_loss: 3.0965e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 3.2517e-04 - val_loss: 2.5586e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.7317e-04 - val_loss: 2.0308e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.3033e-04 - val_loss: 1.5304e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1.8266e-04 - val_loss: 8.8655e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.4186e-04 - val_loss: 7.2085e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5273e-04 - val_loss: 1.2594e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.3069e-04 - val_loss: 1.4935e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5938e-04 - val_loss: 1.2080e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.3125e-04 - val_loss: 8.6784e-05\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.4152e-04 - val_loss: 7.4158e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.5965e-04 - val_loss: 7.5779e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6949e-04 - val_loss: 7.6558e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5496e-04 - val_loss: 7.6604e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5245e-04 - val_loss: 7.5977e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5600e-04 - val_loss: 7.4892e-05\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--06--19-E2E_LSTM_ValSet_0.1-ALPHA0.001-BETA_SD100998-42Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005810797416054452\n",
      "Model: \"functional_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_138 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_139 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0077 - val_loss: 0.0074\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0068 - val_loss: 0.0065\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0044 - val_loss: 0.0041\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0015 - val_loss: 8.9313e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 9.2664e-04 - val_loss: 4.0592e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.4945e-04 - val_loss: 4.3810e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.6589e-04 - val_loss: 0.0014\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0020 - val_loss: 5.3609e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0012 - val_loss: 2.5413e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.8614e-04 - val_loss: 2.1958e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.5397e-04 - val_loss: 3.9101e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.1065e-04 - val_loss: 6.3198e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.5200e-04 - val_loss: 8.3829e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.0470e-04 - val_loss: 9.6970e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.7006e-04 - val_loss: 0.0010\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0010 - val_loss: 9.8670e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.678e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.5525e-04 - val_loss: 2.2753e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 5.3772e-04 - val_loss: 2.3736e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.7178e-04 - val_loss: 2.4832e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.7144e-04 - val_loss: 2.5991e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.4133e-04 - val_loss: 2.7179e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.3752e-04 - val_loss: 2.8350e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.678e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 6.2776e-04 - val_loss: 2.2024e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 5.4236e-04 - val_loss: 2.2090e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.7758e-04 - val_loss: 2.2154e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.5148e-04 - val_loss: 2.2218e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.8765e-04 - val_loss: 2.2281e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.1262e-04 - val_loss: 2.2343e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--06--34-E2E_LSTM_ValSet_0.1-ALPHA0.01-BETA_SD100998-54Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005227113467776171\n",
      "Model: \"functional_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_142 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_143 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 1.3676e-05 - val_loss: 3.4933e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.5777e-05 - val_loss: 3.5809e-06\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5214e-05 - val_loss: 3.6766e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3127e-05 - val_loss: 3.7793e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3170e-05 - val_loss: 3.8439e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3539e-05 - val_loss: 3.8291e-06\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.000e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.4829e-05 - val_loss: 3.5011e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.3958e-05 - val_loss: 3.5101e-06\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4667e-05 - val_loss: 3.5192e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5265e-05 - val_loss: 3.5276e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3604e-05 - val_loss: 3.5364e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4192e-05 - val_loss: 3.5453e-06\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--06--48-E2E_LSTM_ValSet_0.1-ALPHA0-BETA_SD100998-12Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0009600191097630716\n",
      "Model: \"functional_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_37 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_146 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_147 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.9062 - val_loss: 0.8967\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.9011 - val_loss: 0.8866\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.8938 - val_loss: 0.8766\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.8827 - val_loss: 0.8668\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8726 - val_loss: 0.8572\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8614 - val_loss: 0.8478\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.8540 - val_loss: 0.8329\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.8361 - val_loss: 0.8179\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.8174 - val_loss: 0.8026\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.8063 - val_loss: 0.7873\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7886 - val_loss: 0.7720\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7735 - val_loss: 0.7570\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.7614 - val_loss: 0.7337\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.7386 - val_loss: 0.7121\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7135 - val_loss: 0.6907\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.6879 - val_loss: 0.6692\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.6677 - val_loss: 0.6476\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.6458 - val_loss: 0.6257\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.6193 - val_loss: 0.5900\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.5840 - val_loss: 0.5522\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.5433 - val_loss: 0.5137\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5013 - val_loss: 0.4749\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.4673 - val_loss: 0.4351\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4260 - val_loss: 0.3942\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3843 - val_loss: 0.3272\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3205 - val_loss: 0.2588\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2499 - val_loss: 0.1894\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1802 - val_loss: 0.1208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1163 - val_loss: 0.0594\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0635 - val_loss: 0.0210\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0408 - val_loss: 0.1058\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1702 - val_loss: 0.1209\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1933 - val_loss: 0.0621\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1361 - val_loss: 0.0246\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0750 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0439 - val_loss: 0.0229\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0490 - val_loss: 0.0163\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0505 - val_loss: 0.0166\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0522 - val_loss: 0.0170\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0482 - val_loss: 0.0175\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0453 - val_loss: 0.0180\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0428 - val_loss: 0.0186\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0495 - val_loss: 0.0161\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0472 - val_loss: 0.0161\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0509 - val_loss: 0.0162\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0524 - val_loss: 0.0162\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0508 - val_loss: 0.0162\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0483 - val_loss: 0.0162\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--07--04-E2E_LSTM_ValSet_0.1-ALPHA1.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_150 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_151 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 9.0615 - val_loss: 8.9669\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 9.0109 - val_loss: 8.8656\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.9380 - val_loss: 8.7659\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.8263 - val_loss: 8.6680\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 8.7252 - val_loss: 8.5719\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.6137 - val_loss: 8.4775\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 8.5392 - val_loss: 8.3285\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 8.3609 - val_loss: 8.1784\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.1738 - val_loss: 8.0255\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.0619 - val_loss: 7.8721\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 7.8848 - val_loss: 7.7196\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.7338 - val_loss: 7.5688\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 7.6134 - val_loss: 7.3365\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 7.3849 - val_loss: 7.1204\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7.1341 - val_loss: 6.9062\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 6.8778 - val_loss: 6.6910\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.6755 - val_loss: 6.4743\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.4563 - val_loss: 6.2552\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 6.1909 - val_loss: 5.8979\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 5.8382 - val_loss: 5.5205\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 5.4306 - val_loss: 5.1349\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 5.0107 - val_loss: 4.7460\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 4.6699 - val_loss: 4.3482\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.2573 - val_loss: 3.9389\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 3.8395 - val_loss: 3.2682\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 3.2005 - val_loss: 2.5827\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.4936 - val_loss: 1.8882\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.7959 - val_loss: 1.2016\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.1571 - val_loss: 0.5885\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6304 - val_loss: 0.2080\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4073 - val_loss: 1.0804\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.7267 - val_loss: 1.1983\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.9204 - val_loss: 0.6078\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3432 - val_loss: 0.2397\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7382 - val_loss: 0.1611\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4349 - val_loss: 0.2326\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4859 - val_loss: 0.1635\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5015 - val_loss: 0.1669\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5182 - val_loss: 0.1713\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4798 - val_loss: 0.1764\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4507 - val_loss: 0.1821\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4270 - val_loss: 0.1881\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4910 - val_loss: 0.1613\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4682 - val_loss: 0.1615\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5053 - val_loss: 0.1617\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5201 - val_loss: 0.1619\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5032 - val_loss: 0.1621\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4786 - val_loss: 0.1623\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--07--20-E2E_LSTM_ValSet_0.1-ALPHA10.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_154 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_155 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 90.6153 - val_loss: 89.6690\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 90.1092 - val_loss: 88.6558\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 89.3795 - val_loss: 87.6591\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 88.2630 - val_loss: 86.6801\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 87.2512 - val_loss: 85.7186\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 86.1364 - val_loss: 84.7747\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 85.3915 - val_loss: 83.2850\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 83.6080 - val_loss: 81.7831\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 81.7367 - val_loss: 80.2544\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 80.6182 - val_loss: 78.7198\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 78.8474 - val_loss: 77.1952\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 77.3367 - val_loss: 75.6874\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 76.1329 - val_loss: 73.3638\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 73.8475 - val_loss: 71.2030\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 71.3393 - val_loss: 69.0611\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 68.7769 - val_loss: 66.9089\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 66.7532 - val_loss: 64.7415\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 64.5616 - val_loss: 62.5503\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 61.9071 - val_loss: 58.9767\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 58.3794 - val_loss: 55.2027\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 54.3038 - val_loss: 51.3458\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 50.1037 - val_loss: 47.4571\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 46.6961 - val_loss: 43.4785\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 42.5689 - val_loss: 39.3848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 38.3907 - val_loss: 32.6773\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 32.0004 - val_loss: 25.8216\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 24.9300 - val_loss: 18.8764\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 17.9528 - val_loss: 12.0095\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 11.5651 - val_loss: 5.8796\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.2990 - val_loss: 2.0785\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.0730 - val_loss: 10.8194\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 17.2822 - val_loss: 11.9778\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 19.1968 - val_loss: 6.0712\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 13.4215 - val_loss: 2.3933\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.3748 - val_loss: 1.6110\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3464 - val_loss: 2.3288\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 4.8564 - val_loss: 1.6349\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.0119 - val_loss: 1.6696\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.1790 - val_loss: 1.7139\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.7958 - val_loss: 1.7652\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.5046 - val_loss: 1.8223\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.2680 - val_loss: 1.8825\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.9068 - val_loss: 1.6128\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.6793 - val_loss: 1.6147\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.0501 - val_loss: 1.6166\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.1977 - val_loss: 1.6186\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.0285 - val_loss: 1.6206\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.7831 - val_loss: 1.6227\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--07--35-E2E_LSTM_ValSet_0.1-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_158 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_159 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 906.1531 - val_loss: 896.6898\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 901.0918 - val_loss: 886.5582\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 893.7943 - val_loss: 876.5912\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 882.6297 - val_loss: 866.8011\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 872.5119 - val_loss: 857.1858\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 861.3632 - val_loss: 847.7470\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 853.9143 - val_loss: 832.8492\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 836.0791 - val_loss: 817.8306\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 817.3665 - val_loss: 802.5430\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 806.1811 - val_loss: 787.1971\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 788.4733 - val_loss: 771.9510\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 773.3660 - val_loss: 756.8734\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 761.3282 - val_loss: 733.6367\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 738.4738 - val_loss: 712.0289\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 713.3917 - val_loss: 690.6091\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 687.7675 - val_loss: 669.0874\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 667.5299 - val_loss: 647.4135\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 645.6140 - val_loss: 625.5005\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 619.0685 - val_loss: 589.7646\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 583.7918 - val_loss: 552.0242\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 543.0353 - val_loss: 513.4550\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 501.0338 - val_loss: 474.5675\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 466.9575 - val_loss: 434.7805\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 425.6849 - val_loss: 393.8438\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 383.9025 - val_loss: 326.7674\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 319.9990 - val_loss: 258.2102\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 249.2933 - val_loss: 188.7575\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 179.5212 - val_loss: 120.0871\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 115.6436 - val_loss: 58.7896\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 62.9843 - val_loss: 20.7826\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 40.7300 - val_loss: 108.2123\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step - loss: 172.8404 - val_loss: 119.7764\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 191.9628 - val_loss: 60.7048\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 134.2040 - val_loss: 23.9281\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 73.7399 - val_loss: 16.1099\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 43.4605 - val_loss: 23.2905\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 48.5610 - val_loss: 16.3486\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 50.1155 - val_loss: 16.6967\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.7865 - val_loss: 17.1396\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 47.9557 - val_loss: 17.6529\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 45.0437 - val_loss: 18.2242\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 42.6788 - val_loss: 18.8267\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 49.0638 - val_loss: 16.1282\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 46.7896 - val_loss: 16.1470\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 50.4975 - val_loss: 16.1659\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.9738 - val_loss: 16.1854\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 50.2812 - val_loss: 16.2059\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 47.8264 - val_loss: 16.2271\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--07--51-E2E_LSTM_ValSet_0.1-ALPHA1000.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_162 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_163 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 9.1983e-05 - val_loss: 9.0439e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 9.2160e-05 - val_loss: 8.9800e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 9.1811e-05 - val_loss: 8.9140e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 9.0881e-05 - val_loss: 8.8469e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 9.0285e-05 - val_loss: 8.7794e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 8.9662e-05 - val_loss: 8.7117e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 8.9169e-05 - val_loss: 8.6041e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 8.7787e-05 - val_loss: 8.4981e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 8.6497e-05 - val_loss: 8.3934e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 8.5963e-05 - val_loss: 8.2883e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 8.4484e-05 - val_loss: 8.1818e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 8.3547e-05 - val_loss: 8.0733e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 8.2491e-05 - val_loss: 7.8966e-05\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step - loss: 8.0821e-05 - val_loss: 7.7194e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 7.9023e-05 - val_loss: 7.5449e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.7126e-05 - val_loss: 7.3812e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.6080e-05 - val_loss: 7.2246e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.4560e-05 - val_loss: 7.0697e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 7.2157e-05 - val_loss: 6.8232e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 6.9849e-05 - val_loss: 6.5780e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.7124e-05 - val_loss: 6.3335e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.4425e-05 - val_loss: 6.0953e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.2730e-05 - val_loss: 5.8665e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.0107e-05 - val_loss: 5.6393e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 5.7876e-05 - val_loss: 5.2694e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 5.4078e-05 - val_loss: 4.8832e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 5.0109e-05 - val_loss: 4.4951e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.6418e-05 - val_loss: 4.1139e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 4.2417e-05 - val_loss: 3.7336e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.9378e-05 - val_loss: 3.3520e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 3.5252e-05 - val_loss: 2.7469e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 3.0103e-05 - val_loss: 2.1676e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.4823e-05 - val_loss: 1.6301e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.9188e-05 - val_loss: 1.1690e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.6134e-05 - val_loss: 8.3715e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.4632e-05 - val_loss: 6.9632e-06\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.4092e-05 - val_loss: 9.4839e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.1210e-05 - val_loss: 1.4223e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9681e-05 - val_loss: 1.4074e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7650e-05 - val_loss: 1.0810e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.1927e-05 - val_loss: 8.0133e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.7948e-05 - val_loss: 6.9877e-06\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.684e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.4167e-05 - val_loss: 1.4767e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8383e-05 - val_loss: 2.1964e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7605e-05 - val_loss: 1.5513e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9645e-05 - val_loss: 9.0131e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9721e-05 - val_loss: 7.2167e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.5962e-05 - val_loss: 8.5283e-06\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.684e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.5239e-05 - val_loss: 6.9563e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.4888e-05 - val_loss: 7.0412e-06\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.6148e-05 - val_loss: 7.1602e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.7917e-05 - val_loss: 7.2536e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.6900e-05 - val_loss: 7.2904e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6617e-05 - val_loss: 7.2756e-06\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--08--07-E2E_LSTM_ValSet_0.01-ALPHA0.0001-BETA_SD100998-54Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0004986755447333272\n",
      "Model: \"functional_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_166 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_167 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 9.0752e-04 - val_loss: 8.9847e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 9.0414e-04 - val_loss: 8.8950e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 8.9806e-04 - val_loss: 8.8054e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.8786e-04 - val_loss: 8.7163e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.7888e-04 - val_loss: 8.6281e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.6887e-04 - val_loss: 8.5411e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 8.6228e-04 - val_loss: 8.4044e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 8.4584e-04 - val_loss: 8.2690e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.2863e-04 - val_loss: 8.1312e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.1894e-04 - val_loss: 7.9905e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.0230e-04 - val_loss: 7.8491e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.8845e-04 - val_loss: 7.7085e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 7.7725e-04 - val_loss: 7.4879e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 7.5618e-04 - val_loss: 7.2815e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.3305e-04 - val_loss: 7.0856e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.0930e-04 - val_loss: 6.8903e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.9157e-04 - val_loss: 6.6939e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 6.7197e-04 - val_loss: 6.4970e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 6.4762e-04 - val_loss: 6.1820e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 6.1696e-04 - val_loss: 5.8660e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.8155e-04 - val_loss: 5.5411e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.4491e-04 - val_loss: 5.2055e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.1669e-04 - val_loss: 4.8657e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.8144e-04 - val_loss: 4.5218e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 4.4659e-04 - val_loss: 3.9650e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.9266e-04 - val_loss: 3.4098e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.3423e-04 - val_loss: 2.8434e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7667e-04 - val_loss: 2.2707e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.1935e-04 - val_loss: 1.7063e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.6598e-04 - val_loss: 1.1637e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step - loss: 1.1488e-04 - val_loss: 4.4418e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 6.5982e-05 - val_loss: 2.5034e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.9782e-05 - val_loss: 9.0943e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.8231e-04 - val_loss: 1.1559e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.0875e-04 - val_loss: 8.0850e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5551e-04 - val_loss: 4.3525e-05\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.0898e-05 - val_loss: 2.7430e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.1179e-05 - val_loss: 2.9385e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.0460e-05 - val_loss: 3.0462e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.4602e-05 - val_loss: 3.0691e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.4414e-05 - val_loss: 3.0142e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.9140e-05 - val_loss: 2.9072e-05\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.3105e-05 - val_loss: 2.5236e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.1200e-05 - val_loss: 2.5381e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.3981e-05 - val_loss: 2.5475e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8393e-05 - val_loss: 2.5519e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.7017e-05 - val_loss: 2.5517e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.5290e-05 - val_loss: 2.5476e-05\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--08--23-E2E_LSTM_ValSet_0.01-ALPHA0.001-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005810797416054452\n",
      "Model: \"functional_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_43 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_170 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_171 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.0906 - val_loss: 0.0897\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0901 - val_loss: 0.0887\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0894 - val_loss: 0.0877\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0883 - val_loss: 0.0867\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0873 - val_loss: 0.0857\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0862 - val_loss: 0.0848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0854 - val_loss: 0.0833\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0836 - val_loss: 0.0818\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0818 - val_loss: 0.0803\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0807 - val_loss: 0.0788\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0789 - val_loss: 0.0772\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0774 - val_loss: 0.0757\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0762 - val_loss: 0.0734\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0739 - val_loss: 0.0713\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0714 - val_loss: 0.0691\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0689 - val_loss: 0.0670\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0668 - val_loss: 0.0648\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0647 - val_loss: 0.0627\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0620 - val_loss: 0.0591\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0585 - val_loss: 0.0554\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0545 - val_loss: 0.0515\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0503 - val_loss: 0.0477\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0469 - val_loss: 0.0437\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0428 - val_loss: 0.0397\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0387 - val_loss: 0.0330\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0323 - val_loss: 0.0262\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0253 - val_loss: 0.0193\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0184 - val_loss: 0.0125\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0120 - val_loss: 0.0063\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0067 - val_loss: 0.0022\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0041 - val_loss: 0.0090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0152 - val_loss: 0.0126\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0200 - val_loss: 0.0070\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0148 - val_loss: 0.0028\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0081 - val_loss: 0.0016\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0046 - val_loss: 0.0021\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0051 - val_loss: 0.0016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0054 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0050 - val_loss: 0.0017\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0044 - val_loss: 0.0018\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0050 - val_loss: 0.0016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0048 - val_loss: 0.0016\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0053 - val_loss: 0.0016\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0049 - val_loss: 0.0016\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--08--39-E2E_LSTM_ValSet_0.01-ALPHA0.1-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_174 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_175 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 1.3676e-06 - val_loss: 3.4772e-07\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.6005e-06 - val_loss: 3.5569e-07\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5642e-06 - val_loss: 3.6482e-07\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3720e-06 - val_loss: 3.7472e-07\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3983e-06 - val_loss: 3.8547e-07\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.4806e-06 - val_loss: 3.9717e-07\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.000e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.5026e-06 - val_loss: 3.4838e-07\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.4147e-06 - val_loss: 3.4916e-07\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.4908e-06 - val_loss: 3.5002e-07\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5514e-06 - val_loss: 3.5089e-07\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3877e-06 - val_loss: 3.5182e-07\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.4515e-06 - val_loss: 3.5281e-07\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--08--53-E2E_LSTM_ValSet_0.01-ALPHA0-BETA_SD100998-12Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0010730377563384608\n",
      "Model: \"functional_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_178 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_179 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.9062 - val_loss: 0.8967\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.9011 - val_loss: 0.8866\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.8938 - val_loss: 0.8766\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.8827 - val_loss: 0.8668\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8725 - val_loss: 0.8572\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.8614 - val_loss: 0.8478\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8540 - val_loss: 0.8329\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.8361 - val_loss: 0.8179\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.8174 - val_loss: 0.8026\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.8062 - val_loss: 0.7873\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.7885 - val_loss: 0.7720\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.7734 - val_loss: 0.7569\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.7614 - val_loss: 0.7337\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.7386 - val_loss: 0.7121\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.7135 - val_loss: 0.6907\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.6879 - val_loss: 0.6692\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.6677 - val_loss: 0.6475\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6458 - val_loss: 0.6256\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.6192 - val_loss: 0.5899\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step - loss: 0.5840 - val_loss: 0.5522\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.5432 - val_loss: 0.5137\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.5013 - val_loss: 0.4748\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.4672 - val_loss: 0.4351\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.4260 - val_loss: 0.3942\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.3843 - val_loss: 0.3272\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3204 - val_loss: 0.2587\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2498 - val_loss: 0.1894\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1801 - val_loss: 0.1208\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1162 - val_loss: 0.0594\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0635 - val_loss: 0.0210\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0407 - val_loss: 0.1057\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1701 - val_loss: 0.1208\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1932 - val_loss: 0.0621\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1361 - val_loss: 0.0246\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0749 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0438 - val_loss: 0.0228\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0489 - val_loss: 0.0163\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0504 - val_loss: 0.0166\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0521 - val_loss: 0.0170\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0482 - val_loss: 0.0174\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0452 - val_loss: 0.0180\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0428 - val_loss: 0.0186\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0495 - val_loss: 0.0161\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0471 - val_loss: 0.0161\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0508 - val_loss: 0.0161\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0523 - val_loss: 0.0161\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0507 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0482 - val_loss: 0.0162\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--09--09-E2E_LSTM_ValSet_0.01-ALPHA1.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_182 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_183 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 629ms/step - loss: 9.0615 - val_loss: 8.9669\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 9.0109 - val_loss: 8.8656\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.9380 - val_loss: 8.7659\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step - loss: 8.8263 - val_loss: 8.6680\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.7252 - val_loss: 8.5719\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.6137 - val_loss: 8.4775\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 8.5392 - val_loss: 8.3285\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 8.3609 - val_loss: 8.1784\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1737 - val_loss: 8.0255\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.0619 - val_loss: 7.8720\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.8848 - val_loss: 7.7196\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.7338 - val_loss: 7.5688\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 7.6134 - val_loss: 7.3365\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 7.3849 - val_loss: 7.1204\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.1340 - val_loss: 6.9062\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.8778 - val_loss: 6.6910\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 6.6755 - val_loss: 6.4743\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 6.4563 - val_loss: 6.2552\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 6.1909 - val_loss: 5.8979\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 5.8381 - val_loss: 5.5205\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 5.4306 - val_loss: 5.1349\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 5.0106 - val_loss: 4.7460\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.6699 - val_loss: 4.3482\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 4.2572 - val_loss: 3.9389\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 3.8394 - val_loss: 3.2682\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 3.2005 - val_loss: 2.5827\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.4935 - val_loss: 1.8882\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.7959 - val_loss: 1.2016\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.1570 - val_loss: 0.5885\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.6303 - val_loss: 0.2080\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4073 - val_loss: 1.0803\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.7266 - val_loss: 1.1982\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9203 - val_loss: 0.6078\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3431 - val_loss: 0.2397\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.7381 - val_loss: 0.1611\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4349 - val_loss: 0.2326\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4859 - val_loss: 0.1634\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5014 - val_loss: 0.1669\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5181 - val_loss: 0.1713\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4797 - val_loss: 0.1764\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4506 - val_loss: 0.1821\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4269 - val_loss: 0.1881\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4909 - val_loss: 0.1613\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4682 - val_loss: 0.1614\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5052 - val_loss: 0.1616\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5200 - val_loss: 0.1618\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5031 - val_loss: 0.1620\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4786 - val_loss: 0.1622\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--09--26-E2E_LSTM_ValSet_0.01-ALPHA10.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_186 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_187 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 90.6153 - val_loss: 89.6690\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 90.1092 - val_loss: 88.6558\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 89.3794 - val_loss: 87.6591\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 88.2630 - val_loss: 86.6801\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 87.2512 - val_loss: 85.7186\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 86.1364 - val_loss: 84.7747\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 85.3915 - val_loss: 83.2850\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 83.6080 - val_loss: 81.7831\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 81.7367 - val_loss: 80.2544\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 80.6182 - val_loss: 78.7198\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 78.8474 - val_loss: 77.1952\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 77.3367 - val_loss: 75.6874\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 76.1329 - val_loss: 73.3638\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 73.8475 - val_loss: 71.2030\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 71.3393 - val_loss: 69.0610\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 68.7769 - val_loss: 66.9089\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 66.7532 - val_loss: 64.7415\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 64.5616 - val_loss: 62.5503\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 61.9071 - val_loss: 58.9767\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 58.3794 - val_loss: 55.2027\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 54.3038 - val_loss: 51.3458\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 50.1037 - val_loss: 47.4571\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 46.6961 - val_loss: 43.4784\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 42.5689 - val_loss: 39.3848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 38.3907 - val_loss: 32.6772\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 32.0004 - val_loss: 25.8216\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 24.9299 - val_loss: 18.8764\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 17.9528 - val_loss: 12.0095\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 11.5650 - val_loss: 5.8796\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 6.2989 - val_loss: 2.0784\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.0729 - val_loss: 10.8193\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 17.2821 - val_loss: 11.9778\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step - loss: 19.1967 - val_loss: 6.0711\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 13.4214 - val_loss: 2.3932\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.3747 - val_loss: 1.6110\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.3463 - val_loss: 2.3287\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.8564 - val_loss: 1.6348\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.0118 - val_loss: 1.6696\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.1789 - val_loss: 1.7138\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.7957 - val_loss: 1.7651\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.5045 - val_loss: 1.8222\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.2680 - val_loss: 1.8825\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 4.9067 - val_loss: 1.6128\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.6793 - val_loss: 1.6147\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.0500 - val_loss: 1.6166\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.1977 - val_loss: 1.6185\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.0284 - val_loss: 1.6206\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.7830 - val_loss: 1.6227\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--09--42-E2E_LSTM_ValSet_0.01-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_190 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_191 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 906.1531 - val_loss: 896.6898\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 901.0918 - val_loss: 886.5582\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 893.7943 - val_loss: 876.5912\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 882.6297 - val_loss: 866.8011\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 872.5119 - val_loss: 857.1858\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 861.3632 - val_loss: 847.7469\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 853.9143 - val_loss: 832.8492\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 836.0791 - val_loss: 817.8305\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 817.3665 - val_loss: 802.5430\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 806.1811 - val_loss: 787.1971\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 788.4733 - val_loss: 771.9510\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 773.3659 - val_loss: 756.8734\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 761.3282 - val_loss: 733.6367\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 738.4738 - val_loss: 712.0289\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 713.3917 - val_loss: 690.6091\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step - loss: 687.7675 - val_loss: 669.0874\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 667.5299 - val_loss: 647.4135\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 645.6140 - val_loss: 625.5005\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 619.0685 - val_loss: 589.7646\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 583.7918 - val_loss: 552.0242\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 543.0353 - val_loss: 513.4550\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 501.0338 - val_loss: 474.5675\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 466.9574 - val_loss: 434.7805\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 425.6848 - val_loss: 393.8438\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 383.9025 - val_loss: 326.7673\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 319.9990 - val_loss: 258.2102\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 249.2932 - val_loss: 188.7575\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 179.5211 - val_loss: 120.0871\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 115.6435 - val_loss: 58.7895\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 62.9842 - val_loss: 20.7826\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 40.7299 - val_loss: 108.2122\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 172.8403 - val_loss: 119.7763\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 191.9627 - val_loss: 60.7048\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 134.2039 - val_loss: 23.9281\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 73.7398 - val_loss: 16.1099\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 43.4604 - val_loss: 23.2905\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 48.5609 - val_loss: 16.3486\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 50.1155 - val_loss: 16.6966\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.7864 - val_loss: 17.1395\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 47.9556 - val_loss: 17.6529\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 45.0436 - val_loss: 18.2241\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 42.6787 - val_loss: 18.8266\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 49.0637 - val_loss: 16.1281\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 46.7895 - val_loss: 16.1469\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 50.4974 - val_loss: 16.1659\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 51.9737 - val_loss: 16.1854\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 50.2812 - val_loss: 16.2058\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 47.8263 - val_loss: 16.2271\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--10--00-E2E_LSTM_ValSet_0.01-ALPHA1000.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_194 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_195 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 9.0752e-05 - val_loss: 9.0130e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 9.0665e-05 - val_loss: 8.9482e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 9.0300e-05 - val_loss: 8.8810e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 8.9513e-05 - val_loss: 8.8125e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 8.8845e-05 - val_loss: 8.7433e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 8.8048e-05 - val_loss: 8.6737e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 8.7591e-05 - val_loss: 8.5628e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 8.6263e-05 - val_loss: 8.4532e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.4846e-05 - val_loss: 8.3448e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 8.4204e-05 - val_loss: 8.2361e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 8.2826e-05 - val_loss: 8.1258e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.1695e-05 - val_loss: 8.0136e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 8.0844e-05 - val_loss: 7.8314e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 7.9116e-05 - val_loss: 7.6491e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 7.7181e-05 - val_loss: 7.4701e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.5137e-05 - val_loss: 7.3018e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.3723e-05 - val_loss: 7.1400e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 7.2084e-05 - val_loss: 6.9795e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 7.0059e-05 - val_loss: 6.7235e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 6.7589e-05 - val_loss: 6.4690e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 6.4712e-05 - val_loss: 6.2158e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.1804e-05 - val_loss: 5.9678e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 5.9763e-05 - val_loss: 5.7271e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 5.7153e-05 - val_loss: 5.4881e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 5.4663e-05 - val_loss: 5.0968e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 5.0707e-05 - val_loss: 4.6892e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 4.6415e-05 - val_loss: 4.2780e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 4.2293e-05 - val_loss: 3.8716e-05\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 3.8000e-05 - val_loss: 3.4657e-05\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.4117e-05 - val_loss: 3.0576e-05\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.9828e-05 - val_loss: 2.4086e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 2.4281e-05 - val_loss: 1.7851e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.8172e-05 - val_loss: 1.2093e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.2654e-05 - val_loss: 7.2067e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 9.0242e-06 - val_loss: 3.7946e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 5.9859e-06 - val_loss: 2.6229e-06\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 6.9303e-06 - val_loss: 6.4102e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.4325e-05 - val_loss: 1.2100e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 2.3982e-05 - val_loss: 1.1409e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 2.1815e-05 - val_loss: 7.3465e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.5243e-05 - val_loss: 4.0936e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 9.7496e-06 - val_loss: 2.8931e-06\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.684e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 7.1391e-06 - val_loss: 1.3277e-05\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.3195e-05 - val_loss: 2.0474e-05\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2561e-05 - val_loss: 1.2275e-05\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.3047e-05 - val_loss: 5.0602e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2453e-05 - val_loss: 3.0832e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.2585e-06 - val_loss: 4.3609e-06\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.684e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.2285e-06 - val_loss: 2.7295e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 7.2669e-06 - val_loss: 2.9238e-06\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 8.1679e-06 - val_loss: 3.1349e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.0471e-05 - val_loss: 3.2864e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 9.7844e-06 - val_loss: 3.3429e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.1407e-06 - val_loss: 3.3217e-06\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 2.684e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 7.4230e-06 - val_loss: 2.6271e-06\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.6474e-06 - val_loss: 2.6313e-06\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.2596e-06 - val_loss: 2.6351e-06\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.4188e-06 - val_loss: 2.6384e-06\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9081e-06 - val_loss: 2.6406e-06\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.4287e-06 - val_loss: 2.6416e-06\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--10--18-E2E_LSTM_ValSet_0.001-ALPHA0.0001-BETA_SD100998-60Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0004986755447333272\n",
      "Model: \"functional_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_50 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_198 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_199 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0086 - val_loss: 0.0083\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0084 - val_loss: 0.0082\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0082 - val_loss: 0.0080\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0081 - val_loss: 0.0079\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0067 - val_loss: 0.0065\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0044 - val_loss: 0.0041\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0014 - val_loss: 8.5017e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.4641e-04 - val_loss: 3.5425e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 4.6361e-04 - val_loss: 3.7069e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.7105e-04 - val_loss: 0.0014\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0020 - val_loss: 4.8404e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0011 - val_loss: 2.0198e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 5.9523e-04 - val_loss: 1.6918e-04\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.678e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.7644e-04 - val_loss: 3.4327e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.2572e-04 - val_loss: 5.8575e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.7280e-04 - val_loss: 7.9237e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.3103e-04 - val_loss: 9.2351e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.9883e-04 - val_loss: 9.7159e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.4663e-04 - val_loss: 9.3874e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.678e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.7690e-04 - val_loss: 1.7741e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.5114e-04 - val_loss: 1.8748e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.8842e-04 - val_loss: 1.9865e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.9217e-04 - val_loss: 2.1039e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.6215e-04 - val_loss: 2.2241e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.4345e-04 - val_loss: 2.3420e-04\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.678e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 5.5180e-04 - val_loss: 1.6987e-04\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.5883e-04 - val_loss: 1.7055e-04\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.8916e-04 - val_loss: 1.7122e-04\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.6613e-04 - val_loss: 1.7187e-04\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.0500e-04 - val_loss: 1.7253e-04\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.2159e-04 - val_loss: 1.7316e-04\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--10--37-E2E_LSTM_ValSet_0.001-ALPHA0.01-BETA_SD100998-54Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005227113467776171\n",
      "Model: \"functional_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_202 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_203 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 675ms/step - loss: 0.0906 - val_loss: 0.0897\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0901 - val_loss: 0.0887\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0894 - val_loss: 0.0877\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0883 - val_loss: 0.0867\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0873 - val_loss: 0.0857\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0862 - val_loss: 0.0848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0854 - val_loss: 0.0833\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0836 - val_loss: 0.0818\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0818 - val_loss: 0.0803\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0807 - val_loss: 0.0788\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0789 - val_loss: 0.0772\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0774 - val_loss: 0.0757\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0762 - val_loss: 0.0734\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0739 - val_loss: 0.0713\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0714 - val_loss: 0.0691\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0689 - val_loss: 0.0670\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0668 - val_loss: 0.0648\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0647 - val_loss: 0.0627\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0620 - val_loss: 0.0591\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0585 - val_loss: 0.0554\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0545 - val_loss: 0.0515\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0503 - val_loss: 0.0477\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0469 - val_loss: 0.0437\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0428 - val_loss: 0.0397\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0387 - val_loss: 0.0330\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0323 - val_loss: 0.0262\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0253 - val_loss: 0.0193\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0184 - val_loss: 0.0125\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0120 - val_loss: 0.0063\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0067 - val_loss: 0.0022\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0041 - val_loss: 0.0090\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0152 - val_loss: 0.0126\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0200 - val_loss: 0.0070\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0148 - val_loss: 0.0028\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0081 - val_loss: 0.0016\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0046 - val_loss: 0.0021\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0051 - val_loss: 0.0016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0054 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0050 - val_loss: 0.0017\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0043 - val_loss: 0.0017\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0050 - val_loss: 0.0016\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0048 - val_loss: 0.0016\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0053 - val_loss: 0.0016\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0049 - val_loss: 0.0016\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--10--54-E2E_LSTM_ValSet_0.001-ALPHA0.1-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_52 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_206 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_207 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 1.3676e-07 - val_loss: 3.4420e-08\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.6101e-07 - val_loss: 3.4667e-08\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5842e-07 - val_loss: 3.4965e-08\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4006e-07 - val_loss: 3.5301e-08\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4417e-07 - val_loss: 3.5680e-08\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.5496e-07 - val_loss: 3.6118e-08\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--11--09-E2E_LSTM_ValSet_0.001-ALPHA0-BETA_SD100998-6Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 -0.0007068790658073748\n",
      "Model: \"functional_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_53 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_210 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_211 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 0.9062 - val_loss: 0.8967\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.9011 - val_loss: 0.8866\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.8938 - val_loss: 0.8766\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.8827 - val_loss: 0.8668\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.8725 - val_loss: 0.8572\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.8614 - val_loss: 0.8478\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.8540 - val_loss: 0.8329\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.8361 - val_loss: 0.8179\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.8174 - val_loss: 0.8026\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.8062 - val_loss: 0.7873\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.7885 - val_loss: 0.7720\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7734 - val_loss: 0.7569\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.7614 - val_loss: 0.7337\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.7386 - val_loss: 0.7121\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.7135 - val_loss: 0.6907\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.6879 - val_loss: 0.6692\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6677 - val_loss: 0.6475\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.6458 - val_loss: 0.6256\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.6192 - val_loss: 0.5899\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.5840 - val_loss: 0.5522\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.5432 - val_loss: 0.5137\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5013 - val_loss: 0.4748\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.4672 - val_loss: 0.4351\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4260 - val_loss: 0.3942\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3843 - val_loss: 0.3272\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3204 - val_loss: 0.2587\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2498 - val_loss: 0.1894\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1801 - val_loss: 0.1207\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1162 - val_loss: 0.0594\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0634 - val_loss: 0.0209\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0407 - val_loss: 0.1057\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1701 - val_loss: 0.1208\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1932 - val_loss: 0.0621\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1361 - val_loss: 0.0246\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0749 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0438 - val_loss: 0.0228\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0489 - val_loss: 0.0163\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0504 - val_loss: 0.0166\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0521 - val_loss: 0.0170\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0482 - val_loss: 0.0174\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0452 - val_loss: 0.0180\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0427 - val_loss: 0.0186\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0495 - val_loss: 0.0161\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0471 - val_loss: 0.0161\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0508 - val_loss: 0.0161\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0523 - val_loss: 0.0161\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0507 - val_loss: 0.0161\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0482 - val_loss: 0.0162\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--11--26-E2E_LSTM_ValSet_0.001-ALPHA1.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_214 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_215 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 644ms/step - loss: 9.0615 - val_loss: 8.9669\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 9.0109 - val_loss: 8.8656\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 8.9380 - val_loss: 8.7659\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.8263 - val_loss: 8.6680\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 8.7252 - val_loss: 8.5719\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 8.6137 - val_loss: 8.4775\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 8.5392 - val_loss: 8.3285\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 8.3609 - val_loss: 8.1784\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.1737 - val_loss: 8.0255\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step - loss: 8.0619 - val_loss: 7.8720\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7.8848 - val_loss: 7.7196\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.7338 - val_loss: 7.5688\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 7.6134 - val_loss: 7.3365\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 7.3849 - val_loss: 7.1204\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 7.1340 - val_loss: 6.9062\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 6.8778 - val_loss: 6.6910\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 6.6755 - val_loss: 6.4743\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 6.4563 - val_loss: 6.2552\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 6.1909 - val_loss: 5.8979\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 5.8381 - val_loss: 5.5205\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 5.4306 - val_loss: 5.1349\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 5.0106 - val_loss: 4.7460\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 4.6699 - val_loss: 4.3482\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 4.2572 - val_loss: 3.9389\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 3.8394 - val_loss: 3.2682\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.2005 - val_loss: 2.5827\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.4935 - val_loss: 1.8882\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.7958 - val_loss: 1.2016\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.1570 - val_loss: 0.5885\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.6303 - val_loss: 0.2080\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4073 - val_loss: 1.0803\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.7266 - val_loss: 1.1982\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9203 - val_loss: 0.6078\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3431 - val_loss: 0.2396\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.7381 - val_loss: 0.1611\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4349 - val_loss: 0.2326\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4858 - val_loss: 0.1634\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5014 - val_loss: 0.1669\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5181 - val_loss: 0.1713\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4797 - val_loss: 0.1764\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4506 - val_loss: 0.1821\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4269 - val_loss: 0.1881\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4909 - val_loss: 0.1613\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4682 - val_loss: 0.1614\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5052 - val_loss: 0.1616\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5200 - val_loss: 0.1618\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5031 - val_loss: 0.1620\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4786 - val_loss: 0.1622\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--11--43-E2E_LSTM_ValSet_0.001-ALPHA10.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | Day 8 | Day 9 | Day 10 | Day 11 | Day 12 | Day 13 | Day 14 | Day 15 | Day 16 | Day 17 | Day 18 | Day 19 | Day 20 | Day 21 | Day 22 | Day 23 | Day 24 | Day 25 | Day 26 | Day 27 | Day 28 | Day 29 | Day 30 | Day 31 | Day 32 | Day 33 | Day 34 | Day 35 | Day 36 | Day 37 | Day 38 | Day 39 | Day 40 | Day 41 | Day 42 | Day 43 | Day 44 | Day 45 | Day 46 | Day 47 | Day 48 | Day 49 | Day 50 | Day 51 | Day 52 | Day 53 | Day 54 | Day 55 | Day 56 | Day 57 | Day 58 | Day 59 | Day 60 | Day 61 | Day 62 | Day 63 | Day 64 | Day 65 | Day 66 | Day 67 | Day 68 | Day 69 | Day 70 | Day 71 | Day 72 | Day 73 | Day 74 | Day 75 | Day 76 | Day 77 | Day 78 | Day 79 | Day 80 | Day 81 | Day 82 | Day 83 | Day 84 | Day 85 | Day 86 | Day 87 | Day 88 | Day 89 | Day 90 | Day 91 | Day 92 | Day 93 | Day 94 | Day 95 | Day 96 | Day 97 | Day 98 | Day 99 | Day 100 | Day 101 | Day 102 | Day 103 | Day 104 | Day 105 | Day 106 | Day 107 | Day 108 | Day 109 | Day 110 | Day 111 | Day 112 | Day 113 | Day 114 | Day 115 | Day 116 | Day 117 | Day 118 | Day 119 | Day 120 | Day 121 | Day 122 | Day 123 | Day 124 | Day 125 | Day 126 | Day 127 | Day 128 | Day 129 | Day 130 | Day 131 | Day 132 | Day 133 | Day 134 | Day 135 | Day 136 | Day 137 | Day 138 | Day 139 | Day 140 | Day 141 | Day 142 | Day 143 | Day 144 | Day 145 | Day 146 | Day 147 | Day 148 | Day 149 | Day 150 | Day 151 | Day 152 | Day 153 | Day 154 | Day 155 | Day 156 | Day 157 | Day 158 | Day 159 | Day 160 | Day 161 | Day 162 | Day 163 | Day 164 | Day 165 | Day 166 | Day 167 | Day 168 | Day 169 | Day 170 | Day 171 | Day 172 | Day 173 | Day 174 | Day 175 | Day 176 | Day 177 | Day 178 | Day 179 | Day 180 | Day 181 | Day 182 | Day 183 | Day 184 | Day 185 | Day 186 | Day 187 | Day 188 | Day 189 | Day 190 | Day 191 | Day 192 | Day 193 | Day 194 | Day 195 | Day 196 | Day 197 | Day 198 | Day 199 | Day 200 | Day 201 | Day 202 | Day 203 | Day 204 | Day 205 | Day 206 | Day 207 | Day 208 | Day 209 | Day 210 | Day 211 | Day 212 | Day 213 | Day 214 | Day 215 | Day 216 | Day 217 | Day 218 | Day 219 | Day 220 | Day 221 | Day 222 | Day 223 | Day 224 | Day 225 | Day 226 | Day 227 | Day 228 | Day 229 | Day 230 | Day 231 | Day 232 | Day 233 | Day 234 | Day 235 | Day 236 | Day 237 | Day 238 | Day 239 | Day 240 | Day 241 | Day 242 | Day 243 | Day 244 | Day 245 | Day 246 | Day 247 | Day 248 | Day 249 | Day 250 | Day 251 | Day 252 | Day 253 | Day 254 | Day 255 | Day 256 | Day 257 | Day 258 | Day 259 | Day 260 | Day 261 | Day 262 | Day 263 | Day 264 | Day 265 | Day 266 | Day 267 | Day 268 | Day 269 | Day 270 | Day 271 | Day 272 | Day 273 | Day 274 | Day 275 | Day 276 | Day 277 | Day 278 | Day 279 | Day 280 | Day 281 | Day 282 | Day 283 | Day 284 | Day 285 | Day 286 | Day 287 | Day 288 | Day 289 | Day 290 | Day 291 | Day 292 | Day 293 | Day 294 | Day 295 | Day 296 | Day 297 | Day 298 | Day 299 | Day 300 | Day 301 | Day 302 | Day 303 | Day 304 | Day 305 | Day 306 | Day 307 | -0.063431635 0.06305041 0.00073330535 0.0005553306711963545\n",
      "Model: \"functional_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(None, 1239, 5)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_218 (LSTM)              (None, 1239, 64)          17920     \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 1239, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_219 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 51,009\n",
      "Trainable params: 51,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 90.6153 - val_loss: 89.6690\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 90.1092 - val_loss: 88.6558\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 89.3794 - val_loss: 87.6591\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 88.2630 - val_loss: 86.6801\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 87.2512 - val_loss: 85.7186\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 86.1364 - val_loss: 84.7747\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.600e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 85.3915 - val_loss: 83.2850\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 83.6080 - val_loss: 81.7831\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 81.7367 - val_loss: 80.2544\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 80.6182 - val_loss: 78.7198\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 78.8474 - val_loss: 77.1952\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 77.3367 - val_loss: 75.6874\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 2.560e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 76.1329 - val_loss: 73.3638\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 73.8475 - val_loss: 71.2030\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 71.3393 - val_loss: 69.0611\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 68.7769 - val_loss: 66.9089\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 66.7532 - val_loss: 64.7415\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 64.5616 - val_loss: 62.5503\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 4.096e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 61.9071 - val_loss: 58.9767\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 58.3794 - val_loss: 55.2027\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 54.3038 - val_loss: 51.3458\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 50.1037 - val_loss: 47.4571\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 46.6961 - val_loss: 43.4784\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 42.5689 - val_loss: 39.3848\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 6.554e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 38.3907 - val_loss: 32.6772\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 32.0004 - val_loss: 25.8216\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 24.9299 - val_loss: 18.8764\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 17.9528 - val_loss: 12.0095\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 11.5650 - val_loss: 5.8796\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 6.2989 - val_loss: 2.0784\n",
      "#############################################################################################################################\n",
      "Increasing learning rate to: 1.049e-03\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 4.0729 - val_loss: 10.8193\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 17.2821 - val_loss: 11.9778\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 19.1967 - val_loss: 6.0711\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 13.4214 - val_loss: 2.3932\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 7.3747 - val_loss: 1.6110\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.3463 - val_loss: 2.3287\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-04\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 4.8564 - val_loss: 1.6348\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.0118 - val_loss: 1.6696\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.1789 - val_loss: 1.7138\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.7957 - val_loss: 1.7651\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.5045 - val_loss: 1.8222\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2680 - val_loss: 1.8825\n",
      "#############################################################################################################################\n",
      "Decreasing learning rate to: 1.049e-05\n",
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.9067 - val_loss: 1.6128\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.6792 - val_loss: 1.6147\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.0500 - val_loss: 1.6166\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.1977 - val_loss: 1.6185\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.0284 - val_loss: 1.6206\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.7830 - val_loss: 1.6227\n",
      "#############################################################################################################################\n",
      "\n",
      "Loading Model: '02-08-2021--12--01-E2E_LSTM_ValSet_0.001-ALPHA100.0-BETA_SD100998-48Epochs-rlf-Loss-64-HU-'\n",
      "Total number of days: 309\n",
      "Day 0 | Day 1 | Day 2 | Day 3 | Day 4 | Day 5 | Day 6 | Day 7 | "
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "alpha = [1e-4, 1e-3, 1e-2, 1e-1, 0, 1e0, 1e1, 1e2, 1e3]\n",
    "beta = [1e-4, 1e-3, 1e-2, 1e-1, 0, 1e0, 1e1, 1e2, 1e3]\n",
    "alpha.sort(reverse=True)\n",
    "\n",
    "model_folder = './RL_val_models'\n",
    "\n",
    "DMJ = TF_Models('./data_sets/NASDAQ_Cleaned', model_folder, reload=False)\n",
    "data_splits = DMJ.split_data()\n",
    "# model = DMJ.generate_model()\n",
    "# GP = Graph_Predictions(model_folder, \"./strategies/RL_validation_strategies\", DMJ)\n",
    "\n",
    "for a in alpha:\n",
    "    for b in beta:\n",
    "        if a == b:\n",
    "            continue\n",
    "        # Reset the training object to get rid of old data\n",
    "        DMJ = TF_Models('./data_sets/NASDAQ_Cleaned', model_folder, reload=False)\n",
    "        \n",
    "        # Create the model using parameters we're tuning\n",
    "        DMJ._generate_model(model_type='lstm',loss_function='rank_loss', activation='leaky_relu', hidden_units=64, true_random=True, alpha=a, beta=b)\n",
    "        \n",
    "        # Have it train as much as it can\n",
    "        DMJ.train_model_loop(epoch_batches=6)\n",
    "        \n",
    "        # Save the model with a tag\n",
    "        DMJ.save_model(tag=f'E2E_LSTM_ValSet_{a}-ALPHA{b}-BETA_SD100998')\n",
    "        \n",
    "        # Reset the training object to get rid of old data\n",
    "        GP = Graph_Predictions(model_folder, \"./strategies/RL_validation_strategies\", DMJ)\n",
    "        \n",
    "        # Generate the prediction file\n",
    "        GP.generate_prediction_json(DMJ.model_name, neural_net_type='lstm')\n",
    "        \n",
    "        # Create the diagnostics file for the most recently saved model\n",
    "        GP.generate_model_diagnostics(GP.model_name, datablock_folder='RL_validation_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.history.history['val_loss']\n",
    "DMJ.history.history['lr']\n",
    "DMJ.epochs_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the  model\n",
    "model = DMJ.train_model(epochs=500)\n",
    "# model = DMJ.train_model(model, data_splits['x_train'], data_splits['y_train'], data_splits['x_val'], data_splits['y_val'], epochs=50, learning_rate=5e-5, gcn_matrix=DMJ.Normalized_Adjacency_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMJ.save_model(tag='E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GP = Graph_Predictions(\"./models\", \"./strategies\", DMJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=False, name_override='test_')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=False, average=10, name_override='LSTM-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--17--32--LSTM--50Epochs--mse-Loss--64-HU--None', avoid_fall=True, name_override='LSTM-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, name_override='LSTM-RankLoss-50Epoch')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='LSTM-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_lstm('11-22-2020--14--55--LSTM-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='LSTM-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--31--GCN1-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN1-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--31--GCN1-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN1-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--34--GCN1-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN1-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--34--GCN1-Rankloss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN1-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--37--GCN2-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN2-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--37--GCN2-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN2-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--39--GCN2-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN2-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--39--GCN2-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN2-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN3-MSE-50Epoch-AvoidFall-2ndBest')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN3-RankLoss-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--20--46--GCN3-MSE-Ratio+1--50Epochs--mse-Loss--64-HU--', avoid_fall=True, average=1, name_override='GCN3-MSE-200Epoch-Ratio+1')\n",
    "# GP.strategy_ratio_gcn('11-22-2020--20--43--GCN3-RankLoss-Ratio+1--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, average=1, name_override='GCN3-RankLoss-200Epoch-Ratio+1')\n",
    "\n",
    "'''New prediction memory storage'''\n",
    "# GP.generate_prediction_json('11-22-2020--15--01--LSTM-MSE--50Epochs--mse-Loss--64-HU--', neural_net_type='lstm')\n",
    "# GP.generate_prediction_json('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', neural_net_type='gcn')\n",
    "\n",
    "'''Seperately trained LSTM combo'''\n",
    "# GP.generate_prediction_json('01-04-2021--14--20--SEP_LSTM_GCN3-1e-5LR--70Epochs--mse-Loss--64-HU--', neural_net_type='gcn')\n",
    "\n",
    "''''''\n",
    "GP.generate_prediction_json('02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--', neural_net_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strat_name = '02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--_PM'\n",
    "\n",
    "# Testing the PM file feature\n",
    "avg = [1, 5, 20, 50, 100, 200]\n",
    "avg = [1]\n",
    "for a in avg:\n",
    "#     GP.prediction_json_strategy_max_entities(strat_name, average=a, avoid_fall=False, name_override=strat_name+ f'{a}AVG')\n",
    "    GP.prediction_json_strategy_determine_best(strat_name, average=a, avoid_fall=False, name_override=strat_name+ f'{a}AVG_Correct_BuyDay_plus1')\n",
    "    GP.save_results()\n",
    "# Testing the mse tracking feature\n",
    "# GP.prediction_json_mse('01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--_PM')\n",
    "GP.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.generate_model_diagnostics('02-04-2021--15--11--E2E_LSTM_VariedLR_RL_ALPHA_1_NOMSE--500Epochs--rlf-Loss--64-HU--_PM', datablock_folder='RL_validation_set')\n",
    "\n",
    "'01-04-2021--15--57--SEP_LSTM_GCN3-5e-6LR--820Epochs--mse-Loss--64-HU--_PM'\n",
    "255025890000.0\n",
    "0.012249682697757544\n",
    "0.7766990291262136\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GP.generate_upper_lower_avg_bounds()\n",
    "\n",
    "GP.display_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP.strategy_ratio_gcn('11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-MSE-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--46--GCN3-MSE--50Epochs--mse-Loss--64-HU--', avoid_fall=True, name_override='GCN3-MSE-50Epoch-AvoidFall')\n",
    "\n",
    "# GP.strategy_ratio_gcn('11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=False, average=10, name_override='GCN3-RankLoss-50Epoch-Average10')\n",
    "# GP.strategy_ratio_gcn(r'11-22-2020--16--43--GCN3-RankLoss--50Epochs--rlf-Loss--64-HU--', avoid_fall=True, name_override='GCN3-RankLoss-50Epoch-AvoidFall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP.save_results('./strategies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import GridBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "bar = widgets.IntProgress(min=0, max=10, description='Loading:', bar_style='info')\n",
    "display(bar)\n",
    "\n",
    "for i in range(11):\n",
    "    time.sleep(0.2)\n",
    "    bar.value = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "widgets.Text(\n",
    "    value='Loading',\n",
    "    description='',\n",
    "    disabled=True\n",
    "    layout=L\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the loading bar by initializing it\n",
    "nam_bar = widgets.IntProgress(min=0, max=5, value=0, description='Loading Normalized Adjacency Matrix:',\n",
    "                              layout=widgets.Layout(width='auto'))\n",
    "text = widgets.Text(value='Loading', description='', disabled=True, layout=widgets.Layout(width='auto'))\n",
    "\n",
    "test = widgets.GridBox(children=[text, nam_bar], layout=widgets.Layout(width='auto'))\n",
    "\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.amax(GP.rr_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(GP.rr_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.rr_test[476,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(GP.rr_test[5, :], GP.rr_test[100, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def swap_random(seq):\n",
    "    idx = range(len(seq))\n",
    "    i1, i2 = random.sample(idx, 2)\n",
    "    seq[i1], seq[i2] = seq[i2], seq[i1]\n",
    "\n",
    "trials_RL = []\n",
    "for e in range(100,300):\n",
    "    RL = []\n",
    "    for t in range(300):\n",
    "        A = []; B = [];\n",
    "        for l in range(e):\n",
    "            n = random.uniform(-1, 1)\n",
    "            A.append(n)\n",
    "            B.append(n)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        swap_random(B)\n",
    "        \n",
    "        return_ratio = tf.constant(A, shape=(len(A), 1))\n",
    "        ground_truth = tf.constant(B, shape=(len(B), 1))\n",
    "\n",
    "        ###############################################################\n",
    "        # Create an array of all_ones so that we can calculate all permutations of subtractions\n",
    "        all_ones = tf.ones([len(return_ratio), 1], dtype=tf.float32)\n",
    "\n",
    "        # Creates a N x N matrix with every predicted return ratio for each company subtracted with every other\n",
    "        # company\n",
    "        pred_dif = tf.math.subtract(\n",
    "            tf.matmul(return_ratio, all_ones, transpose_b=True),\n",
    "            tf.matmul(all_ones, return_ratio, transpose_b=True)\n",
    "        )\n",
    "\n",
    "        # Creates an N x N matrix containing every actual return ratio for each company subtracted with every other\n",
    "        # company By switching the order of the all_ones matricies and the actual prices, a negative sign is introduced\n",
    "        # When RELU is applied later, correct predictions will not affect loss while incorrect predictions will affect\n",
    "        # loss depending on how incorrect the prediction was\n",
    "        actual_dif = tf.math.subtract(\n",
    "            tf.matmul(all_ones, ground_truth, transpose_b=True),\n",
    "            tf.matmul(ground_truth, all_ones, transpose_b=True)\n",
    "        )\n",
    "\n",
    "        # Using the above two qualities, the algorithm can be punished for incorrectly calculating when a company is\n",
    "        # doing better than another company Reduces the mean across each dimension until only 1 value remains\n",
    "        rank_loss = tf.reduce_mean(\n",
    "            # Takes if a given value is >0, it is kept, otherwise, it becomes 0\n",
    "            tf.nn.relu(\n",
    "                # Multiplies all of the\n",
    "                tf.multiply(pred_dif, actual_dif)\n",
    "            )\n",
    "        )\n",
    "        RL.append(rank_loss)\n",
    "    trials_RL.append(np.mean(RL))\n",
    "\n",
    "plt.plot(trials_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [0.1, 0.5, 0.2, 0.3]\n",
    "print(list(zip(range(4), r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [100, 300, 200, 400]\n",
    "predictions = list(zip(range(len(predictions)), predictions))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normal Rank\n",
    "A = list(range(1, 10001))\n",
    "B = [i**(-1) for i in A]\n",
    "print(np.mean(A))\n",
    "print(np.mean(B)**-1)\n",
    "\n",
    "\n",
    "'000_Avg_RR.p'\n",
    "'000_Highest_RR_Possible.p'\n",
    "'000_Lowest_RR_Possible.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import similaritymeasures as sm\n",
    "import numpy as np\n",
    "\n",
    "# Generate random experimental data\n",
    "n = 5\n",
    "x = list(range(n))\n",
    "y = [11, 26, 26, 11, -60]\n",
    "exp_data = np.zeros((n, 2))\n",
    "exp_data[:, 0] = x\n",
    "exp_data[:, 1] = y\n",
    "\n",
    "# Generate random numerical data\n",
    "x = list(range(n))\n",
    "y = [1, 2, 30, 4, -9000]\n",
    "num_data = np.zeros((n, 2))\n",
    "num_data[:, 0] = x\n",
    "num_data[:, 1] = y\n",
    "\n",
    "area = sm.area_between_two_curves(exp_data, num_data)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_name = 'uhhhh.json'\n",
    "# If the .json file was already attached, this will fix the problem\n",
    "pm_name = pm_name.split('.json')\n",
    "pm_name = pm_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
